{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dcad43d-67f7-49b9-84ce-6d91a0f468a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e7cb9df-5746-44a3-998a-b671b284957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fda64db1-8058-4e15-b2d4-1a3e64f23fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "import math\n",
    "import string\n",
    "\n",
    "import tf_utils as tfu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dfb3b1a-2db7-4463-a208-b5d29e20f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tfu.strategy.gpu(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54593a0f-b5b4-401e-91e2-8c1a1598432e",
   "metadata": {},
   "source": [
    "---\n",
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddb61447-8228-4b92-8f3f-0eddb8e33574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-07-17 19:38:00--  https://www.cs.mtsu.edu/~jphillips/courses/CSCI4850-5850/public/PandP_Jane_Austen.txt\n",
      "Resolving www.cs.mtsu.edu (www.cs.mtsu.edu)... 161.45.162.100\n",
      "Connecting to www.cs.mtsu.edu (www.cs.mtsu.edu)|161.45.162.100|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 683838 (668K) [text/plain]\n",
      "Saving to: ‘PandP_Jane_Austen.txt.1’\n",
      "\n",
      "PandP_Jane_Austen.t 100%[===================>] 667.81K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2022-07-17 19:38:01 (11.6 MB/s) - ‘PandP_Jane_Austen.txt.1’ saved [683838/683838]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10657, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!wget https://www.cs.mtsu.edu/~jphillips/courses/CSCI4850-5850/public/PandP_Jane_Austen.txt\n",
    "with open('PandP_Jane_Austen.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "# Paragraphs are separated by blank\n",
    "# lines -> just drop those lines...\n",
    "text = []\n",
    "for i in range(len(lines)):\n",
    "     if lines[i] != '':\n",
    "        text = text + [lines[i]]\n",
    "data = np.vstack([[text[0:-1]],[text[1:]]]).T\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5f65884-85ad-4ebd-ba51-29430504b0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq = 100\n",
    "split_point = 80\n",
    "data = data[0:n_seq]\n",
    "np.random.shuffle(data) # In-place modification\n",
    "max_length = np.max([len(i) for i in data.flatten()]) + 2 # Add start/stop\n",
    "max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689bbd23-6cbf-472f-bd6f-dc4a008b9072",
   "metadata": {},
   "source": [
    "---\n",
    "# Encode Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09c4c6ef-93df-4294-af8f-2fbaf01ca9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_seq(x,mapping,max_length=0):\n",
    "    # String to integer\n",
    "    return [mapping['<START>']] + \\\n",
    "    [mapping[i] for i in list(x)] + \\\n",
    "    [mapping['<STOP>']] + \\\n",
    "    [0]*(max_length-len(list(x))-2)\n",
    "def decode_seq(x,mapping):\n",
    "    # Integer-to-string\n",
    "    try:\n",
    "        idx = list(x).index(2) # Stop token?\n",
    "    except:\n",
    "        idx = len(list(x)) # No stop token found\n",
    "    return ''.join([mapping[i] for i in list(x)[0:idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76b7c608-fe15-4420-a4fb-8356ebdcefc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_to_c_pandp = ['','<START>','<STOP>'] + list({char for word in data[:,0] for char in word})\n",
    "c_to_i_pandp = {i_to_c_pandp[i]:i for i in range(len(i_to_c_pandp))}\n",
    "i_to_c_pandp[1] = i_to_c_pandp[2] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6889059-fdd9-43e2-b0bc-199ff07c4199",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack([encode_seq(x,c_to_i_pandp,max_length) for x in data[:,0]])\n",
    "Y = np.vstack([encode_seq(x,c_to_i_pandp,max_length) for x in data[:,1]])\n",
    "x_train = X[:split_point]\n",
    "x_test = X[split_point:]\n",
    "pre_y_train = Y[:,0:-1][:split_point]\n",
    "pre_y_test = Y[:,0:-1][split_point:]\n",
    "post_y_train = Y[:,1:][:split_point]\n",
    "post_y_test = Y[:,1:][split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca664bb9-544b-4264-8da4-34acaca2d379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 74)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f725c81-51b2-4497-981a-a15981f9dba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 74)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6a03188-6146-4487-8483-b0a3296d4426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 51, 22, 41, 17, 47, 33, 35, 33, 51, 47, 17, 22, 51, 17, 43, 33,\n",
       "       48, 17, 26, 33, 48, 19, 44, 51, 18, 17, 45, 22, 20, 25, 31,  2,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "154bc9ef-0e64-4ad5-96d4-13012994718f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([51, 22, 41, 17, 47, 33, 35, 33, 51, 47, 17, 22, 51, 17, 43, 33, 48,\n",
       "       17, 26, 33, 48, 19, 44, 51, 18, 17, 45, 22, 20, 25, 31,  2,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2e33b84-c781-43ae-a1f6-5c721f8a5697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1, 31, 36, ...,  0,  0,  0],\n",
       "       [ 1, 31, 32, ..., 48,  2,  0],\n",
       "       [ 1, 31,  7, ...,  2,  0,  0],\n",
       "       ...,\n",
       "       [ 1, 38, 52, ...,  0,  0,  0],\n",
       "       [ 1, 48, 33, ...,  0,  0,  0],\n",
       "       [ 1, 31, 24, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd768ff5-2158-4e46-9852-2551bf73d350",
   "metadata": {},
   "source": [
    "---\n",
    "# Flatten Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be0ed294-df8f-47e7-b127-4d5d33a862ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Flatten_Data(x):\n",
    "    x = x.flatten()\n",
    "    \n",
    "    x = x[x != 0]\n",
    "    x = x[x != 1]\n",
    "    x = x[x != 2]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd7b115-1756-4eab-8494-a2c3cd6cfc77",
   "metadata": {},
   "source": [
    "---\n",
    "# Create Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93d7530f-e127-4ab2-b006-d8f3711ca453",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_flattened_train = Flatten_Data(x_train)\n",
    "pre_y_flattened_train = Flatten_Data(pre_y_train)\n",
    "post_y_flattened_train = Flatten_Data(post_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f33669-803a-45b3-8856-9f01c2ec70d4",
   "metadata": {},
   "source": [
    "---\n",
    "# Create Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f48728bc-d9fb-4f57-88b4-fac9bf7344b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_flattened_test = Flatten_Data(x_test)\n",
    "pre_y_flattened_test = Flatten_Data(pre_y_test)\n",
    "post_y_flattened_test = Flatten_Data(post_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1293d03c-68bd-496d-b9fb-91ae84688fe0",
   "metadata": {},
   "source": [
    "--- \n",
    "# Batch Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e23e34f9-3066-4756-ad0b-ccc1a3194438",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "rng = np.random.default_rng(seed)\n",
    "batch_size = 10\n",
    "block_size = 20\n",
    "seq_len = 200\n",
    "seq_len_padded = seq_len + 2\n",
    "maxlen = seq_len + 2 #Add start/stop tokens\n",
    "vocab_size = len(i_to_c_pandp)\n",
    "num_chars_data_train = x_flattened_train.shape[0]\n",
    "num_chars_data_test = x_flattened_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e768970c-7af4-4626-9d6e-d712ab7de959",
   "metadata": {},
   "outputs": [],
   "source": [
    "if block_size-2 > seq_len:\n",
    "    raise ValueError(\"Block size should not be bigger than sequence length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dce0777e-c8af-4835-833d-f1717dbf2501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202\n",
      "57\n",
      "4596\n",
      "994\n"
     ]
    }
   ],
   "source": [
    "print(maxlen)\n",
    "print(vocab_size)\n",
    "print(num_chars_data_train)\n",
    "print(num_chars_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56decdad-3c1d-496f-8f04-d783de100a1c",
   "metadata": {},
   "source": [
    "---\n",
    "# Generate Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc198d86-abfe-4a39-86fb-d8722200c228",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "def Partial_Batch(dataset, pre_y, post_y, num_chars, seq_len, rng):\n",
    "    rints = rng.integers(low=0, high=num_chars-seq_len, size=1)[0]\n",
    "    \n",
    "    end_x = rints + seq_len\n",
    "    end_y = end_x + seq_len\n",
    "    \n",
    "    x = dataset[rints:end_x]\n",
    "    x = np.insert(x, 0, 1)\n",
    "    x = np.insert(x, x.shape[0], 2)\n",
    "    \n",
    "    pre_y = dataset[end_x:end_y]\n",
    "    pre_y = np.insert(pre_y, 0, 1)\n",
    "    pre_y[-1] = 2\n",
    "    \n",
    "    post_y = dataset[end_x:end_y]\n",
    "    post_y = np.insert(post_y, post_y.shape[0], 2)\n",
    "    \n",
    "    batch = [x, pre_y, post_y]    \n",
    "    \n",
    "    batch = tf.keras.preprocessing.sequence.pad_sequences(batch, maxlen=maxlen, padding='post', value=0)\n",
    "\n",
    "    padding = maxlen + (block_size-(maxlen%block_size))\n",
    "                        \n",
    "    if (batch.shape[1] % block_size) != 0:\n",
    "        batch = tf.keras.preprocessing.sequence.pad_sequences(batch, maxlen=padding, padding='post', value=0)\n",
    "   \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ccc910a-fc5a-47c0-bf90-83587350fb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Full_Batch(batch_size, x, pre_y, post_y, num_chars, seq_len, rng):\n",
    "    x0 = []\n",
    "    y1 = []\n",
    "    y2 = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        a, b, c, = Partial_Batch(x, pre_y, post_y, num_chars, seq_len, rng)\n",
    "    \n",
    "        x0.append(a)\n",
    "        y1.append(b)\n",
    "        y2.append(c) \n",
    "    \n",
    "    return np.asarray(x0), np.asarray(y1), np.asarray(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33326847-2235-4ec9-ae39-a808a34050c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_train = Full_Batch(batch_size, x_flattened_train, pre_y_flattened_train, post_y_flattened_train, num_chars_data_train, seq_len, rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74123bfb-38c6-48cf-b830-39c7266398d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_test = Full_Batch(batch_size, x_flattened_test, pre_y_flattened_test, post_y_flattened_test, num_chars_data_test, seq_len, rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45d5d65-b728-42de-b529-85643adef96d",
   "metadata": {},
   "source": [
    "---\n",
    "# Masked Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a199f10-268d-4153-863c-2d430a4d2c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedTransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(MaskedTransformerBlock, self).__init__()\n",
    "        self.att1 = keras.layers.MultiHeadAttention(num_heads=num_heads,\n",
    "        key_dim=embed_dim)\n",
    "        self.att2 = keras.layers.MultiHeadAttention(num_heads=num_heads,\n",
    "        key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "        [keras.layers.Dense(ff_dim, activation=\"gelu\"),\n",
    "        keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "        self.dropout3 = keras.layers.Dropout(rate)\n",
    "    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n",
    "        i = tf.range(n_dest)[:, None]\n",
    "        j = tf.range(n_src)\n",
    "        m = i >= j - n_src + n_dest\n",
    "        mask = tf.cast(m, dtype)\n",
    "        mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "        mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "    def call(self, inputs, training):\n",
    "        input_shape = tf.shape(inputs[0])\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        mask = self.causal_attention_mask(batch_size,\n",
    "        seq_len, seq_len,\n",
    "        tf.bool)\n",
    "        attn_output1 = self.att1(inputs[0], inputs[0],\n",
    "        attention_mask = mask)\n",
    "        attn_output1 = self.dropout1(attn_output1, training=training)\n",
    "        out1 = self.layernorm1(inputs[0] + attn_output1)\n",
    "        attn_output2 = self.att2(out1, inputs[1])\n",
    "        attn_output2 = self.dropout2(attn_output2, training=training)\n",
    "        out2 = self.layernorm1(out1 + attn_output2)\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        return self.layernorm2(out2 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befee659-3a43-4372-ae56-860a836497e1",
   "metadata": {},
   "source": [
    "---\n",
    "# Masked Token and Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a27a7c63-8d17-4e49-bfd7-4e05d92eb915",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedTokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim, **kwargs):\n",
    "        super(MaskedTokenAndPositionEmbedding, self).__init__(**kwargs)\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size,\n",
    "        output_dim=embed_dim,\n",
    "        mask_zero=True)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen+1,\n",
    "        output_dim=embed_dim,\n",
    "        mask_zero=True)\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=1, limit=maxlen+1, delta=1)\n",
    "        positions = positions * tf.cast(tf.sign(x),tf.int32)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348583b3-a8cf-4d03-bc57-cbd8c106b890",
   "metadata": {},
   "source": [
    "---\n",
    "# Custom Loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "160d05e4-b851-4348-85df-508526fb154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "def MaskedSparseCategoricalCrossentropy(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "def MaskedSparseCategoricalAccuracy(real, pred):\n",
    "    accuracies = tf.equal(tf.cast(real,tf.int64), tf.argmax(pred, axis=2))\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b66c98-f8b5-4b80-9736-66a9769cc966",
   "metadata": {},
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9af4ac04-1eab-4d74-bfc0-716408efeed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_initializer(initializer):\n",
    "    if isinstance(initializer, tf.keras.initializers.Initializer):\n",
    "        return initializer.__class__.from_config(initializer.get_config())\n",
    "    return initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aadfeaaa-08b0-4155-8381-328ac71f4c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape_list(tensor, expected_rank=None, name=None):\n",
    "    if expected_rank is not None:\n",
    "        assert_rank(tensor, expected_rank, name)\n",
    "\n",
    "    shape = tensor.shape.as_list()\n",
    "\n",
    "    non_static_indexes = []\n",
    "    for (index, dim) in enumerate(shape):\n",
    "        if dim is None:\n",
    "            non_static_indexes.append(index)\n",
    "\n",
    "    if not non_static_indexes:\n",
    "        return shape\n",
    "\n",
    "    dyn_shape = tf.shape(tensor)\n",
    "    for index in non_static_indexes:\n",
    "        shape[index] = dyn_shape[index]\n",
    "    return shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835c39dd-9fb5-49bf-8763-14ca694a1a35",
   "metadata": {},
   "source": [
    "---\n",
    "# Cache Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da4cb5e4-2d26-4e40-a00f-3e71f470b273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_state: `Tensor`, the current state.\n",
    "# previous_state: `Tensor`, the previous state.\n",
    "# memory_length: `int`, the number of tokens to cache.\n",
    "# reuse_length: `int`, the number of tokens in the current batch to be cached\n",
    "#     and reused in the future.\n",
    "# Returns:  `Tensor`, representing the cached state with stopped gradients.\n",
    "def _cache_memory(current_state, previous_state, memory_length, reuse_length=0):\n",
    "    if memory_length is None or memory_length == 0:\n",
    "        return None\n",
    "    else:\n",
    "        if reuse_length > 0:\n",
    "            current_state = current_state[:, :reuse_length, :]\n",
    "\n",
    "        if previous_state is None:\n",
    "            new_mem = current_state[:, -memory_length:, :]\n",
    "        else:\n",
    "            new_mem = tf.concat(\n",
    "                    [previous_state, current_state], 1)[:, -memory_length:, :]\n",
    "\n",
    "    return tf.stop_gradient(new_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3126b9-a96e-4d6a-ae65-3c9579514cf1",
   "metadata": {},
   "source": [
    "---\n",
    "# MultiHead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b32faad-fd06-4448-b817-b3a1f5943aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query: Query `Tensor` of shape `[B, T, dim]`.\n",
    "# value: Value `Tensor` of shape `[B, S, dim]`.\n",
    "# content_attention_bias: Bias `Tensor` for content based attention of shape\n",
    "# `[num_heads, dim]`.\n",
    "# positional_attention_bias: Bias `Tensor` for position based attention of\n",
    "# shape `[num_heads, dim]`.\n",
    "# key: Optional key `Tensor` of shape `[B, S, dim]`. If not given, will use\n",
    "# `value` for both `key` and `value`, which is the most common case.\n",
    "# relative_position_encoding: Relative positional encoding `Tensor` of shape\n",
    "# `[B, L, dim]`.\n",
    "# state: Optional `Tensor` of shape `[B, M, E]` where M is the length of the\n",
    "# state or memory. If passed, this is also attended over as in Transformer\n",
    "# XL.\n",
    "# attention_mask: A boolean mask of shape `[B, T, S]` that prevents attention\n",
    "# to certain positions.\n",
    "class MultiHeadRelativeAttention(tf.keras.layers.MultiHeadAttention):\n",
    "    def __init__(self,\n",
    "                 kernel_initializer=\"variance_scaling\",\n",
    "                 **kwargs):\n",
    "        super().__init__(kernel_initializer=kernel_initializer,\n",
    "                                         **kwargs)\n",
    "\n",
    "    def _build_from_signature(self, query, value, key=None):\n",
    "        super(MultiHeadRelativeAttention, self)._build_from_signature(\n",
    "                query=query,\n",
    "                value=value,\n",
    "                key=key)\n",
    "        if hasattr(value, \"shape\"):\n",
    "            value_shape = tf.TensorShape(value.shape)\n",
    "        else:\n",
    "            value_shape = value\n",
    "        if key is None:\n",
    "            key_shape = value_shape\n",
    "        elif hasattr(key, \"shape\"):\n",
    "            key_shape = tf.TensorShape(key.shape)\n",
    "        else:\n",
    "            key_shape = key\n",
    "\n",
    "        common_kwargs = dict(\n",
    "                kernel_initializer=self._kernel_initializer,\n",
    "                bias_initializer=self._bias_initializer,\n",
    "                kernel_regularizer=self._kernel_regularizer,\n",
    "                bias_regularizer=self._bias_regularizer,\n",
    "                activity_regularizer=self._activity_regularizer,\n",
    "                kernel_constraint=self._kernel_constraint,\n",
    "                bias_constraint=self._bias_constraint)\n",
    "\n",
    "        with tf.init_scope():\n",
    "            einsum_equation, _, output_rank = _build_proj_equation(\n",
    "                    key_shape.rank - 1, bound_dims=1, output_dims=2)\n",
    "            self._encoding_dense = tf.keras.layers.experimental.EinsumDense(\n",
    "                    einsum_equation,\n",
    "                    output_shape=_get_output_shape(\n",
    "                        output_rank - 1,\n",
    "                        [self._num_heads, self._key_dim]),\n",
    "                        bias_axes=None,\n",
    "                        name=\"encoding\",\n",
    "                        **common_kwargs)\n",
    "\n",
    "# query: Projected query `Tensor` of shape `[B, T, N, key_dim]`.\n",
    "# key: Projected key `Tensor` of shape `[B, S + M, N, key_dim]`.\n",
    "# value: Projected value `Tensor` of shape `[B, S + M, N, key_dim]`.\n",
    "# position: Projected position `Tensor` of shape `[B, L, N, key_dim]`.\n",
    "# content_attention_bias: Trainable bias parameter added to the query head\n",
    "#     when calculating the content-based attention score.\n",
    "# positional_attention_bias: Trainable bias parameter added to the query\n",
    "#     head when calculating the position-based attention score.\n",
    "# attention_mask: (default None) Optional mask that is added to attention\n",
    "#     logits. If state is not None, the mask source sequence dimension should\n",
    "#     extend M.\n",
    "# Returns:\n",
    "# attention_output: Multi-headed output of attention computation of shape\n",
    "#     `[B, S, N, key_dim]`.\n",
    "    def compute_attention(\n",
    "        self,\n",
    "        query,\n",
    "        key,\n",
    "        value,\n",
    "        position,\n",
    "        content_attention_bias,\n",
    "        positional_attention_bias,\n",
    "        attention_mask=None\n",
    "    ):\n",
    "        \n",
    "        #AC\n",
    "        content_attention = tf.einsum(self._dot_product_equation, key, query + content_attention_bias)\n",
    "        \n",
    "        positional_attention = tf.einsum(self._dot_product_equation, position, query + positional_attention_bias)\n",
    "        \n",
    "        #BD\n",
    "        positional_attention = _rel_shift(positional_attention, klen=tf.shape(content_attention)[3])\n",
    "        \n",
    "        attention_sum = content_attention + positional_attention\n",
    "\n",
    "        attention_scores = tf.multiply(attention_sum, 1.0 / math.sqrt(float(self._key_dim)))\n",
    "\n",
    "        attention_scores = self._masked_softmax(attention_scores, attention_mask)\n",
    "\n",
    "        attention_output = self._dropout_layer(attention_scores)\n",
    "\n",
    "        attention_output = tf.einsum(self._combine_equation,\n",
    "                                                                 attention_output,\n",
    "                                                                 value)\n",
    "        return attention_output\n",
    "\n",
    "# * Number of heads (H): the number of attention heads.\n",
    "# * Value size (V): the size of each value embedding per head.\n",
    "# * Key size (K): the size of each key embedding per head. Equally, the size\n",
    "#     of each query embedding per head. Typically K <= V.\n",
    "# * Batch dimensions (B).\n",
    "# * Query (target) attention axes shape (T).\n",
    "# * Value (source) attention axes shape (S), the rank must match the target.\n",
    "# * Encoding length (L): The relative positional encoding length.\n",
    "# query: attention input.\n",
    "# value: attention input.\n",
    "# content_attention_bias: A trainable bias parameter added to the query head\n",
    "#     when calculating the content-based attention score.\n",
    "# positional_attention_bias: A trainable bias parameter added to the query\n",
    "#     head when calculating the position-based attention score.\n",
    "# key: attention input.\n",
    "# relative_position_encoding: relative positional encoding for key and\n",
    "#     value.\n",
    "# state: (default None) optional state. If passed, this is also attended\n",
    "#     over as in TransformerXL.\n",
    "# attention_mask: (default None) Optional mask that is added to attention\n",
    "#     logits. If state is not None, the mask source sequence dimension should\n",
    "#     extend M.\n",
    "# Returns:\n",
    "# attention_output: The result of the computation, of shape [B, T, E],\n",
    "#     where `T` is for target sequence shapes and `E` is the query input last\n",
    "#     dimension if `output_shape` is `None`. Otherwise, the multi-head outputs\n",
    "#     are projected to the shape specified by `output_shape`.\n",
    "    def call(self,\n",
    "             query,\n",
    "             value,\n",
    "             content_attention_bias,\n",
    "             positional_attention_bias,\n",
    "             key=None,\n",
    "             relative_position_encoding=None,\n",
    "             state=None,\n",
    "             attention_mask=None):\n",
    "        \n",
    "        if not self._built_from_signature:\n",
    "            self._build_from_signature(query, value, key=key)\n",
    "        if key is None:\n",
    "            key = value\n",
    "        if state is not None and state.shape.ndims > 1:\n",
    "            value = tf.concat([state, value], 1)\n",
    "            key = tf.concat([state, key], 1)\n",
    "\n",
    "        # `query` = [B, T, N ,H]\n",
    "        query = self._query_dense(query)\n",
    "\n",
    "        # `key` = [B, S + M, N, H]\n",
    "        key = self._key_dense(key)\n",
    "\n",
    "        # `value` = [B, S + M, N, H]\n",
    "        value = self._value_dense(value)\n",
    "\n",
    "        # `position` = [B, L, N, H]\n",
    "        position = self._encoding_dense(relative_position_encoding)\n",
    "\n",
    "        attention_output = self.compute_attention(\n",
    "                query=query,\n",
    "                key=key,\n",
    "                value=value,\n",
    "                position=position,\n",
    "                content_attention_bias=content_attention_bias,\n",
    "                positional_attention_bias=positional_attention_bias,\n",
    "                attention_mask=attention_mask)\n",
    "\n",
    "        # `attention_output` = [B, S, N, H]\n",
    "        attention_output = self._output_dense(attention_output)\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95abdafa-2c98-4c04-b9e8-aae23e53265e",
   "metadata": {},
   "source": [
    "---\n",
    "# Relative Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc8c99a8-3755-48f9-9704-c1dbd04e0ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rel_shift(x, klen=-1):    \n",
    "    x = tf.transpose(x, perm=[2, 3, 0, 1])\n",
    "    x = tf.pad(x, [[0, 0], [1, 0], [0, 0], [0, 0]])\n",
    "    x_size = tf.shape(x)\n",
    "    x = tf.reshape(x, [x_size[1], x_size[0], x_size[2], x_size[3]])\n",
    "    x = tf.slice(x, [1, 0, 0, 0], [-1, -1, -1, -1])\n",
    "    x = tf.reshape(x, [x_size[0], x_size[1] - 1, x_size[2], x_size[3]])\n",
    "    x = tf.transpose(x, perm=[2, 3, 0, 1])\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a026fd-2cbe-45b7-88f7-a3ffd3548806",
   "metadata": {},
   "source": [
    "---\n",
    "# Build Einsum Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5102422c-82af-49d4-a8d5-5081262b21e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_CHR_IDX = string.ascii_lowercase\n",
    "\n",
    "# Builds an einsum equation for projections inside multi-head attention\n",
    "def _build_proj_equation(free_dims, bound_dims, output_dims):\n",
    "    input_str = \"\"\n",
    "    kernel_str = \"\"\n",
    "    output_str = \"\"\n",
    "    bias_axes = \"\"\n",
    "    letter_offset = 0\n",
    "    for i in range(free_dims):\n",
    "        char = _CHR_IDX[i + letter_offset]\n",
    "        input_str += char\n",
    "        output_str += char\n",
    "\n",
    "    letter_offset += free_dims\n",
    "    for i in range(bound_dims):\n",
    "        char = _CHR_IDX[i + letter_offset]\n",
    "        input_str += char\n",
    "        kernel_str += char\n",
    "\n",
    "    letter_offset += bound_dims\n",
    "    for i in range(output_dims):\n",
    "        char = _CHR_IDX[i + letter_offset]\n",
    "        kernel_str += char\n",
    "        output_str += char\n",
    "        bias_axes += char\n",
    "    equation = \"%s,%s->%s\" % (input_str, kernel_str, output_str)\n",
    "\n",
    "    return equation, bias_axes, len(output_str)\n",
    "\n",
    "\n",
    "def _get_output_shape(output_rank, known_last_dims):\n",
    "    return [None] * (output_rank - len(known_last_dims)) + list(known_last_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5343f3e3-8536-46d5-9d1b-28f8c3f7908b",
   "metadata": {},
   "source": [
    "---\n",
    "# Relative Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a5f5eeac-4c07-4796-8281-f5c039fe7c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_size: Size of the hidden layer.\n",
    "# min_timescale: Minimum scale that will be applied at each position\n",
    "# max_timescale: Maximum scale that will be applied at each position.\n",
    "class RelativePositionEmbedding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 hidden_size: int,\n",
    "                 min_timescale: float = 1.0,\n",
    "                 max_timescale: float = 1.0e4,\n",
    "                 **kwargs):\n",
    "        \n",
    "        if \"dtype\" not in kwargs:\n",
    "            kwargs[\"dtype\"] = \"float32\"\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self._hidden_size = hidden_size\n",
    "        self._min_timescale = min_timescale\n",
    "        self._max_timescale = max_timescale\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "                \"hidden_size\": self._hidden_size,\n",
    "                \"min_timescale\": self._min_timescale,\n",
    "                \"max_timescale\": self._max_timescale,\n",
    "        }\n",
    "        base_config = super(RelativePositionEmbedding, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "# inputs: An tensor whose second dimension will be used as `length`. If\n",
    "# `None`, the other `length` argument must be specified.\n",
    "# length: An optional integer specifying the number of positions. If both\n",
    "# `inputs` and `length` are spcified, `length` must be equal to the second\n",
    "# dimension of `inputs`.\n",
    "# Returns:\n",
    "# A tensor in shape of `(length, hidden_size)`.\n",
    "    def call(self, inputs, length=None):\n",
    "        if inputs is None and length is None:\n",
    "            raise ValueError(\"If inputs is None, `length` must be set in \"\n",
    "                                             \"RelativePositionEmbedding().\")\n",
    "        if inputs is not None:\n",
    "            input_shape = get_shape_list(inputs)\n",
    "            if length is not None and length != input_shape[1]:\n",
    "                raise ValueError(\n",
    "                        \"If inputs is not None, `length` must equal to input_shape[1].\")\n",
    "            length = input_shape[1]\n",
    "        position = tf.cast(tf.range(length), tf.float32)\n",
    "        num_timescales = self._hidden_size // 2\n",
    "        min_timescale, max_timescale = self._min_timescale, self._max_timescale\n",
    "        log_timescale_increment = (\n",
    "                math.log(float(max_timescale) / float(min_timescale)) /\n",
    "                (tf.cast(num_timescales, tf.float32) - 1))\n",
    "        inv_timescales = min_timescale * tf.exp(\n",
    "                tf.cast(tf.range(num_timescales), tf.float32) *\n",
    "                -log_timescale_increment)\n",
    "        scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(\n",
    "                inv_timescales, 0)\n",
    "        position_embeddings = tf.concat(\n",
    "                [tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n",
    "        position_embeddings = tf.expand_dims(position_embeddings, axis=0)\n",
    "        return tf.tile(position_embeddings, (tf.shape(inputs)[0], 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c32bb5-4520-4802-a8f0-5f7849905a83",
   "metadata": {},
   "source": [
    "---\n",
    "# XL Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7245b7dd-a3b2-464a-9f5b-7efd4a9d0ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size: The size of the token vocabulary.\n",
    "# hidden_size: The size of the transformer hidden layers.\n",
    "# num_attention_heads: The number of attention heads.\n",
    "# head_size: The dimension size of each attention head.\n",
    "# inner_size: The inner size for the transformer layers.\n",
    "# dropout_rate: Dropout rate for the output of this layer.\n",
    "# attention_dropout_rate: Dropout rate on attention probabilities.\n",
    "# norm_epsilon: Epsilon value to initialize normalization layers.\n",
    "# inner_activation: The activation to use for the inner\n",
    "#     FFN layers.\n",
    "# kernel_initializer: Initializer for dense layer kernels.\n",
    "# inner_dropout: Dropout probability for the inner dropout\n",
    "#     layer.\n",
    "class TransformerXLBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 hidden_size,\n",
    "                 num_attention_heads,\n",
    "                 head_size,\n",
    "                 inner_size,\n",
    "                 dropout_rate,\n",
    "                 attention_dropout_rate,\n",
    "                 norm_epsilon=1e-12,\n",
    "                 inner_activation=\"relu\",\n",
    "                 kernel_initializer=\"variance_scaling\",\n",
    "                 inner_dropout=0.0,\n",
    "                 **kwargs):\n",
    "        \"\"\"Initializes TransformerXLBlock layer.\"\"\"\n",
    "\n",
    "        super(TransformerXLBlock, self).__init__(**kwargs)\n",
    "        self._vocab_size = vocab_size\n",
    "        self._num_heads = num_attention_heads\n",
    "        self._head_size = head_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._inner_size = inner_size\n",
    "        self._dropout_rate = dropout_rate\n",
    "        self._attention_dropout_rate = attention_dropout_rate\n",
    "        self._inner_activation = inner_activation\n",
    "        self._norm_epsilon = norm_epsilon\n",
    "        self._kernel_initializer = kernel_initializer\n",
    "        self._inner_dropout = inner_dropout\n",
    "        self._attention_layer_type = MultiHeadRelativeAttention\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_tensor = input_shape[0] if len(input_shape) == 2 else input_shape\n",
    "        input_tensor_shape = tf.TensorShape(input_tensor)\n",
    "        if len(input_tensor_shape.as_list()) != 3:\n",
    "            raise ValueError(\"TransformerLayer expects a three-dimensional input of \"\n",
    "                                             \"shape [batch, sequence, width].\")\n",
    "        batch_size, sequence_length, hidden_size = input_tensor_shape\n",
    "\n",
    "        if len(input_shape) == 2:\n",
    "            mask_tensor_shape = tf.TensorShape(input_shape[1])\n",
    "            expected_mask_tensor_shape = tf.TensorShape(\n",
    "                    [batch_size, sequence_length, sequence_length])\n",
    "            if not expected_mask_tensor_shape.is_compatible_with(mask_tensor_shape):\n",
    "                raise ValueError(\"When passing a mask tensor to TransformerXLBlock, \"\n",
    "                                                 \"the mask tensor must be of shape [batch, \"\n",
    "                                                 \"sequence_length, sequence_length] (here %s). Got a \"\n",
    "                                                 \"mask tensor of shape %s.\" %\n",
    "                                                 (expected_mask_tensor_shape, mask_tensor_shape))\n",
    "        if hidden_size % self._num_heads != 0:\n",
    "            raise ValueError(\n",
    "                    \"The input size (%d) is not a multiple of the number of attention \"\n",
    "                    \"heads (%d)\" % (hidden_size, self._num_heads))\n",
    "        self._attention_layer = self._attention_layer_type(\n",
    "                num_heads=self._num_heads,\n",
    "                key_dim=self._head_size,\n",
    "                value_dim=self._head_size,\n",
    "                dropout=self._attention_dropout_rate,\n",
    "                use_bias=False,\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer),\n",
    "                name=\"rel_attn\")\n",
    "        self._attention_dropout = tf.keras.layers.Dropout(\n",
    "                rate=self._attention_dropout_rate)\n",
    "        self._attention_layer_norm = tf.keras.layers.LayerNormalization(\n",
    "                name=\"self_attention_layer_norm\",\n",
    "                axis=-1,\n",
    "                epsilon=self._norm_epsilon,\n",
    "                dtype=tf.float32)\n",
    "        self._inner_dense = tf.keras.layers.experimental.EinsumDense(\n",
    "                \"abc,cd->abd\",\n",
    "                output_shape=(None, self._inner_size),\n",
    "                bias_axes=\"d\",\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer),\n",
    "                name=\"inner\")\n",
    "\n",
    "        self._inner_activation_layer = tf.keras.layers.Activation(\n",
    "                self._inner_activation)\n",
    "        self._inner_dropout_layer = tf.keras.layers.Dropout(\n",
    "                rate=self._inner_dropout)\n",
    "        self._output_dense = tf.keras.layers.experimental.EinsumDense(\n",
    "                \"abc,cd->abd\",\n",
    "                output_shape=(None, hidden_size),\n",
    "                bias_axes=\"d\",\n",
    "                name=\"output\",\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer))\n",
    "        self._output_dropout = tf.keras.layers.Dropout(rate=self._dropout_rate)\n",
    "        self._output_layer_norm = tf.keras.layers.LayerNormalization(\n",
    "                name=\"output_layer_norm\",\n",
    "                axis=-1,\n",
    "                epsilon=self._norm_epsilon)\n",
    "\n",
    "        super(TransformerXLBlock, self).build(input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "                \"vocab_size\":\n",
    "                        self._vocab_size,\n",
    "                \"hidden_size\":\n",
    "                        self._hidden_size,\n",
    "                \"num_attention_heads\":\n",
    "                        self._num_heads,\n",
    "                \"head_size\":\n",
    "                        self._head_size,\n",
    "                \"inner_size\":\n",
    "                        self._inner_size,\n",
    "                \"dropout_rate\":\n",
    "                        self._dropout_rate,\n",
    "                \"attention_dropout_rate\":\n",
    "                        self._attention_dropout_rate,\n",
    "                \"norm_epsilon\":\n",
    "                        self._norm_epsilon,\n",
    "                \"inner_activation\":\n",
    "                        self._inner_activation,\n",
    "                \"kernel_initializer\":\n",
    "                        self._kernel_initializer,\n",
    "                \"inner_dropout\":\n",
    "                        self._inner_dropout,\n",
    "        }\n",
    "        base_config = super(TransformerXLBlock, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "# content_stream: `Tensor`, the input content stream. This is the standard\n",
    "# input to Transformer XL and is commonly referred to as `h` in XLNet.\n",
    "# content_attention_bias: Bias `Tensor` for content based attention of shape\n",
    "# `[num_heads, dim]`.\n",
    "# positional_attention_bias: Bias `Tensor` for position based attention of\n",
    "# shape `[num_heads, dim]`.\n",
    "# relative_position_encoding: Relative positional encoding `Tensor` of shape\n",
    "# `[B, L, dim]`.\n",
    "# state: Optional `Tensor` of shape `[B, M, E]`, where M is the length of\n",
    "# the state or memory. If passed, this is also attended over as in\n",
    "# Transformer XL.\n",
    "# content_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to content attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# query_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to query attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# target_mapping: Optional `Tensor` representing the target mapping when\n",
    "# calculating query attention.\n",
    "# Returns:\n",
    "# A `dict` object, containing the key value pairs for `content_attention`\n",
    "    def call(self,\n",
    "             content_stream,\n",
    "             content_attention_bias,\n",
    "             positional_attention_bias,\n",
    "             relative_position_encoding=None,\n",
    "             state=None,\n",
    "             content_attention_mask=None,\n",
    "             query_attention_mask=None,\n",
    "             target_mapping=None):\n",
    "        \n",
    "        attention_kwargs = dict(\n",
    "                query=content_stream,\n",
    "                value=content_stream,\n",
    "                key=content_stream,\n",
    "                attention_mask=content_attention_mask)\n",
    "\n",
    "        common_attention_kwargs = dict(\n",
    "                content_attention_bias=content_attention_bias,\n",
    "                relative_position_encoding=relative_position_encoding,\n",
    "                positional_attention_bias=positional_attention_bias,\n",
    "                state=state)\n",
    "\n",
    "        attention_kwargs.update(common_attention_kwargs)\n",
    "        attention_output = self._attention_layer(**attention_kwargs)\n",
    "\n",
    "        attention_stream = attention_output\n",
    "        input_stream = content_stream\n",
    "        attention_key = \"content_attention\"\n",
    "        attention_output = {}\n",
    "        \n",
    "        attention_stream = self._attention_dropout(attention_stream)\n",
    "        attention_stream = self._attention_layer_norm(attention_stream + input_stream)\n",
    "        inner_output = self._inner_dense(attention_stream)\n",
    "        inner_output = self._inner_activation_layer(\n",
    "                inner_output)\n",
    "        inner_output = self._inner_dropout_layer(\n",
    "                inner_output)\n",
    "        layer_output = self._output_dense(inner_output)\n",
    "        layer_output = self._output_dropout(layer_output)\n",
    "        layer_output = self._output_layer_norm(layer_output + attention_stream)\n",
    "        attention_output[attention_key] = layer_output\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8749ec30-1281-4f9d-8baa-54b890fb7949",
   "metadata": {},
   "source": [
    "---\n",
    "# Transformer XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "019ca25d-e899-4e17-8f41-dfdc3d9bdee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_layers: The number of layers.\n",
    "# hidden_size: The hidden size.\n",
    "# num_attention_heads: The number of attention heads.\n",
    "# head_size: The dimension size of each attention head.\n",
    "# inner_size: The hidden size in feed-forward layers.\n",
    "# dropout_rate: Dropout rate used in each Transformer XL block.\n",
    "# attention_dropout_rate: Dropout rate on attention probabilities.\n",
    "# initializer: The initializer to use for attention biases.\n",
    "# tie_attention_biases: Whether or not to tie biases together. If `True`, then\n",
    "# each Transformer XL block shares the same trainable attention bias. If\n",
    "# `False`, then each block has its own attention bias. This is usually set\n",
    "# to `True`.\n",
    "# memory_length: The number of tokens to cache.\n",
    "# reuse_length: The number of tokens in the current batch to be cached\n",
    "# and reused in the future.\n",
    "# inner_activation: The activation to use in the inner layers\n",
    "# for Transformer XL blocks. Typically \"relu\" or \"gelu\".\n",
    "class TransformerXL(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 num_layers,\n",
    "                 hidden_size,\n",
    "                 maxlen,\n",
    "                 embed_dim,\n",
    "                 num_attention_heads,\n",
    "                 head_size,\n",
    "                 inner_size,\n",
    "                 dropout_rate,\n",
    "                 attention_dropout_rate,\n",
    "                 initializer,\n",
    "                 tie_attention_biases=True,\n",
    "                 memory_length=None,\n",
    "                 reuse_length=None,\n",
    "                 inner_activation=\"relu\",\n",
    "                 **kwargs):\n",
    "        super(TransformerXL, self).__init__(**kwargs)\n",
    "\n",
    "        self._vocab_size = vocab_size\n",
    "        self._initializer = initializer\n",
    "        self._num_layers = num_layers\n",
    "        self._hidden_size = hidden_size\n",
    "        self._num_attention_heads = num_attention_heads\n",
    "        self._head_size = head_size\n",
    "        self._inner_size = inner_size\n",
    "        self._inner_activation = inner_activation\n",
    "        self._dropout_rate = dropout_rate\n",
    "        self._attention_dropout_rate = attention_dropout_rate\n",
    "        self._tie_attention_biases = tie_attention_biases\n",
    "\n",
    "        self._memory_length = memory_length\n",
    "        self._reuse_length = reuse_length\n",
    "\n",
    "        if self._tie_attention_biases:\n",
    "            attention_bias_shape = [self._num_attention_heads, self._head_size]\n",
    "        else:\n",
    "            attention_bias_shape = [self._num_layers, self._num_attention_heads, self._head_size]\n",
    "\n",
    "        self.content_attention_bias = self.add_weight(\n",
    "                \"content_attention_bias\",\n",
    "                shape=attention_bias_shape,\n",
    "                dtype=tf.float32,\n",
    "                initializer=clone_initializer(self._initializer))\n",
    "        self.positional_attention_bias = self.add_weight(\n",
    "                \"positional_attention_bias\",\n",
    "                shape=attention_bias_shape,\n",
    "                dtype=tf.float32,\n",
    "                initializer=clone_initializer(self._initializer))\n",
    "\n",
    "        self.transformer_xl_layers = []\n",
    "        for i in range(self._num_layers):\n",
    "            self.transformer_xl_layers.append(\n",
    "                    TransformerXLBlock(\n",
    "                            vocab_size=self._vocab_size,\n",
    "                            hidden_size=self._head_size * self._num_attention_heads,\n",
    "                            num_attention_heads=self._num_attention_heads,\n",
    "                            head_size=self._head_size,\n",
    "                            inner_size=self._inner_size,\n",
    "                            dropout_rate=self._dropout_rate,\n",
    "                            attention_dropout_rate=self._attention_dropout_rate,\n",
    "                            norm_epsilon=1e-12,\n",
    "                            inner_activation=self._inner_activation,\n",
    "                            kernel_initializer=\"variance_scaling\",\n",
    "                            name=\"layer_%d\" % i))\n",
    "\n",
    "        self.output_dropout = tf.keras.layers.Dropout(rate=self._dropout_rate)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "                \"vocab_size\":\n",
    "                        self._vocab_size,\n",
    "                \"num_layers\":\n",
    "                        self._num_layers,\n",
    "                \"hidden_size\":\n",
    "                        self._hidden_size,\n",
    "                \"num_attention_heads\":\n",
    "                        self._num_attention_heads,\n",
    "                \"head_size\":\n",
    "                        self._head_size,\n",
    "                \"inner_size\":\n",
    "                        self._inner_size,\n",
    "                \"dropout_rate\":\n",
    "                        self._dropout_rate,\n",
    "                \"attention_dropout_rate\":\n",
    "                        self._attention_dropout_rate,\n",
    "                \"initializer\":\n",
    "                        self._initializer,\n",
    "                \"tie_attention_biases\":\n",
    "                        self._tie_attention_biases,\n",
    "                \"memory_length\":\n",
    "                        self._memory_length,\n",
    "                \"reuse_length\":\n",
    "                        self._reuse_length,\n",
    "                \"inner_activation\":\n",
    "                        self._inner_activation,\n",
    "        }\n",
    "        base_config = super(TransformerXL, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "# content_stream: `Tensor`, the input content stream. This is the standard\n",
    "# input to Transformer XL and is commonly referred to as `h` in XLNet.\n",
    "# relative_position_encoding: Relative positional encoding `Tensor` of shape\n",
    "# `[B, L, dim]`.\n",
    "# state: Optional `Tensor` of shape `[B, M, E]`, where M is the length of\n",
    "# the state or memory. If passed, this is also attended over as in\n",
    "# Transformer XL.\n",
    "# content_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to content attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# query_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to query attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# target_mapping: Optional `Tensor` representing the target mapping when\n",
    "# calculating query attention.\n",
    "# Returns:\n",
    "# A tuple consisting of the attention output and the list of cached memory\n",
    "# states.\n",
    "# The attention output is `content_attention`\n",
    "    def call(self,\n",
    "             content_stream,\n",
    "             relative_position_encoding,\n",
    "             state=None,\n",
    "             content_attention_mask=None,\n",
    "             query_attention_mask=None,\n",
    "             target_mapping=None):\n",
    "        \n",
    "        new_mems = []\n",
    "\n",
    "        if state is None:\n",
    "            state = [None] * self._num_layers\n",
    "        for i in range(self._num_layers):\n",
    "            # cache new mems\n",
    "            new_mems.append(\n",
    "                    _cache_memory(content_stream, state[i],\n",
    "                                                self._memory_length, self._reuse_length))\n",
    "\n",
    "            if self._tie_attention_biases:\n",
    "                content_attention_bias = self.content_attention_bias\n",
    "            else:\n",
    "                content_attention_bias = self.content_attention_bias[i]\n",
    "                \n",
    "            if self._tie_attention_biases:\n",
    "                positional_attention_bias = self.positional_attention_bias\n",
    "            else:\n",
    "                positional_attention_bias = self.positional_attention_bias[i]\n",
    "\n",
    "            transformer_xl_layer = self.transformer_xl_layers[i]\n",
    "            \n",
    "            transformer_xl_output = transformer_xl_layer(\n",
    "                    content_stream=content_stream,\n",
    "                    content_attention_bias=content_attention_bias,\n",
    "                    positional_attention_bias=positional_attention_bias,\n",
    "                    relative_position_encoding=relative_position_encoding,\n",
    "                    state=state[i],\n",
    "                    content_attention_mask=content_attention_mask,\n",
    "                    query_attention_mask=query_attention_mask,\n",
    "                    target_mapping=target_mapping)\n",
    "            content_stream = transformer_xl_output[\"content_attention\"]\n",
    "\n",
    "        output_stream = content_stream\n",
    "        return output_stream, new_mems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a127a4b5-15e3-46e3-838b-a5e18e4d8dc8",
   "metadata": {},
   "source": [
    "---\n",
    "# Xl Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "855d43dd-e182-4c2a-961a-ff9781141f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XlModel(keras.Model):\n",
    "    def __init__(self, block_size, seq_len_padded, embed_dim, vocab_size, num_layers, hidden_size, num_attention_heads, maxlen, memory_length, reuse_length, head_size, inner_size, dropout_rate, attention_dropout_rate, initializer):\n",
    "        super(XlModel, self).__init__()\n",
    "        \n",
    "        self.block_size = block_size\n",
    "        self.seq_len_padded = seq_len_padded\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_attention_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.maxlen = maxlen\n",
    "        self.memory_length = memory_length\n",
    "        \n",
    "        self.embedding_layer = MaskedTokenAndPositionEmbedding(maxlen=maxlen, vocab_size=vocab_size, embed_dim=embed_dim)\n",
    "        \n",
    "        self.rel_embeddings = RelativePositionEmbedding(embed_dim)\n",
    "\n",
    "        self.transformer_xl = TransformerXL(\n",
    "                vocab_size=vocab_size,\n",
    "                num_layers=num_layers,\n",
    "                hidden_size=hidden_size,\n",
    "                num_attention_heads=num_attention_heads,\n",
    "                maxlen=maxlen,\n",
    "                embed_dim=embed_dim,\n",
    "                memory_length=memory_length,\n",
    "                reuse_length=reuse_length,\n",
    "                head_size=head_size,\n",
    "                inner_size=inner_size,\n",
    "                dropout_rate=dropout_rate,\n",
    "                attention_dropout_rate=attention_dropout_rate,\n",
    "                initializer=initializer, \n",
    "            )\n",
    "        \n",
    "    \n",
    "    def call(self, x, training=None):        \n",
    "        \n",
    "        encoder_output = []\n",
    " \n",
    "        mems = tf.zeros((self.num_layers, tf.shape(x)[0], self.memory_length, self.embed_dim))\n",
    "        \n",
    "        embeddings = self.embedding_layer(x)\n",
    "        \n",
    "        if self.memory_length > 0:\n",
    "            rel_embeddings = self.rel_embeddings(tf.concat((mems[0], embeddings), axis=1))\n",
    "        else:\n",
    "            rel_embeddings = self.rel_embeddings(embeddings)\n",
    "            \n",
    "        for i in range(0, self.seq_len_padded, self.block_size):\n",
    "            block = embeddings[:,i:i+block_size]\n",
    "            rel_block = rel_embeddings[:,i:i+block_size+self.memory_length]\n",
    "            \n",
    "            output, mems = self.transformer_xl(content_stream=block, relative_position_encoding=rel_block, state=mems)\n",
    "        \n",
    "            if i == 0:\n",
    "                model_output = output\n",
    "            else:\n",
    "                model_output = tf.concat([model_output, output], axis=1)\n",
    "        \n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3469de1-250d-4c14-bb79-a4e74f0fe93e",
   "metadata": {},
   "source": [
    "---\n",
    "# Xl Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2bc3ea7d-d4a1-4a1e-bf2a-7cd1c91430cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xl Parameters \n",
    "embed_dim = 64\n",
    "num_layers = 8\n",
    "hidden_size = 64\n",
    "num_attention_heads = 8\n",
    "memory_length = 40\n",
    "reuse_length = 0\n",
    "head_size = 32\n",
    "inner_size = 32\n",
    "dropout_rate = 0.0\n",
    "attention_dropout_rate = 0.0\n",
    "initializer = keras.initializers.RandomNormal(stddev=0.1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89649663-7d18-48bc-8678-ce5f093908cc",
   "metadata": {},
   "source": [
    "---\n",
    "# Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "821b0c4a-6d05-4867-a713-474bfdbff881",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():  \n",
    "    model = XlModel(block_size, seq_len_padded, embed_dim, vocab_size, num_layers, hidden_size, num_attention_heads, maxlen, memory_length, reuse_length, head_size, inner_size, dropout_rate, attention_dropout_rate, initializer)\n",
    "    model.compile(loss = MaskedSparseCategoricalCrossentropy, optimizer = keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c726b5aa-bc90-44a8-bc2e-5bf755a0dcdc",
   "metadata": {},
   "source": [
    "---\n",
    "# Train Step Test Split Inside Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "78f3a0f1-b39a-4b87-a3ef-e00ee12505bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_step(batches):\n",
    "    \n",
    "#     batch_train, batch_test = batches\n",
    "    \n",
    "#     x_train, pre_y_train, post_y_train = batch_train\n",
    "#     x_test, pre_y_test, post_y_test = batch_test\n",
    "    \n",
    "#     with tf.GradientTape() as tape:\n",
    "\n",
    "#         output_train = model(x_train)\n",
    "#         loss_train = MaskedSparseCategoricalCrossentropy(post_y_train, output_train)\n",
    "    \n",
    "#     accuracy_train = MaskedSparseCategoricalAccuracy(post_y_train, output_train)\n",
    "          \n",
    "#     grads = tape.gradient(loss_train, model.trainable_weights)\n",
    "    \n",
    "#     model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "#     return loss_train, accuracy_train\n",
    "\n",
    "# @tf.function()\n",
    "# def dist_train_step(batches):\n",
    "#     losses, accuracy = strategy.run(train_step, args=(batches,))\n",
    "#     return strategy.reduce(tf.distribute.ReduceOp.SUM, losses, axis=None), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3c9d81f1-57e9-416c-a7ea-bf8b8a3c2881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(batches):\n",
    "    \n",
    "    batch_train, batch_test = batches\n",
    "    \n",
    "    x_train, pre_y_train, post_y_train = batch_train\n",
    "    x_test, pre_y_test, post_y_test = batch_test\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        output_train = model(x_train)\n",
    "        loss_train = MaskedSparseCategoricalCrossentropy(post_y_train, output_train)\n",
    "        \n",
    "        output_test = model(x_test)\n",
    "        loss_test = MaskedSparseCategoricalCrossentropy(post_y_test, output_test)\n",
    "        \n",
    "    accuracy_train = MaskedSparseCategoricalAccuracy(post_y_train, output_train)\n",
    "    accuracy_test = MaskedSparseCategoricalAccuracy(post_y_test, output_test)\n",
    "          \n",
    "    grads = tape.gradient(loss_train, model.trainable_weights)\n",
    "    \n",
    "    model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "    return loss_train, accuracy_train, loss_test, accuracy_test\n",
    "\n",
    "@tf.function()\n",
    "def dist_train_step(batches):\n",
    "    loss_train, accuracy_train, loss_test, accuracy_test = strategy.run(train_step, args=(batches,))\n",
    "    return strategy.reduce(tf.distribute.ReduceOp.SUM, loss_train, axis=None), accuracy_train, loss_test, accuracy_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63c3a2e-02b5-4dbe-9527-a33c23338ce5",
   "metadata": {},
   "source": [
    "---\n",
    "# Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5a6127f4-fb94-4bf2-b3cf-fae83ef0aa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_loss_train = []\n",
    "history_accuracy_train = []\n",
    "history_loss_test = []\n",
    "history_accuracy_test = []\n",
    "\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fba5ece2-ae3c-4480-a78c-c24d7275c9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with strategy.scope():    \n",
    "#     for epoch in range(epochs):\n",
    "        \n",
    "#         batch_train = Full_Batch(batch_size, x_flattened_train, pre_y_flattened_train, post_y_flattened_train, num_chars_data_train, seq_len, rng)\n",
    "#         batch_test = Full_Batch(batch_size, x_flattened_test, pre_y_flattened_test, post_y_flattened_test, num_chars_data_test, seq_len, rng)\n",
    "\n",
    "#         batches = [batch_train, batch_test]\n",
    "        \n",
    "#         loss, accuracy = dist_train_step(batches)\n",
    "        \n",
    "#         history_loss_train.append(loss)\n",
    "#         history_accuracy_train.append(accuracy) \n",
    "        \n",
    "#         print(f\"\\r{epoch+1}/{epochs} Loss: {loss} Accuracy = {accuracy}\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a260118e-c39d-4232-8ee2-11ae86d8c8f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 Train Loss: 0.1929730921983719 Train Accuracy = 0.948756217956543 Test Loss: 5.9262542724609375 Test Accuracy = 0.069364160299301151"
     ]
    }
   ],
   "source": [
    "with strategy.scope():    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        batch_train = Full_Batch(batch_size, x_flattened_train, pre_y_flattened_train, post_y_flattened_train, num_chars_data_train, seq_len, rng)\n",
    "        batch_test = Full_Batch(batch_size, x_flattened_test, pre_y_flattened_test, post_y_flattened_test, num_chars_data_test, seq_len, rng)\n",
    "        \n",
    "        batches = [batch_train, batch_test]\n",
    "        \n",
    "        loss_train, accuracy_train, loss_test, accuracy_test = dist_train_step(batches)\n",
    "        \n",
    "        history_loss_train.append(loss_train)\n",
    "        history_accuracy_train.append(accuracy_train) \n",
    "        history_loss_test.append(loss_test)\n",
    "        history_accuracy_test.append(accuracy_test) \n",
    "        \n",
    "        print(f\"\\r{epoch+1}/{epochs} Train Loss: {loss_train} Train Accuracy = {accuracy_train} Test Loss: {loss_test} Test Accuracy = {accuracy_test}\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5029869f-09b0-4ad7-8b69-06a155104667",
   "metadata": {},
   "source": [
    "---\n",
    "# Train vs Test Accuracy and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7c3ecbaa-4117-47fa-9ad3-de755a5b98e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAACQCAYAAAAC/XD9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAApM0lEQVR4nO3dd3yURf7A8c9sSy+kkEAChN4xQESpB2IB0ZPz9Kxn5VDPs+DZPX92D68qlkMUxe55Njz7ASIiCIIgNfQAAQIppLfN7vz+mE0jbRMSNlm+79crrzzPPGVnlvDd2Zl5ZpTWGiGEEP7H4usMCCGEaBsS4IUQwk9JgBdCCD8lAV4IIfyUBHghhPBTEuCFEMJP2XydgZpiYmJ0UlKSr7MhhBAdxtq1a7O01rH1HWtXAT4pKYk1a9b4OhtCCNFhKKX2NnSsTZtolFKRSqn3lVKpSqmtSqnRbfl6QgghqrV1G/wzwJda6wHAKcDWNn49IYRoX/IOQFF29b6z9IS9dJs10SilwoEJwDUAWutyoLytXk8IIdqN4hxwOeHNC+HwJrA64IZlsH8V/Pc2SBgJB9bC5e9B78lgbZtQrNpqLhqlVDIwD9iCqb2vBW7TWhcdc95MYCZA9+7dR+7d22BzkhBC+JbbDY92AmWFUy6DgedB/6m1z8nYBHPHNu++F78Ggy4ApZqdJaXUWq11Sr3H2jDApwA/AGO11quUUs8A+VrrBxu6JiUlRUsnqxDCK6vmwfo3Tc34eBzdC7l7oeeE6jStoSgLAsPBFmDSCjLAYoO/9q59/YwlkLEBnMVQXgzfPN6yfDyc16LLGgvwbTmKJh1I11qv8uy/D9zbhq8nhDiZfHGX+V2aB4ERkH8IUj+FoRdBUKfq87SuXTOuDN6hsZD6Gbx7uUmfsRi6joD/3gLr3qw+P7oPRCTC7qX15+PlM+pPD+sKBQdh+JW179d1BITGwd4VUF4A2t3sonurzQK81jpDKbVfKdVfa70NmIxprhFCdFRaQ/YuiOnTNvd3uyH1v5A0Hta/bWrFw6+E8K7gdsHS2aYW3Wdy9TUvnwXxQ2DTB2b/8zvhzIdh04emZg0wfS4UHILOA+GdS01aVC/I2V3jPpNNMM/eWTtP2TvrpjXGYod70iAgtDpt7O3gCIXMVOg9qfb5D0dA37O9v38ztFkTDVS1w78MOIDdwLVa66MNnS9NNEK0cz+/Cx/dAFd9Ar1+UfuYs8QEt8oOw9I8WLsATv+9CcpHtkJeOix+BC56FY7ugdDO0HU4LH/aHItIhEUP1b5v4iiY8T/430Pw/dMnoJDHsNjAXVE3PaY/ZG2rmz75IRh/h/f3P/Z9ayZfNdGgtV4P1PvCQogOoKwA9nwHEQmmRlxZkz2wtjrAO0shZxe8dAZUlELcELjkTZiTbI5v/ghOuby6SQXg+VOrt2MHmJptQ9JXw9zxZjTKsbokQ1Em5B8w+wPOM800VRR0G2VGrzTkT0dg7WvV+bvmcxPQXeWmlh/VC/7e37Sv359e470phO/+DsMuMd8wZncz6Y7Quq/RGHtQ885vhjatwTeX1OCFaAFXRcuH2S19ygSYsbfWPTZvIhxc1/C1l//HBLZ3L4PcfS17/YZ0Ox3OeRLWzIf1b1Wnz1gMcYPhiXiI7AG3e5pgMrfB7m/htJnwr3FweCPcuQOCY8BigcWPwcb3qvM57g7Y/CGc82cYcK5J+34OoGHsbXXzU+4Z/OcIaTjPad/DgnPhD2vbrgmrHj4ZRdMSEuCF8ILbbYIWwI8vw2d/hNs3QkS3usPsKsrBaof9qyExxbRp20Ng9zemyeTrB8x5Q38Dv3wWMreaoJix0QTu5grpDEVHqvcn3A3L/mK2bUFQUVJ97IIXYOHvzfaMxSZIJ19uxpA7QsAeaI4VZMCih+HUGaYMYJo1UNXn1FSYCftWmGGHxzrwE1is0OWU5petnZIAL0RHUpQFITGmU/FommkiqAzcWz+F96+FG5ebGmtls0KXU+DQz+ahmZ4TIHtH7ZEb3ojpB1nb66b/ah58NLPxa3/zBqCh31TY8jEER0F0XwiKhJfPhJHXwPDfVjdjdB8D130B6WsgfhjYHM3Lq6giAV4IXyg5ajoaOyXVTtcadi2B7V/BmQ+Z2mpZgWnfXrvA/NQUGm/Oi+gGr53X+vkceU3d16yUcj2c9w/Yudh86ARFmRr6S2eYJpKJ90H/KbWHJTbmwxtgw7tw6dswYFprleCk5rNOViFOGjm74bM74dy/mmaQueOqjz2cZx6msdpNZ+LBdbD4UXMsLM6Ml/74xobvXZgBH9/kfV4sdnA7a6dN+7tpygH47UdmHPayv8F5/zS1bO2Gn16vPn/KU+Yhn2GeIYU1hyWGxcOQi2DMH8wImOa48EXzI04IqcEL0RwluWALrNv2u/APsO4Ns13ZXFIp+YraHYVNueQt+PcV9R877UbTMbj5Q9j2BQz5tWmOydljhvOdfhNE9zbfCFBQmmtq1411DoLpqC3OMucqi/kwEh2CNNEI4a3c/WZ43/Vfw/4fofcZENPXtIFrDY9EmvP+uM3UZME8kfjq1IbuWL/zn4Hl/zRt7ABnPwF5+82HQ/LlZqKqT26FvctNB+iIq0zaCRydIToGCfBCeMNZCk/E1U3/9XxTo114C5TVmC8kfphpY8/1TJB37AiSKU9BynWwc1H1iJSoXia939mmjf71C6DPmTD5/9quXMKvSYAXQmvz8EpxDvzwggnKUb2qH1Q5409m/HRZExM+KStoV9307mPg6k8ABV/dBz3GwuDp1ceLc8wDMDJaRLQyCfBCLHrYNIm0VGicmRZ2/J1m/6MbzKiWLsOg3xTT7i2ED8goGuG/dn9rHmEffXN1Wnmx6TB0V5gHfIKiGg7uE+8348ULDpkOy4HnmYd8vn3KjO3WLgiJhaRxta+79vO2K5MQrUQCvOiYNrxnAnjl8ME+Z5kRIGnfmQeBGnPd1/DK2aYDdeI95tF0i7V65MjA82HE1Wb+FSE6ML8I8M9/s5NhiRGM7xvr66yItlRWAAFhZvvD39U+VnPyqoZM/j/z6H730+De/Wa4I9T/uLsEd+EH/CLAd156F1m9J0LfeiYJEh2XsxR+eN4s5rDiOTO9rLcqV8c5mgYFh01TS48x1ccDw1s1q0K0R34R4KeqlWwokNp7h6Q1LHncPBE50PMYvtsFz6XUXoyhMRPvh6VPVu9P/Wv1dqekulMFCHGS8IsAX44d5SrzdTZESxzeBN/9zcxg2OdMeONXkP5j3UftAcbcYuY/iRsCCSNg5XOwZgH84m4zmqXzQAnmQtTgFwHeqRxYXKW+zoZorpKj8N7VZrs4y3SY7lth9rsOr56LfMhFMOXPZvWfmsb/0fxA3ZXthRD+FOClBt9hZG6DgHD412gT5Ctt/tD8HnEVnD8Htiw0HasjfuubfArRwTUZ4JVSfwEeB0qAL4FTgNu11s2cbLrtlCsHVne5r7MhmpK1w7StH+vMR6rX4ay5Gk7NJ0GFEM3mTQ3+bK313UqpXwHpwMXAN0C7CfAVyoHNLTX4dktrM1nX/LNqp5/6OzO9rlIw5EKzEo9MpiVEq/EmwFfOG3ou8I7WOkcduyyYj1VYArBKgG9ftnwCn99lFoNYuwCGXly7OeYPayCqd/VKRZHdzY8QotVYvDjnv0qpVCAFWKyUigW87tFUSlmVUuuUUp82fXbLuCwObNJE075sfM8sVFG5UtDG/1Qf+90SMwWvxZs/PyFESzX5P0xrfS8wGkjRWjuBIqCe1WwbdBuwtWXZ806FJUACvC9lboP0tWa+8mV/g3+Nha3/rT4++MLq7T8dgYSRJz6PQpyEvOlkvRj4UmvtUkr9CRiB6XTN8OLaRGAa8ARwx3HmtUEuayAOLU00PvP8qPrT+001S72dOgMm3Q/RfaqbZIQQbc6b78gPaq0LlFLjgHOA14B/eXn/p4G7AXfLsucdlz2MYF3Uli8hanK7oMLzgZq9q/5zElJg0n0w6ncmqFeuiiSEOGG86WStXN1gGvAvrfVCpdTDTV2klDoPOKK1XquUmtjIeTOBmQDdu7esk00HRhCmi9BuN0raddveRzdA6udwzuPw6azqdHsIXPYOxA+F4Cjf5U8IAXhXgz+glHoR+A3wuVIqwMvrxgK/VEqlAe8CZyil6gyt1FrP01qnaK1TYmNbOJ9MYAQOVUFxsdTiT4iN/wFnUe3g/ot74f4D0OsXEtyFaCe8CdS/Ab4Cpmitc4Eo4K6mLtJa36e1TtRaJwGXAku01lceR14bZA2OBKAgL7stbi8qZWyEfw6pmz7wl6Y5RppghGhXvBlFUwzsAs5RSv0B6Ky1/rrNc9YMjjBT88/LPOjjnPgZrWHDf8x0Aft/hLnjIG9/7XNmLIFL3vBN/oQQjfJmFM1twO8Az0QhvKmUmqe1ftbbF9FaLwWWtiSD3giLN+thFmbsAsY0frLwXvqP8OEMCE+E/PTax2Z+C12TfZItIYR3vOlkvR44TWszTEUp9RSwEvA6wLe16O79AXBmeTl/uPDOriXmd2VwV1aYcCecdqO0swvRAXgT4BXVI2nwbLerxtaITrHk6xBUbpqvs9KxOUvhnUvMb3cFHFhTfezX82HoRb7LmxCi2bwJ8K8Cq5RSH3n2pwPz2yxHLXTYFk9I0T5fZ6NjOvQzpK+B75+B3L21j924HHL2mIWohRAdSpMBXmv9D6XUUmAcpuZ+LXC4jfPVbIfDhjAy90vKSwpxBIX6OjvtW85uWPk87FwEp98MX9QYFBU7EDI9M0tc+LIZ0x4/1Df5FEIcF68W/NBa/wT8VLmvlNoHtKup/8JGXETQkoVsXPgXhl76qK+z41tuNxQcgtA42P4F9J4MSx4zS+It+yvsW1l97hfHjHi9/muwWKE4W2Z3FKKDa+mKTu2qDR5g2LhpLP92FKdtfY775vTk0Zuvxq40u7NLiAsPJCTALxavql9lQI9IgPxDpqllVT2zSfzwQu39lOug8AiMvhkcoXB0DwSGm2OOkLbPtxCiTSmtdfMvUmqf1rrVq3cpKSl6zZo1TZ/YgMXrttP/46l05ijZRNBF5ZClw9nq7k50IESXp7PQNZZ4ewnR9jKcMYMoy0ojUJeQaM1lfdh44tyZdLI5oespHEjbwdd5Cfwi9CBJCfEE95/E3sM5TBo7hl1FQWzYe4Q8p4VfDU9gT2YhA7tGEGi3sintMEmRFn7McNMlIpABATlmMeiCQ7DvB+h2GoR3NXO6WG1mvLmzBOxBJuDmp0N4AtgC4Mf5ENsfgqJg5/+g2+mw51tI+850hmbvqH4DAiKgLK/xN6nzYDjjAeh/rjyYJIQfUEqt1VrXs1RaIwFeKfUsUN9BBVyttQ5vvSwaxxvgtda88M6HTN77Tw45Q5jk/qEVc1dbkQ4gRJXh1FZKCCBcFbPbHU+IKiVO5dY5P4tOxFC94EWeDiFYlZGlw+micup9DW11oFzNmAY5pDNol2mKyd4JB9bCrM1wJBW6n246UzslmZq+EMIvtDTAX93YTbXWr7VC3mo53gB/LJdbs3L7QfrERxEfEcj2Q0fJPbyXAzlF9OkaxY69B4nt3p8hCZHYCvazbedOonsMJCoqhsVffkyAw06yZSeZ9gTK17zJMNdmvnWfQpgq5YgOp5MqIkuHE04xEyw/k6q7c0DH0EcdoJ/lANlEEE0eW93d6KJyKCaAve54uqos9utYSnHQU2WQRQSRFBKmiumqcjiqQ3nbdQZRFLBO92Gi5WciKeQ5fTHJOpUuKhsLmoOhg3kxN4VQSnjk0vFUuDSjekYR7LAS4i4k8Oh26DG61d5PIUT706IA7wutHeDbSlmFi7IKN+GB9lrp6UeLCQ+yU1zmIirEgcNmoaCknMMF5ThdbqwWxTOLdvB/5w9i3rLduNya2yb35WBeCW+v2sdbq/YRExpAVmEZnYLtjOjeiQ0H8sgsMFPzhgfayC+t8Dqfl43qTmKnIFbvyWH2r4fSJSKoVd8HIYTvSYD3I6VOF1aLYvvhAsID7Xy87gCr03LILixny6F8LArcjfyTXjaqGw9MG0SoP3c6C3ESkQB/kiivcGO3Kp5ZvIMe0cH8vD+PBSvS6j03PjyQ6cMTuOTUbvSMkREzQnRUxxXglVJjtdbfN5XWGiTAtz6XW2O1KDYfzCP1UAFr9h7lndW1n/h98ldDiQy2E2S3MmlAZx/lVAjREscb4H/SWo9oKq01SIBveyXlLq6cv4oZ43ryyc8H+WJT7aV1l901ie7RwT7KnRCiuRoL8A02xCqlRmPm3o1VStVcMDscsLZuFsWJEuSw8sFNZkrlswfHM3b2EjLyS6uOT/jrN4xKiuLJC4fSp7NM+SBER9bYgh8OIBTzIRBW4ycfkGkF/YDVolh53xn85aJhLLtrEjaLefBpdVoOZ/7jW656ZTXZhWW0p34aIYT3vGmi6aG13uvZtgChWuv8tsiMNNH4VlZhGSmPL6qTfvagOJ67fAQOmyxoLkR701gTjTf/Y/+slApXSoUAW4BtSqkm12QVHU9MaAC7nzyX4d0ja6V/veUwj3+2xTeZEkK0mDcBfpCnxj4d+Bwzi+Rv2zJTwncsFsVr141i0R2/4JzBcZya1AmA11fu5flvdvo4d0KI5vDmaRe7UsqOCfDPaa2dSilplPVj4YF2wgPtvPhb861vxa4sLn9pFX/9ahuT+nfmQG4J/ePCZLSNEO2cNwH+RSAN+BlYppTqgeloFSeJMb1jqrbPnfNd1fbCm8cysEu4tM2L4+J0OklPT6e0tLTpk09igYGBJCYmYrfbmz7Zo6XTBdu01o1OiqKU6ga8DsQDbmCe1vqZxq6RTtb2a8vB/FrBvaZnLk0mJSmKhEiZ60Y03549ewgLCyM6OholU1jXS2tNdnY2BQUF9OzZs9axFo2Dr3FxHPAk0FVrPVUpNQgYTdPrslYAf9Ra/6SUCgPWKqX+p7WW3roOaFDXcHY+MZWM/FJe/HY3b/xQvXbrbe+ur9re9vgUAmzymITwXmlpKUlJSRLcG6GUIjo6mszMzGZd58136wXAV0BXz/524PamLtJaH/Is9YfWugDYCshE5B2YzWohsVMwj00fwstX1VthoP+fvpRx86LZJLg3rSXvUYMBXilVWbuP0Vq/h2lmwdM042pmxpKA4cCqZudQtEtnDorjq9sn1Hus5pOxQrR32dnZJCcnk5ycTHx8PAkJCVX75eWNL7izZs0abr311iZfY8yYMa2V3WZprIlmNTACKFJKReNZ3UkpdTrQxLpw1ZRSocAHwO31PSCllJoJzATo3l0Wee5I+seH8e7M0wm0W+kaEcioJxcDcPu765l75Ug6hTh8nEMhmhYdHc369esBePjhhwkNDeXOO++sOl5RUYHNVn+oTElJISWl/m+zNa1YsaJV8tpcjTXRVH4fuAP4BOitlPoe03F6izc39wyv/AB4S2v9YX3naK3naa1TtNYpsbGx3udctAun94omuVskncMDef26UQCs2pPDPxdtZ192MXnFTh/nUIjmu+aaa7jjjjuYNGkS99xzD6tXr2bMmDEMHz6cMWPGsG3bNgCWLl3KeeedB5gPh+uuu46JEyfSq1cv5syZU3W/0NDQqvMnTpzIRRddxIABA7jiiiuqmjQ///xzBgwYwLhx47j11lur7ns8GqvB15xk7CPMQ04KKAPOBDY0dmNlGozmA1u11v847pyKdm9Cv1juOqc/f/1qG6+v3MvrK01H7FszTmNsn5gmrhYCHvnvZrYcbN1R2IO6hvPQ+YObfd327dtZtGgRVquV/Px8li1bhs1mY9GiRdx///188MEHda5JTU3lm2++oaCggP79+3PTTTfVGda4bt06Nm/eTNeuXRk7dizff/89KSkp3HDDDSxbtoyePXty2WWXtbi8NTVWg7diJhsLA0IwHwZWINiT1pSxmCdez1BKrff8nHuc+RXt3M2T+jDvtyNrpV3x8io2HzStekcKpH1edAwXX3wxVqsZEZaXl8fFF1/MkCFDmDVrFps3b673mmnTphEQEEBMTAydO3fm8OHDdc4ZNWoUiYmJWCwWkpOTSUtLIzU1lV69elUNgWytAN9YDf6Q1vrRlt5Ya72c6mYecRI5e3A8qY9NYcCDX1alTZuzvGp76Z0TSZJVpEQ9WlLTbishIdV/ow8++CCTJk3io48+Ii0tjYkTJ9Z7TUBAQNW21WqloqLu40L1ndNWI8+8aYMXotkC7Vb+fOHQeo9N/NtSyiqaNRBLCJ/Ky8sjIcGM8l6wYEGr33/AgAHs3r2btLQ0AP7973+3yn0bC/CTW+UVxEnrslHdSZs9jbTZ09jxxFT+c+PoqmNfbMwg6d7P2Hmk0Ic5FMI7d999N/fddx9jx47F5Wr9yklQUBAvvPACU6ZMYdy4ccTFxREREXHc95VFt8UJlXTvZ7X275kygJsm9vZRbkR7sHXrVgYOHOjrbPhcYWEhoaGhaK25+eab6du3L7Nmzap1Tn3v1fHOBy9Eq3lrxmm19vcfLQbA6XLjcrefyoYQJ9pLL71EcnIygwcPJi8vjxtuuOG47+nNbJJCtJouEYG19t9etY93V++jMra/dt0oukQEcjC3hIn9O/sgh0L4xqxZs+rU2I+XBHhxQvWKDeXvF5/CmYPiWL0nh9+9voaaFferX1ldtb310SkEOcwwtU0H8ugZE0JIgPzJCuEtaaIRJ9yvRyYSEWTnrEFxzJzQi2nDutR73rp9RwEodbo479nlXP/ajycym0J0eFIdEj51/7mmw+izDZ/VOXb5y2ZuusoFRX7YnVPnnFe/38PgrhGM6hnVhrkUomOSAC/ahRX3nsHi1CM4K9w8+mntJQPKK9xV27e/u46BXcL5ad9Rnr5kOI/815ybNnvaCc2vEB2BBHjRLnSNDOK3p/cAzJw2kcF2Hvx4E19syuCGCb14cdluAD5ef5CP1x8E4NoF1e31brem3OXmtRVpXDM2SRYdEV7Lzs5m8mTz2E9GRgZWq5XKiQ9Xr16Nw9H4rKhLly7F4XBUTQk8d+5cgoODueqqq9o2416QcfCiQ/huRybXL1jDhH6xBNgsfLbxUJ1zZk7oxTzPB8HaP51JdGgAmw/mUVhawYb0PHpEB3P24PgTnXXRhPY0Dr6+6YLb4pqWau44eKnBiw5hfN9Ytj8xFTBNNvUF+MrgDnD1q6v5+Pdja82BA9KUI7yzdu1a7rjjDgoLC4mJiWHBggV06dKFOXPmMHfuXGw2G4MGDWL27NnMnTsXq9XKm2++ybPPPsvixYurAv7EiRM57bTT+Oabb8jNzWX+/PmMHz+e4uJirrnmGlJTUxk4cCBpaWk8//zzXs0t3xwS4EWH47BZqgL1w59sZsGKNEb1jGL1nupO2E0H8unzwBd1rn1jZRpXnt6j1vJnR/JLCXRYCQ/0frV60Ua+uBcyNrbuPeOHwtTZXp+uteaWW25h4cKFxMbG8u9//5sHHniAV155hdmzZ7Nnzx4CAgLIzc0lMjKSG2+8sVYNfvHixbXuV1FRwerVq/n888955JFHWLRoES+88AKdOnViw4YNbNq0ieTk5NYscRUJ8KJDe/iXg3n4l2YGQq01h/JKGTN7SYPnP7hwMw8u3Mz7N46md2wonUIcjHpyMV0jAllxX+3pl3YcLuCDnw5wz5T+tT4QsgrLsHr2ZdUq/1NWVsamTZs466yzAHC5XHTpYobyDhs2jCuuuILp06czffp0r+534YUXAjBy5MiqycSWL1/ObbfdBsCQIUMYNmxY6xbCQwK88BtKKbpGBrH98amkZRfRNTKIZdsz+f1bP9U596K5KwHoHWumhD2YV8q32zPpFRNCZmEZyYmRXPPqjxzILSEm1MGM8b0AWLz1MNe/Vt1PJE0+rawZNe22orVm8ODBrFy5ss6xzz77jGXLlvHJJ5/w2GOPNTgvfE2V0wPXnD74RPV9SoAXfsdhs9AvzqxJc+7QLlVBOK/YybbDBWw9lM9Dn5j/mLsyi6quq/kUbU2Pf7aVvnFhhAbY+N+W2gs45BU7iQiWph1/EhAQQGZmJitXrmT06NE4nU62b9/OwIED2b9/P5MmTWLcuHG8/fbbFBYWEhYWRn5+81ahGjduHO+99x6TJk1iy5YtbNzYys1SHhLgxUkjItjOqJ5RjOoZxfTkBHZlFXLhC94thtxQ8D/l0a8Z1yeGh84fRFZhOYO6hFcF/K82ZxAbFsBV81fzt4tPYcoQM4KnpNzF7C+2ctuZ/cgpKqN7VEjVw1yVNh/MI7+kgtG9o4+jxKIlLBYL77//Prfeeit5eXlUVFRw++23069fP6688kry8vLQWjNr1iwiIyM5//zzueiii1i4cCHPPvusV6/x+9//nquvvpphw4YxfPhwhg0b1irTAx9LhkmKk9r+HDObZdfIICwKFm89wpebM3h/bToAp/WM4u4p/Vm4/mDVGrNN6RwWQFSIg9SMglrpZw2KI7lbJN9uz6zVIWy3KmaM74Vba26f3I8gh7VqWuW02dNwuzVbDuUzJMEEgMr/s5X9As9/s5Pc4nIemDaIvGInwQFW7NbaHxiFZRWEttN5fNrTMMkTxeVy4XQ6CQwMZNeuXUyePJnt27c3Oea+ucMkJcALcQytNakZBRSVVTCwS3jVBGdaa5Zuy6R/fBg2iyIjv5Q1aUdZtiOT3rGhzF++57hfOzTAxtCECFbuzgZgZI9OrN1r5uR59ZpTiQ51cO2rPxISYOOPZ/djV2YRcxbvAOCWM/rw7JKdAATYLHx6yzi6Rwfzxsq9PP7ZVp65NJnb3l3PYxcM5uKUbhSVVbD1UAEBdgunJvluqoeTMcAXFBQwadIknE4nWmueeuoppk6d2uR1EuCF8JFSp4vsonJmf5HK6b2i6BcXxo9pOazfl0tooI1Sp4slqUcodVZPvZDYKYj0oyVtlqewQBsFpXXXBY0JdZBVWF61v+DaU1m1J4eZ43sRGWwnLbsYu1WxMT2PA7klTB3ahYTIIBZtOUx0qIMth/I5WlTOlCHxfLs9i6tH98BW41tDqdOFzaKq0nZnFhIZ7CCqnlFHJ2OAbykJ8EK0c1pr3BqsFtPEUlbhoqDUNKFk5JWSX+rktRV7+eAn00x06andUErxzup9VfcID7SRX0/g9rWZE3oRZLfyzOIdBNmtTOwfS1mFmyWpRwD49JZx/GvpLn49MoHPN2bQLy6U5LAihg8bQlmFm92ZhSR2CiarsAyloEtEEFprtIaQABsWRa0hqzUdLSonr8RJsMNKVIiDA7klhATYiAkNqHOuW2sUDd+rvWpXAV4pNQV4BrACL2utGx0DJQFeiGqH8kqwKEVcuFkk5b01+wmwWRjXJ4bo0AC01mQXlRMd4mDzwXz6x4dR4dKUOl3syylm+c4sXvx2FzdP6kO/uDBe+m43K3ZlM65PDFmFZVw3rifpOcWs25/LdzuysFsVSdEh7M0uptzlrpWXzmEBHCkoa5NyzvtlPHHdenkdbO1WCw6bhaKy6g+4ILuVEmfDa6XarBYqXG5CA2w4bBZyisy3l+jQAMor3ATaLYQH2knLKsKlNVEhDsor3JRVuLFaFGEBNkICzLcwp1sTEWhjX04JFW43UcEOghxWyircBNmthAXZQEOJ00V5hZuwQBtKKZwVboIcVorLzbcbpUyHu0ubD/2oEEej74HWuurJ15p8EuCVUlZgO3AWkA78CFymtd7S0DUS4IXwPbe7shO3dg23vMJNfqmTEIeNtOwiDuWV0LdzGMEOKyEBNvZkFXHjm2s5NSmKyCA704cncCivlMyCMtbvP0qp083OI4WM7h3N3uxiFm09jEXB09MS6JcYS4U9pGrxl07BDo4WmyDssFlAU+dDpyNSStU7Bt5utdA/LgyLpf4Ar7UmOzubgoICevbseew9fRLgRwMPa63P8ezf58nonxu6RgK8ECePo0XlRATZcbkqSE9Pp7S0tM4IIafLXWtEkNYaDVg8gVLV+O1yayo/jxR49s3xCrfGbrVgUeaDyma1UFRegd1iwelyE2C3UFzuIsBmwWqxVDULFZZVEBlsp8Tpwu3WWJTCpTVWi6LM6SbYYcWiFGUVLuxWCyXlLko901vbrQqnSxNgs+Bym3zbLIoyz3GrReH2vE5MqINAe+MzoAYGBpKYmIjdXvu5C19NNpYA7K+xnw6c1sC5QoiTTOU0DxaLvU6t1N+53bqqtl75AdUW2nLJvvpyXOfrglJqplJqjVJqTWZmZhtmRwgh2oeaTTFt2dHblgE+HehWYz8ROHjsSVrreVrrFK11SuUk+0IIIY5fWwb4H4G+SqmeSikHcCnwSRu+nhBCiBraepjkucDTmGGSr2itn2ji/EzAu+fB64oBslp4bUclZT45SJn93/GUt4fWut7mj3b1oNPxUEqtaagn2V9JmU8OUmb/11blbcsmGiGEED4kAV4IIfyUPwX4eb7OgA9ImU8OUmb/1ybl9Zs2eCGEELX5Uw1eCCFEDR0+wCulpiiltimldiql7vV1flqLUqqbUuobpdRWpdRmpdRtnvQopdT/lFI7PL871bjmPs/7sE0pdY7vcn98lFJWpdQ6pdSnnn2/LrNSKlIp9b5SKtXz7z36JCjzLM/f9Sal1DtKqUB/K7NS6hWl1BGl1KYaac0uo1JqpFJqo+fYHNWcR1/NpDod8wczvn4X0AtwAD8Dg3ydr1YqWxdghGc7DDMz5yDgL8C9nvR7gac824M85Q8AenreF6uvy9HCst8BvA186tn36zIDrwEzPNsOINKfy4yZp2oPEOTZfw+4xt/KDEwARgCbaqQ1u4zAamA0ZvqXL4Cp3uaho9fgRwE7tda7tdblwLvABT7OU6vQWh/SWv/k2S4AtmL+Y1yACQh4fk/3bF8AvKu1LtNa7wF2Yt6fDkUplQhMA16ukey3ZVZKhWMCwXwArXW51joXPy6zhw0IUkrZgGDMNCZ+VWat9TIg55jkZpVRKdUFCNdar9Qm2r9e45omdfQAX9+MlQk+ykubUUolAcOBVUCc1voQmA8BoLPnNH95L54G7gZqTv7tz2XuBWQCr3qapV5WSoXgx2XWWh8A/gbsAw4BeVrrr/HjMtfQ3DImeLaPTfdKRw/wXs1Y2ZEppUKBD4Dbtdb5jZ1aT1qHei+UUucBR7TWa729pJ60DlVmTE12BPAvrfVwoAjz1b0hHb7MnnbnCzBNEV2BEKXUlY1dUk9ahyqzFxoq43GVvaMHeK9mrOyolFJ2THB/S2v9oSf5sOdrG57fRzzp/vBejAV+qZRKwzS3naGUehP/LnM6kK61XuXZfx8T8P25zGcCe7TWmVprJ/AhMAb/LnOl5pYx3bN9bLpXOnqA99sZKz095fOBrVrrf9Q49AlwtWf7amBhjfRLlVIBSqmeQF9M50yHobW+T2udqLVOwvxbLtFaX4l/lzkD2K+U6u9JmgxswY/LjGmaOV0pFez5O5+M6WPy5zJXalYZPc04BUqp0z3v1VU1rmmar3uaW6Gn+lzMCJNdwAO+zk8rlmsc5qvYBmC95+dcIBpYDOzw/I6qcc0DnvdhG83oaW+PP8BEqkfR+HWZgWRgjeff+mOg00lQ5keAVGAT8AZm9IhflRl4B9PH4MTUxK9vSRmBFM/7tAt4Ds8Dqt78yJOsQgjhpzp6E40QQogGSIAXQgg/JQFeCCH8lAR4IYTwUxLghRDCT0mAFycVpZRLKbW+xk+rzUCqlEqqOXOgEL5m83UGhDjBSrTWyb7OhBAngtTghQCUUmlKqaeUUqs9P3086T2UUouVUhs8v7t70uOUUh8ppX72/Izx3MqqlHrJM9f510qpIJ8VSpz0JMCLk03QMU00l9Q4lq+1HoV5WvBpT9pzwOta62HAW8AcT/oc4Fut9SmYuWM2e9L7As9rrQcDucCv27Q0QjRCnmQVJxWlVKHWOrSe9DTgDK31bs8kbxla62ilVBbQRWvt9KQf0lrHKKUygUStdVmNeyQB/9Na9/Xs3wPYtdaPn4CiCVGH1OCFqKYb2G7onPqU1dh2If1cwockwAtR7ZIav1d6tldgZrYEuAJY7tleDNwEVWvIhp+oTArhLaldiJNNkFJqfY39L7XWlUMlA5RSqzAVn8s8abcCryil7sKsvHStJ/02YJ5S6npMTf0mzMyBQrQb0gYvBFVt8Cla6yxf50WI1iJNNEII4aekBi+EEH5KavBCCOGnJMALIYSfkgAvhBB+SgK8EEL4KQnwQgjhpyTACyGEn/p/01CfYgmLGY4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.plot(history_accuracy_train)\n",
    "plt.ylabel('Train Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.plot(history_accuracy_test)\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.legend(['Training','Testing'],loc='lower right') \n",
    "\n",
    "plt.figure(2)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(history_loss_train)\n",
    "plt.ylabel('Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(history_loss_test)\n",
    "plt.ylabel('Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.legend(['Training','Testing'],loc='lower right') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031720c2-16f7-486c-83e0-d52cd42661ac",
   "metadata": {},
   "source": [
    "---\n",
    "# Memory vs Accuracy Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "724ebe19-73b6-4aeb-b506-f24d6722ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_lens = []\n",
    "final_accuracies_train = []\n",
    "final_accuracies_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cc71d008-1f06-4d2b-a8b4-0ed7fb8603d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_lens = np.loadtxt(\"/home/jovyan/TransformerXL/XL/Mem_Lens.txt\", dtype=np.float64)\n",
    "final_accuracies_train =  np.loadtxt(\"/home/jovyan/TransformerXL/XL/Final_Accuracies_Train.txt\", dtype=np.float64)\n",
    "final_accuracies_test =  np.loadtxt(\"/home/jovyan/TransformerXL/XL/Final_Accuracies_Test.txt\", dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6b5293b4-e6a7-4fbb-a40a-512e965573c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10.  40.  40. 100.  40.  40.]\n",
      "[0.927778   1.         1.         1.         1.         0.94875622]\n",
      "[0.188889   0.091156   0.091156   0.08429    0.108081   0.06936416]\n"
     ]
    }
   ],
   "source": [
    "mem_lens = np.append(mem_lens, memory_length)\n",
    "final_accuracies_train = np.append(final_accuracies_train, accuracy_train)\n",
    "final_accuracies_test = np.append(final_accuracies_test, accuracy_test)\n",
    "print(mem_lens)\n",
    "print(final_accuracies_train)\n",
    "print(final_accuracies_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "779262e0-716e-431b-9e95-464bb76b8f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_accuracy_train = ({mem_len:accuracy_train for mem_len, accuracy_train in zip(mem_lens, final_accuracies_train)})\n",
    "mem_accuracy_test = ({mem_len:accuracy_test for mem_len, accuracy_test in zip(mem_lens, final_accuracies_test)})\n",
    "\n",
    "mem_lens_train_sorted = np.array([l for l in sorted(mem_accuracy_train)])\n",
    "mem_lens_test_sorted = np.array([l for l in sorted(mem_accuracy_test)])\n",
    "\n",
    "final_accuracies_train_sorted = np.array([mem_accuracy_train[l] for l in mem_lens_train_sorted])\n",
    "final_accuracies_test_sorted = np.array([mem_accuracy_test[l] for l in mem_lens_test_sorted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9358244f-1349-4be4-bc01-cf7298f784d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('/home/jovyan/TransformerXL/XL/Mem_Lens.txt', (mem_lens), fmt='%f', delimiter='\\n')\n",
    "np.savetxt('/home/jovyan/TransformerXL/XL//Final_Accuracies_Train.txt', (final_accuracies_train), fmt='%f', delimiter='\\n')\n",
    "np.savetxt('/home/jovyan/TransformerXL/XL//Final_Accuracies_Test.txt', (final_accuracies_test), fmt='%f', delimiter='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e931bbd-a1a9-4098-a4f2-0ef29f7fd89c",
   "metadata": {},
   "source": [
    "---\n",
    "# Memory vs Accuracy Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ba942651-37b0-465e-b3c0-5dd22621b61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(2)\n",
    "plt.subplot(222)\n",
    "plt.plot(mem_lens_train_sorted, final_accuracies_train_sorted)\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(mem_lens_test_sorted, final_accuracies_test_sorted)\n",
    "plt.xlabel('Memory Length')\n",
    "\n",
    "plt.legend(['Training','Testing'],loc='lower right') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91287ca0-3ff6-4d16-a8fe-837dcd5bf883",
   "metadata": {},
   "source": [
    "---\n",
    "# Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e20aaf95-5aa2-4a44-8373-6eb89dc1fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, pre_y_train, post_y_train = batch_train = Full_Batch(batch_size, x_flattened_train, pre_y_flattened_train, post_y_flattened_train, num_chars_data_train, seq_len, rng)\n",
    "x_test, pre_y_test, post_y_test = Full_Batch(batch_size, x_flattened_test, pre_y_flattened_test, post_y_flattened_test, num_chars_data_test, seq_len, rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a8efcd49-a96f-46f5-b595-14d383c8ca64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a brreleo u daredsa  Mh. Bi gvey will es verysuddenly addrelsed her withetake melight in vexing me.tYou have no compassion for my poor nervys.\"However little  nown the feelings ornviews of such a manm\n",
      "\n",
      "\n",
      ", surely. I dare say Mr. Bingley will be verysuddenly addressed her with:take delight in vexing me. You have no compassion for my poor nerves.\"However little known the feelings or views of such a man \n"
     ]
    }
   ],
   "source": [
    "train = model.predict(x_train)\n",
    "train = tf.argmax(train, -1)\n",
    "train = decode_seq(train[0], i_to_c_pandp)\n",
    "target = decode_seq(post_y_train[0], i_to_c_pandp)\n",
    "print(train)\n",
    "print('\\n')\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b1ea5909-94e5-4b2b-bb58-47cdef22ce7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e o yhen  siohedhed . o mehnet ara llehnl nctyoed isnt is n  itfiher hiu tsmictosooes o ou nae thuthheos l t ad oet ytthlt  s.he_rty .egrte thghee i geellet.\"ihohtto thwthetesonat  i ner ol eor s iori\n",
      "\n",
      "\n",
      "ver it, and live to see many young men of four\"How so? How can it affect them?\"not depend on her serving you.\"told me all about it.\"of a good fortune, must be in want of a wife.\n"
     ]
    }
   ],
   "source": [
    "test = model.predict(x_test)\n",
    "test = tf.argmax(test, -1)\n",
    "test = decode_seq(test[0], i_to_c_pandp)\n",
    "target = decode_seq(post_y_test[0], i_to_c_pandp)\n",
    "print(test)\n",
    "print('\\n')\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090f0d47-26ac-4392-be6e-eabb3f0bac07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b42dbc-1398-488b-9222-13ba7aa5a006",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
