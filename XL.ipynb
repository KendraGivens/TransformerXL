{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3dcad43d-67f7-49b9-84ce-6d91a0f468a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5e7cb9df-5746-44a3-998a-b671b284957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fda64db1-8058-4e15-b2d4-1a3e64f23fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "import math\n",
    "import string\n",
    "\n",
    "import tf_utils as tfu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0dfb3b1a-2db7-4463-a208-b5d29e20f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tfu.strategy.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45d5d65-b728-42de-b529-85643adef96d",
   "metadata": {},
   "source": [
    "# Vanilla Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9a199f10-268d-4153-863c-2d430a4d2c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedTransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(MaskedTransformerBlock, self).__init__()\n",
    "        self.att1 = keras.layers.MultiHeadAttention(num_heads=num_heads,\n",
    "        key_dim=embed_dim)\n",
    "        self.att2 = keras.layers.MultiHeadAttention(num_heads=num_heads,\n",
    "        key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "        [keras.layers.Dense(ff_dim, activation=\"gelu\"),\n",
    "        keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "        self.dropout3 = keras.layers.Dropout(rate)\n",
    "    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n",
    "        i = tf.range(n_dest)[:, None]\n",
    "        j = tf.range(n_src)\n",
    "        m = i >= j - n_src + n_dest\n",
    "        mask = tf.cast(m, dtype)\n",
    "        mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "        mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "    def call(self, inputs, training):\n",
    "        input_shape = tf.shape(inputs[0])\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        mask = self.causal_attention_mask(batch_size,\n",
    "        seq_len, seq_len,\n",
    "        tf.bool)\n",
    "        attn_output1 = self.att1(inputs[0], inputs[0],\n",
    "        attention_mask = mask)\n",
    "        attn_output1 = self.dropout1(attn_output1, training=training)\n",
    "        out1 = self.layernorm1(inputs[0] + attn_output1)\n",
    "        attn_output2 = self.att2(out1, inputs[1])\n",
    "        attn_output2 = self.dropout2(attn_output2, training=training)\n",
    "        out2 = self.layernorm1(out1 + attn_output2)\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        return self.layernorm2(out2 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a27a7c63-8d17-4e49-bfd7-4e05d92eb915",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedTokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim, **kwargs):\n",
    "        super(MaskedTokenAndPositionEmbedding, self).__init__(**kwargs)\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size,\n",
    "        output_dim=embed_dim,\n",
    "        mask_zero=True)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen+1,\n",
    "        output_dim=embed_dim,\n",
    "        mask_zero=True)\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=1, limit=maxlen+1, delta=1)\n",
    "        positions = positions * tf.cast(tf.sign(x),tf.int32)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "160d05e4-b851-4348-85df-508526fb154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "def MaskedSparseCategoricalCrossentropy(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "def MaskedSparseCategoricalAccuracy(real, pred):\n",
    "    accuracies = tf.equal(tf.cast(real,tf.int64), tf.argmax(pred, axis=2))\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b261be2-4cbf-4b92-ae0b-de1f6d86226f",
   "metadata": {},
   "source": [
    "---\n",
    "# TF Official Port"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b66c98-f8b5-4b80-9736-66a9769cc966",
   "metadata": {},
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9af4ac04-1eab-4d74-bfc0-716408efeed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_initializer(initializer):\n",
    "    if isinstance(initializer, tf.keras.initializers.Initializer):\n",
    "        return initializer.__class__.from_config(initializer.get_config())\n",
    "    return initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aadfeaaa-08b0-4155-8381-328ac71f4c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape_list(tensor, expected_rank=None, name=None):\n",
    "    if expected_rank is not None:\n",
    "        assert_rank(tensor, expected_rank, name)\n",
    "\n",
    "    shape = tensor.shape.as_list()\n",
    "\n",
    "    non_static_indexes = []\n",
    "    for (index, dim) in enumerate(shape):\n",
    "        if dim is None:\n",
    "            non_static_indexes.append(index)\n",
    "\n",
    "    if not non_static_indexes:\n",
    "        return shape\n",
    "\n",
    "    dyn_shape = tf.shape(tensor)\n",
    "    for index in non_static_indexes:\n",
    "        shape[index] = dyn_shape[index]\n",
    "    return shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835c39dd-9fb5-49bf-8763-14ca694a1a35",
   "metadata": {},
   "source": [
    "---\n",
    "# Cache Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "da4cb5e4-2d26-4e40-a00f-3e71f470b273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_state: `Tensor`, the current state.\n",
    "# previous_state: `Tensor`, the previous state.\n",
    "# memory_length: `int`, the number of tokens to cache.\n",
    "# reuse_length: `int`, the number of tokens in the current batch to be cached\n",
    "#     and reused in the future.\n",
    "# Returns:  `Tensor`, representing the cached state with stopped gradients.\n",
    "def _cache_memory(current_state, previous_state, memory_length, reuse_length=0):\n",
    "    if memory_length is None or memory_length == 0:\n",
    "        return None\n",
    "    else:\n",
    "        if reuse_length > 0:\n",
    "            current_state = current_state[:, :reuse_length, :]\n",
    "\n",
    "        if previous_state is None:\n",
    "            new_mem = current_state[:, -memory_length:, :]\n",
    "        else:\n",
    "            new_mem = tf.concat(\n",
    "                    [previous_state, current_state], 1)[:, -memory_length:, :]\n",
    "\n",
    "    return tf.stop_gradient(new_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3126b9-a96e-4d6a-ae65-3c9579514cf1",
   "metadata": {},
   "source": [
    "---\n",
    "# MultiHead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5b32faad-fd06-4448-b817-b3a1f5943aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query: Query `Tensor` of shape `[B, T, dim]`.\n",
    "# value: Value `Tensor` of shape `[B, S, dim]`.\n",
    "# content_attention_bias: Bias `Tensor` for content based attention of shape\n",
    "# `[num_heads, dim]`.\n",
    "# positional_attention_bias: Bias `Tensor` for position based attention of\n",
    "# shape `[num_heads, dim]`.\n",
    "# key: Optional key `Tensor` of shape `[B, S, dim]`. If not given, will use\n",
    "# `value` for both `key` and `value`, which is the most common case.\n",
    "# relative_position_encoding: Relative positional encoding `Tensor` of shape\n",
    "# `[B, L, dim]`.\n",
    "# state: Optional `Tensor` of shape `[B, M, E]` where M is the length of the\n",
    "# state or memory. If passed, this is also attended over as in Transformer\n",
    "# XL.\n",
    "# attention_mask: A boolean mask of shape `[B, T, S]` that prevents attention\n",
    "# to certain positions.\n",
    "class MultiHeadRelativeAttention(tf.keras.layers.MultiHeadAttention):\n",
    "    def __init__(self,\n",
    "                 kernel_initializer=\"variance_scaling\",\n",
    "                 **kwargs):\n",
    "        super().__init__(kernel_initializer=kernel_initializer,\n",
    "                                         **kwargs)\n",
    "\n",
    "    def _build_from_signature(self, query, value, key=None):\n",
    "        super(MultiHeadRelativeAttention, self)._build_from_signature(\n",
    "                query=query,\n",
    "                value=value,\n",
    "                key=key)\n",
    "        if hasattr(value, \"shape\"):\n",
    "            value_shape = tf.TensorShape(value.shape)\n",
    "        else:\n",
    "            value_shape = value\n",
    "        if key is None:\n",
    "            key_shape = value_shape\n",
    "        elif hasattr(key, \"shape\"):\n",
    "            key_shape = tf.TensorShape(key.shape)\n",
    "        else:\n",
    "            key_shape = key\n",
    "\n",
    "        common_kwargs = dict(\n",
    "                kernel_initializer=self._kernel_initializer,\n",
    "                bias_initializer=self._bias_initializer,\n",
    "                kernel_regularizer=self._kernel_regularizer,\n",
    "                bias_regularizer=self._bias_regularizer,\n",
    "                activity_regularizer=self._activity_regularizer,\n",
    "                kernel_constraint=self._kernel_constraint,\n",
    "                bias_constraint=self._bias_constraint)\n",
    "\n",
    "        with tf.init_scope():\n",
    "            einsum_equation, _, output_rank = _build_proj_equation(\n",
    "                    key_shape.rank - 1, bound_dims=1, output_dims=2)\n",
    "            self._encoding_dense = tf.keras.layers.experimental.EinsumDense(\n",
    "                    einsum_equation,\n",
    "                    output_shape=_get_output_shape(\n",
    "                        output_rank - 1,\n",
    "                        [self._num_heads, self._key_dim]),\n",
    "                        bias_axes=None,\n",
    "                        name=\"encoding\",\n",
    "                        **common_kwargs)\n",
    "\n",
    "# query: Projected query `Tensor` of shape `[B, T, N, key_dim]`.\n",
    "# key: Projected key `Tensor` of shape `[B, S + M, N, key_dim]`.\n",
    "# value: Projected value `Tensor` of shape `[B, S + M, N, key_dim]`.\n",
    "# position: Projected position `Tensor` of shape `[B, L, N, key_dim]`.\n",
    "# content_attention_bias: Trainable bias parameter added to the query head\n",
    "#     when calculating the content-based attention score.\n",
    "# positional_attention_bias: Trainable bias parameter added to the query\n",
    "#     head when calculating the position-based attention score.\n",
    "# attention_mask: (default None) Optional mask that is added to attention\n",
    "#     logits. If state is not None, the mask source sequence dimension should\n",
    "#     extend M.\n",
    "# Returns:\n",
    "# attention_output: Multi-headed output of attention computation of shape\n",
    "#     `[B, S, N, key_dim]`.\n",
    "    def compute_attention(\n",
    "        self,\n",
    "        query,\n",
    "        key,\n",
    "        value,\n",
    "        position,\n",
    "        content_attention_bias,\n",
    "        positional_attention_bias,\n",
    "        attention_mask=None\n",
    "    ):\n",
    "        #AC\n",
    "        content_attention = tf.einsum(self._dot_product_equation, key, query + content_attention_bias)\n",
    "        \n",
    "        positional_attention = tf.einsum(self._dot_product_equation, position, query + positional_attention_bias)\n",
    "        \n",
    "        #BD\n",
    "        positional_attention = _rel_shift(positional_attention, klen=tf.shape(content_attention)[3])\n",
    "        \n",
    "        attention_sum = content_attention + positional_attention\n",
    "\n",
    "        attention_scores = tf.multiply(attention_sum, 1.0 / math.sqrt(float(self._key_dim)))\n",
    "\n",
    "        attention_scores = self._masked_softmax(attention_scores, attention_mask)\n",
    "\n",
    "        attention_output = self._dropout_layer(attention_scores)\n",
    "\n",
    "        attention_output = tf.einsum(self._combine_equation,\n",
    "                                                                 attention_output,\n",
    "                                                                 value)\n",
    "        return attention_output\n",
    "\n",
    "# * Number of heads (H): the number of attention heads.\n",
    "# * Value size (V): the size of each value embedding per head.\n",
    "# * Key size (K): the size of each key embedding per head. Equally, the size\n",
    "#     of each query embedding per head. Typically K <= V.\n",
    "# * Batch dimensions (B).\n",
    "# * Query (target) attention axes shape (T).\n",
    "# * Value (source) attention axes shape (S), the rank must match the target.\n",
    "# * Encoding length (L): The relative positional encoding length.\n",
    "# query: attention input.\n",
    "# value: attention input.\n",
    "# content_attention_bias: A trainable bias parameter added to the query head\n",
    "#     when calculating the content-based attention score.\n",
    "# positional_attention_bias: A trainable bias parameter added to the query\n",
    "#     head when calculating the position-based attention score.\n",
    "# key: attention input.\n",
    "# relative_position_encoding: relative positional encoding for key and\n",
    "#     value.\n",
    "# state: (default None) optional state. If passed, this is also attended\n",
    "#     over as in TransformerXL.\n",
    "# attention_mask: (default None) Optional mask that is added to attention\n",
    "#     logits. If state is not None, the mask source sequence dimension should\n",
    "#     extend M.\n",
    "# Returns:\n",
    "# attention_output: The result of the computation, of shape [B, T, E],\n",
    "#     where `T` is for target sequence shapes and `E` is the query input last\n",
    "#     dimension if `output_shape` is `None`. Otherwise, the multi-head outputs\n",
    "#     are projected to the shape specified by `output_shape`.\n",
    "    def call(self,\n",
    "             query,\n",
    "             value,\n",
    "             content_attention_bias,\n",
    "             positional_attention_bias,\n",
    "             key=None,\n",
    "             relative_position_encoding=None,\n",
    "             state=None,\n",
    "             attention_mask=None):\n",
    "        \n",
    "        if not self._built_from_signature:\n",
    "            self._build_from_signature(query, value, key=key)\n",
    "        if key is None:\n",
    "            key = value\n",
    "        if state is not None and state.shape.ndims > 1:\n",
    "            value = tf.concat([state, value], 1)\n",
    "            key = tf.concat([state, key], 1)\n",
    "\n",
    "        # `query` = [B, T, N ,H]\n",
    "        query = self._query_dense(query)\n",
    "\n",
    "        # `key` = [B, S + M, N, H]\n",
    "        key = self._key_dense(key)\n",
    "\n",
    "        # `value` = [B, S + M, N, H]\n",
    "        value = self._value_dense(value)\n",
    "\n",
    "        # `position` = [B, L, N, H]\n",
    "        position = self._encoding_dense(relative_position_encoding)\n",
    "\n",
    "        attention_output = self.compute_attention(\n",
    "                query=query,\n",
    "                key=key,\n",
    "                value=value,\n",
    "                position=position,\n",
    "                content_attention_bias=content_attention_bias,\n",
    "                positional_attention_bias=positional_attention_bias,\n",
    "                attention_mask=attention_mask)\n",
    "\n",
    "        # `attention_output` = [B, S, N, H]\n",
    "        attention_output = self._output_dense(attention_output)\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95abdafa-2c98-4c04-b9e8-aae23e53265e",
   "metadata": {},
   "source": [
    "---\n",
    "# Relative Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cc8c99a8-3755-48f9-9704-c1dbd04e0ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rel_shift(x, klen=-1):    \n",
    "    x = tf.transpose(x, perm=[2, 3, 0, 1])\n",
    "    x = tf.pad(x, [[0, 0], [1, 0], [0, 0], [0, 0]])\n",
    "    x_size = tf.shape(x)\n",
    "    x = tf.reshape(x, [x_size[1], x_size[0], x_size[2], x_size[3]])\n",
    "    x = tf.slice(x, [1, 0, 0, 0], [-1, -1, -1, -1])\n",
    "    x = tf.reshape(x, [x_size[0], x_size[1] - 1, x_size[2], x_size[3]])\n",
    "    x = tf.transpose(x, perm=[2, 3, 0, 1])\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a026fd-2cbe-45b7-88f7-a3ffd3548806",
   "metadata": {},
   "source": [
    "---\n",
    "# Build Einsum Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5102422c-82af-49d4-a8d5-5081262b21e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_CHR_IDX = string.ascii_lowercase\n",
    "\n",
    "# Builds an einsum equation for projections inside multi-head attention\n",
    "def _build_proj_equation(free_dims, bound_dims, output_dims):\n",
    "    input_str = \"\"\n",
    "    kernel_str = \"\"\n",
    "    output_str = \"\"\n",
    "    bias_axes = \"\"\n",
    "    letter_offset = 0\n",
    "    for i in range(free_dims):\n",
    "        char = _CHR_IDX[i + letter_offset]\n",
    "        input_str += char\n",
    "        output_str += char\n",
    "\n",
    "    letter_offset += free_dims\n",
    "    for i in range(bound_dims):\n",
    "        char = _CHR_IDX[i + letter_offset]\n",
    "        input_str += char\n",
    "        kernel_str += char\n",
    "\n",
    "    letter_offset += bound_dims\n",
    "    for i in range(output_dims):\n",
    "        char = _CHR_IDX[i + letter_offset]\n",
    "        kernel_str += char\n",
    "        output_str += char\n",
    "        bias_axes += char\n",
    "    equation = \"%s,%s->%s\" % (input_str, kernel_str, output_str)\n",
    "\n",
    "    return equation, bias_axes, len(output_str)\n",
    "\n",
    "\n",
    "def _get_output_shape(output_rank, known_last_dims):\n",
    "    return [None] * (output_rank - len(known_last_dims)) + list(known_last_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5343f3e3-8536-46d5-9d1b-28f8c3f7908b",
   "metadata": {},
   "source": [
    "---\n",
    "# Relative Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a5f5eeac-4c07-4796-8281-f5c039fe7c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_size: Size of the hidden layer.\n",
    "# min_timescale: Minimum scale that will be applied at each position\n",
    "# max_timescale: Maximum scale that will be applied at each position.\n",
    "class RelativePositionEmbedding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 hidden_size: int,\n",
    "                 min_timescale: float = 1.0,\n",
    "                 max_timescale: float = 1.0e4,\n",
    "                 **kwargs):\n",
    "        \n",
    "        if \"dtype\" not in kwargs:\n",
    "            kwargs[\"dtype\"] = \"float32\"\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self._hidden_size = hidden_size\n",
    "        self._min_timescale = min_timescale\n",
    "        self._max_timescale = max_timescale\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "                \"hidden_size\": self._hidden_size,\n",
    "                \"min_timescale\": self._min_timescale,\n",
    "                \"max_timescale\": self._max_timescale,\n",
    "        }\n",
    "        base_config = super(RelativePositionEmbedding, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "# inputs: An tensor whose second dimension will be used as `length`. If\n",
    "# `None`, the other `length` argument must be specified.\n",
    "# length: An optional integer specifying the number of positions. If both\n",
    "# `inputs` and `length` are spcified, `length` must be equal to the second\n",
    "# dimension of `inputs`.\n",
    "# Returns:\n",
    "# A tensor in shape of `(length, hidden_size)`.\n",
    "    def call(self, inputs, length=None):\n",
    "        if inputs is None and length is None:\n",
    "            raise ValueError(\"If inputs is None, `length` must be set in \"\n",
    "                                             \"RelativePositionEmbedding().\")\n",
    "        if inputs is not None:\n",
    "            input_shape = get_shape_list(inputs)\n",
    "            if length is not None and length != input_shape[1]:\n",
    "                raise ValueError(\n",
    "                        \"If inputs is not None, `length` must equal to input_shape[1].\")\n",
    "            length = input_shape[1]\n",
    "        position = tf.cast(tf.range(length), tf.float32)\n",
    "        num_timescales = self._hidden_size // 2\n",
    "        min_timescale, max_timescale = self._min_timescale, self._max_timescale\n",
    "        log_timescale_increment = (\n",
    "                math.log(float(max_timescale) / float(min_timescale)) /\n",
    "                (tf.cast(num_timescales, tf.float32) - 1))\n",
    "        inv_timescales = min_timescale * tf.exp(\n",
    "                tf.cast(tf.range(num_timescales), tf.float32) *\n",
    "                -log_timescale_increment)\n",
    "        scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(\n",
    "                inv_timescales, 0)\n",
    "        position_embeddings = tf.concat(\n",
    "                [tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n",
    "        position_embeddings = tf.expand_dims(position_embeddings, axis=0)\n",
    "        return tf.tile(position_embeddings, (tf.shape(inputs)[0], 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fdbdfc2d-f593-4466-a981-c8e7b3b06e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size: The size of the token vocabulary.\n",
    "# hidden_size: The size of the transformer hidden layers.\n",
    "# num_attention_heads: The number of attention heads.\n",
    "# head_size: The dimension size of each attention head.\n",
    "# inner_size: The inner size for the transformer layers.\n",
    "# dropout_rate: Dropout rate for the output of this layer.\n",
    "# attention_dropout_rate: Dropout rate on attention probabilities.\n",
    "# norm_epsilon: Epsilon value to initialize normalization layers.\n",
    "# inner_activation: The activation to use for the inner\n",
    "# FFN layers.\n",
    "# kernel_initializer: Initializer for dense layer kernels.\n",
    "# inner_dropout: Dropout probability for the inner dropout\n",
    "# layer.\n",
    "class TransformerXLBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 hidden_size,\n",
    "                 num_attention_heads,\n",
    "                 head_size,\n",
    "                 inner_size,\n",
    "                 dropout_rate,\n",
    "                 attention_dropout_rate,\n",
    "                 norm_epsilon=1e-12,\n",
    "                 inner_activation=\"relu\",\n",
    "                 kernel_initializer=\"variance_scaling\",\n",
    "                 inner_dropout=0.0,\n",
    "                 **kwargs):\n",
    "        \"\"\"Initializes TransformerXLBlock layer.\"\"\"\n",
    "\n",
    "        super(TransformerXLBlock, self).__init__(**kwargs)\n",
    "        self._vocab_size = vocab_size\n",
    "        self._num_heads = num_attention_heads\n",
    "        self._head_size = head_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._inner_size = inner_size\n",
    "        self._dropout_rate = dropout_rate\n",
    "        self._attention_dropout_rate = attention_dropout_rate\n",
    "        self._inner_activation = inner_activation\n",
    "        self._norm_epsilon = norm_epsilon\n",
    "        self._kernel_initializer = kernel_initializer\n",
    "        self._inner_dropout = inner_dropout\n",
    "        self._attention_layer_type = MultiHeadRelativeAttention\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_tensor = input_shape[0] if len(input_shape) == 2 else input_shape\n",
    "        input_tensor_shape = tf.TensorShape(input_tensor)\n",
    "        if len(input_tensor_shape.as_list()) != 3:\n",
    "            raise ValueError(\"TransformerLayer expects a three-dimensional input of \"\n",
    "                                             \"shape [batch, sequence, width].\")\n",
    "        batch_size, sequence_length, hidden_size = input_tensor_shape\n",
    "\n",
    "        if len(input_shape) == 2:\n",
    "            mask_tensor_shape = tf.TensorShape(input_shape[1])\n",
    "            expected_mask_tensor_shape = tf.TensorShape(\n",
    "                    [batch_size, sequence_length, sequence_length])\n",
    "            if not expected_mask_tensor_shape.is_compatible_with(mask_tensor_shape):\n",
    "                raise ValueError(\"When passing a mask tensor to TransformerXLBlock, \"\n",
    "                                                 \"the mask tensor must be of shape [batch, \"\n",
    "                                                 \"sequence_length, sequence_length] (here %s). Got a \"\n",
    "                                                 \"mask tensor of shape %s.\" %\n",
    "                                                 (expected_mask_tensor_shape, mask_tensor_shape))\n",
    "        if hidden_size % self._num_heads != 0:\n",
    "            raise ValueError(\n",
    "                    \"The input size (%d) is not a multiple of the number of attention \"\n",
    "                    \"heads (%d)\" % (hidden_size, self._num_heads))\n",
    "        self._attention_layer = self._attention_layer_type(\n",
    "                num_heads=self._num_heads,\n",
    "                key_dim=self._head_size,\n",
    "                value_dim=self._head_size,\n",
    "                dropout=self._attention_dropout_rate,\n",
    "                use_bias=False,\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer),\n",
    "                name=\"rel_attn\")\n",
    "        self._attention_dropout = tf.keras.layers.Dropout(\n",
    "                rate=self._attention_dropout_rate)\n",
    "        self._attention_layer_norm = tf.keras.layers.LayerNormalization(\n",
    "                name=\"self_attention_layer_norm\",\n",
    "                axis=-1,\n",
    "                epsilon=self._norm_epsilon,\n",
    "                dtype=tf.float32)\n",
    "        self._inner_dense = tf.keras.layers.experimental.EinsumDense(\n",
    "                \"abc,cd->abd\",\n",
    "                output_shape=(None, self._inner_size),\n",
    "                bias_axes=\"d\",\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer),\n",
    "                name=\"inner\")\n",
    "\n",
    "        self._inner_activation_layer = tf.keras.layers.Activation(\n",
    "                self._inner_activation)\n",
    "        self._inner_dropout_layer = tf.keras.layers.Dropout(\n",
    "                rate=self._inner_dropout)\n",
    "        self._output_dense = tf.keras.layers.experimental.EinsumDense(\n",
    "                \"abc,cd->abd\",\n",
    "                output_shape=(None, hidden_size),\n",
    "                bias_axes=\"d\",\n",
    "                name=\"output\",\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer))\n",
    "        self._output_dropout = tf.keras.layers.Dropout(rate=self._dropout_rate)\n",
    "        self._output_layer_norm = tf.keras.layers.LayerNormalization(\n",
    "                name=\"output_layer_norm\",\n",
    "                axis=-1,\n",
    "                epsilon=self._norm_epsilon)\n",
    "\n",
    "        super(TransformerXLBlock, self).build(input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "                \"vocab_size\":\n",
    "                        self._vocab_size,\n",
    "                \"hidden_size\":\n",
    "                        self._hidden_size,\n",
    "                \"num_attention_heads\":\n",
    "                        self._num_heads,\n",
    "                \"head_size\":\n",
    "                        self._head_size,\n",
    "                \"inner_size\":\n",
    "                        self._inner_size,\n",
    "                \"dropout_rate\":\n",
    "                        self._dropout_rate,\n",
    "                \"attention_dropout_rate\":\n",
    "                        self._attention_dropout_rate,\n",
    "                \"norm_epsilon\":\n",
    "                        self._norm_epsilon,\n",
    "                \"inner_activation\":\n",
    "                        self._inner_activation,\n",
    "                \"kernel_initializer\":\n",
    "                        self._kernel_initializer,\n",
    "                \"inner_dropout\":\n",
    "                        self._inner_dropout,\n",
    "        }\n",
    "        base_config = super(TransformerXLBlock, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    \n",
    "    # content_stream: `Tensor`, the input content stream. This is the standard\n",
    "    # input to Transformer XL and is commonly referred to as `h` in XLNet.\n",
    "    # content_attention_bias: Bias `Tensor` for content based attention of shape\n",
    "    # `[num_heads, dim]`.\n",
    "    # positional_attention_bias: Bias `Tensor` for position based attention of\n",
    "    # shape `[num_heads, dim]`.\n",
    "    # relative_position_encoding: Relative positional encoding `Tensor` of shape\n",
    "    # `[B, L, dim]`.\n",
    "    # state: Optional `Tensor` of shape `[B, M, E]`, where M is the length of\n",
    "    # the state or memory. If passed, this is also attended over as in\n",
    "    # Transformer XL.\n",
    "    # content_attention_mask: Optional `Tensor` representing the mask that is\n",
    "    # added to content attention logits. If state is not None, the mask source\n",
    "    # sequence dimension should extend M.\n",
    "    # query_attention_mask: Optional `Tensor` representing the mask that is\n",
    "    # added to query attention logits. If state is not None, the mask source\n",
    "    # sequence dimension should extend M.\n",
    "    # target_mapping: Optional `Tensor` representing the target mapping when\n",
    "    # calculating query attention.\n",
    "    # Returns:\n",
    "    # A `dict` object, containing the key value pairs for `content_attention`\n",
    "    def call(self,\n",
    "             content_stream,\n",
    "             content_attention_bias,\n",
    "             positional_attention_bias,\n",
    "             relative_position_encoding=None,\n",
    "             state=None,\n",
    "             content_attention_mask=None,\n",
    "             query_attention_mask=None,\n",
    "             target_mapping=None):\n",
    "\n",
    "        attention_kwargs = dict(\n",
    "                query=content_stream,\n",
    "                value=content_stream,\n",
    "                key=content_stream,\n",
    "                attention_mask=content_attention_mask)\n",
    "\n",
    "        common_attention_kwargs = dict(\n",
    "                content_attention_bias=content_attention_bias,\n",
    "                relative_position_encoding=relative_position_encoding,\n",
    "                positional_attention_bias=positional_attention_bias,\n",
    "                state=state)\n",
    "\n",
    "        attention_kwargs.update(common_attention_kwargs)\n",
    "        attention_output = self._attention_layer(**attention_kwargs)\n",
    "\n",
    "        attention_stream = attention_output\n",
    "        input_stream = content_stream\n",
    "        attention_key = \"content_attention\"\n",
    "        attention_output = {}\n",
    "        \n",
    "        attention_stream = self._attention_dropout(attention_stream)\n",
    "        attention_stream = self._attention_layer_norm(\n",
    "                attention_stream + input_stream)\n",
    "        inner_output = self._inner_dense(attention_stream)\n",
    "        inner_output = self._inner_activation_layer(\n",
    "                inner_output)\n",
    "        inner_output = self._inner_dropout_layer(\n",
    "                inner_output)\n",
    "        layer_output = self._output_dense(inner_output)\n",
    "        layer_output = self._output_dropout(layer_output)\n",
    "        layer_output = self._output_layer_norm(layer_output + attention_stream)\n",
    "        attention_output[attention_key] = layer_output\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c32bb5-4520-4802-a8f0-5f7849905a83",
   "metadata": {},
   "source": [
    "---\n",
    "# XL Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7245b7dd-a3b2-464a-9f5b-7efd4a9d0ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size: The size of the token vocabulary.\n",
    "# hidden_size: The size of the transformer hidden layers.\n",
    "# num_attention_heads: The number of attention heads.\n",
    "# head_size: The dimension size of each attention head.\n",
    "# inner_size: The inner size for the transformer layers.\n",
    "# dropout_rate: Dropout rate for the output of this layer.\n",
    "# attention_dropout_rate: Dropout rate on attention probabilities.\n",
    "# norm_epsilon: Epsilon value to initialize normalization layers.\n",
    "# inner_activation: The activation to use for the inner\n",
    "#     FFN layers.\n",
    "# kernel_initializer: Initializer for dense layer kernels.\n",
    "# inner_dropout: Dropout probability for the inner dropout\n",
    "#     layer.\n",
    "class TransformerXLBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 hidden_size,\n",
    "                 num_attention_heads,\n",
    "                 head_size,\n",
    "                 inner_size,\n",
    "                 dropout_rate,\n",
    "                 attention_dropout_rate,\n",
    "                 norm_epsilon=1e-12,\n",
    "                 inner_activation=\"relu\",\n",
    "                 kernel_initializer=\"variance_scaling\",\n",
    "                 inner_dropout=0.0,\n",
    "                 **kwargs):\n",
    "        \"\"\"Initializes TransformerXLBlock layer.\"\"\"\n",
    "\n",
    "        super(TransformerXLBlock, self).__init__(**kwargs)\n",
    "        self._vocab_size = vocab_size\n",
    "        self._num_heads = num_attention_heads\n",
    "        self._head_size = head_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._inner_size = inner_size\n",
    "        self._dropout_rate = dropout_rate\n",
    "        self._attention_dropout_rate = attention_dropout_rate\n",
    "        self._inner_activation = inner_activation\n",
    "        self._norm_epsilon = norm_epsilon\n",
    "        self._kernel_initializer = kernel_initializer\n",
    "        self._inner_dropout = inner_dropout\n",
    "        self._attention_layer_type = MultiHeadRelativeAttention\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_tensor = input_shape[0] if len(input_shape) == 2 else input_shape\n",
    "        input_tensor_shape = tf.TensorShape(input_tensor)\n",
    "        if len(input_tensor_shape.as_list()) != 3:\n",
    "            raise ValueError(\"TransformerLayer expects a three-dimensional input of \"\n",
    "                                             \"shape [batch, sequence, width].\")\n",
    "        batch_size, sequence_length, hidden_size = input_tensor_shape\n",
    "\n",
    "        if len(input_shape) == 2:\n",
    "            mask_tensor_shape = tf.TensorShape(input_shape[1])\n",
    "            expected_mask_tensor_shape = tf.TensorShape(\n",
    "                    [batch_size, sequence_length, sequence_length])\n",
    "            if not expected_mask_tensor_shape.is_compatible_with(mask_tensor_shape):\n",
    "                raise ValueError(\"When passing a mask tensor to TransformerXLBlock, \"\n",
    "                                                 \"the mask tensor must be of shape [batch, \"\n",
    "                                                 \"sequence_length, sequence_length] (here %s). Got a \"\n",
    "                                                 \"mask tensor of shape %s.\" %\n",
    "                                                 (expected_mask_tensor_shape, mask_tensor_shape))\n",
    "        if hidden_size % self._num_heads != 0:\n",
    "            raise ValueError(\n",
    "                    \"The input size (%d) is not a multiple of the number of attention \"\n",
    "                    \"heads (%d)\" % (hidden_size, self._num_heads))\n",
    "        self._attention_layer = self._attention_layer_type(\n",
    "                num_heads=self._num_heads,\n",
    "                key_dim=self._head_size,\n",
    "                value_dim=self._head_size,\n",
    "                dropout=self._attention_dropout_rate,\n",
    "                use_bias=False,\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer),\n",
    "                name=\"rel_attn\")\n",
    "        self._attention_dropout = tf.keras.layers.Dropout(\n",
    "                rate=self._attention_dropout_rate)\n",
    "        self._attention_layer_norm = tf.keras.layers.LayerNormalization(\n",
    "                name=\"self_attention_layer_norm\",\n",
    "                axis=-1,\n",
    "                epsilon=self._norm_epsilon,\n",
    "                dtype=tf.float32)\n",
    "        self._inner_dense = tf.keras.layers.experimental.EinsumDense(\n",
    "                \"abc,cd->abd\",\n",
    "                output_shape=(None, self._inner_size),\n",
    "                bias_axes=\"d\",\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer),\n",
    "                name=\"inner\")\n",
    "\n",
    "        self._inner_activation_layer = tf.keras.layers.Activation(\n",
    "                self._inner_activation)\n",
    "        self._inner_dropout_layer = tf.keras.layers.Dropout(\n",
    "                rate=self._inner_dropout)\n",
    "        self._output_dense = tf.keras.layers.experimental.EinsumDense(\n",
    "                \"abc,cd->abd\",\n",
    "                output_shape=(None, hidden_size),\n",
    "                bias_axes=\"d\",\n",
    "                name=\"output\",\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer))\n",
    "        self._output_dropout = tf.keras.layers.Dropout(rate=self._dropout_rate)\n",
    "        self._output_layer_norm = tf.keras.layers.LayerNormalization(\n",
    "                name=\"output_layer_norm\",\n",
    "                axis=-1,\n",
    "                epsilon=self._norm_epsilon)\n",
    "\n",
    "        super(TransformerXLBlock, self).build(input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "                \"vocab_size\":\n",
    "                        self._vocab_size,\n",
    "                \"hidden_size\":\n",
    "                        self._hidden_size,\n",
    "                \"num_attention_heads\":\n",
    "                        self._num_heads,\n",
    "                \"head_size\":\n",
    "                        self._head_size,\n",
    "                \"inner_size\":\n",
    "                        self._inner_size,\n",
    "                \"dropout_rate\":\n",
    "                        self._dropout_rate,\n",
    "                \"attention_dropout_rate\":\n",
    "                        self._attention_dropout_rate,\n",
    "                \"norm_epsilon\":\n",
    "                        self._norm_epsilon,\n",
    "                \"inner_activation\":\n",
    "                        self._inner_activation,\n",
    "                \"kernel_initializer\":\n",
    "                        self._kernel_initializer,\n",
    "                \"inner_dropout\":\n",
    "                        self._inner_dropout,\n",
    "        }\n",
    "        base_config = super(TransformerXLBlock, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "# content_stream: `Tensor`, the input content stream. This is the standard\n",
    "# input to Transformer XL and is commonly referred to as `h` in XLNet.\n",
    "# content_attention_bias: Bias `Tensor` for content based attention of shape\n",
    "# `[num_heads, dim]`.\n",
    "# positional_attention_bias: Bias `Tensor` for position based attention of\n",
    "# shape `[num_heads, dim]`.\n",
    "# relative_position_encoding: Relative positional encoding `Tensor` of shape\n",
    "# `[B, L, dim]`.\n",
    "# state: Optional `Tensor` of shape `[B, M, E]`, where M is the length of\n",
    "# the state or memory. If passed, this is also attended over as in\n",
    "# Transformer XL.\n",
    "# content_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to content attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# query_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to query attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# target_mapping: Optional `Tensor` representing the target mapping when\n",
    "# calculating query attention.\n",
    "# Returns:\n",
    "# A `dict` object, containing the key value pairs for `content_attention`\n",
    "    def call(self,\n",
    "             content_stream,\n",
    "             content_attention_bias,\n",
    "             positional_attention_bias,\n",
    "             relative_position_encoding=None,\n",
    "             state=None,\n",
    "             content_attention_mask=None,\n",
    "             query_attention_mask=None,\n",
    "             target_mapping=None):\n",
    "        \n",
    "        attention_kwargs = dict(\n",
    "                query=content_stream,\n",
    "                value=content_stream,\n",
    "                key=content_stream,\n",
    "                attention_mask=content_attention_mask)\n",
    "\n",
    "        common_attention_kwargs = dict(\n",
    "                content_attention_bias=content_attention_bias,\n",
    "                relative_position_encoding=relative_position_encoding,\n",
    "                positional_attention_bias=positional_attention_bias,\n",
    "                state=state)\n",
    "\n",
    "        attention_kwargs.update(common_attention_kwargs)\n",
    "        attention_output = self._attention_layer(**attention_kwargs)\n",
    "\n",
    "        attention_stream = attention_output\n",
    "        input_stream = content_stream\n",
    "        attention_key = \"content_attention\"\n",
    "        attention_output = {}\n",
    "        \n",
    "        attention_stream = self._attention_dropout(attention_stream)\n",
    "        attention_stream = self._attention_layer_norm(attention_stream + input_stream)\n",
    "        inner_output = self._inner_dense(attention_stream)\n",
    "        inner_output = self._inner_activation_layer(\n",
    "                inner_output)\n",
    "        inner_output = self._inner_dropout_layer(\n",
    "                inner_output)\n",
    "        layer_output = self._output_dense(inner_output)\n",
    "        layer_output = self._output_dropout(layer_output)\n",
    "        layer_output = self._output_layer_norm(layer_output + attention_stream)\n",
    "        attention_output[attention_key] = layer_output\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8749ec30-1281-4f9d-8baa-54b890fb7949",
   "metadata": {},
   "source": [
    "---\n",
    "# XL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "019ca25d-e899-4e17-8f41-dfdc3d9bdee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_layers: The number of layers.\n",
    "# hidden_size: The hidden size.\n",
    "# num_attention_heads: The number of attention heads.\n",
    "# head_size: The dimension size of each attention head.\n",
    "# inner_size: The hidden size in feed-forward layers.\n",
    "# dropout_rate: Dropout rate used in each Transformer XL block.\n",
    "# attention_dropout_rate: Dropout rate on attention probabilities.\n",
    "# initializer: The initializer to use for attention biases.\n",
    "# tie_attention_biases: Whether or not to tie biases together. If `True`, then\n",
    "# each Transformer XL block shares the same trainable attention bias. If\n",
    "# `False`, then each block has its own attention bias. This is usually set\n",
    "# to `True`.\n",
    "# memory_length: The number of tokens to cache.\n",
    "# reuse_length: The number of tokens in the current batch to be cached\n",
    "# and reused in the future.\n",
    "# inner_activation: The activation to use in the inner layers\n",
    "# for Transformer XL blocks. Typically \"relu\" or \"gelu\".\n",
    "class TransformerXL(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 num_layers,\n",
    "                 hidden_size,\n",
    "                 maxlen,\n",
    "                 embed_dim,\n",
    "                 num_attention_heads,\n",
    "                 head_size,\n",
    "                 inner_size,\n",
    "                 dropout_rate,\n",
    "                 attention_dropout_rate,\n",
    "                 initializer,\n",
    "                 tie_attention_biases=True,\n",
    "                 memory_length=None,\n",
    "                 reuse_length=None,\n",
    "                 inner_activation=\"relu\",\n",
    "                 **kwargs):\n",
    "        super(TransformerXL, self).__init__(**kwargs)\n",
    "\n",
    "        self._vocab_size = vocab_size\n",
    "        self._initializer = initializer\n",
    "        self._num_layers = num_layers\n",
    "        self._hidden_size = hidden_size\n",
    "        self._num_attention_heads = num_attention_heads\n",
    "        self._head_size = head_size\n",
    "        self._inner_size = inner_size\n",
    "        self._inner_activation = inner_activation\n",
    "        self._dropout_rate = dropout_rate\n",
    "        self._attention_dropout_rate = attention_dropout_rate\n",
    "        self._tie_attention_biases = tie_attention_biases\n",
    "\n",
    "        self._memory_length = memory_length\n",
    "        self._reuse_length = reuse_length\n",
    "\n",
    "        if self._tie_attention_biases:\n",
    "            attention_bias_shape = [self._num_attention_heads, self._head_size]\n",
    "        else:\n",
    "            attention_bias_shape = [self._num_layers, self._num_attention_heads, self._head_size]\n",
    "\n",
    "        self.content_attention_bias = self.add_weight(\n",
    "                \"content_attention_bias\",\n",
    "                shape=attention_bias_shape,\n",
    "                dtype=tf.float32,\n",
    "                initializer=clone_initializer(self._initializer))\n",
    "        self.positional_attention_bias = self.add_weight(\n",
    "                \"positional_attention_bias\",\n",
    "                shape=attention_bias_shape,\n",
    "                dtype=tf.float32,\n",
    "                initializer=clone_initializer(self._initializer))\n",
    "\n",
    "        self.transformer_xl_layers = []\n",
    "        for i in range(self._num_layers):\n",
    "            self.transformer_xl_layers.append(\n",
    "                    TransformerXLBlock(\n",
    "                            vocab_size=self._vocab_size,\n",
    "                            hidden_size=self._head_size * self._num_attention_heads,\n",
    "                            num_attention_heads=self._num_attention_heads,\n",
    "                            head_size=self._head_size,\n",
    "                            inner_size=self._inner_size,\n",
    "                            dropout_rate=self._dropout_rate,\n",
    "                            attention_dropout_rate=self._attention_dropout_rate,\n",
    "                            norm_epsilon=1e-12,\n",
    "                            inner_activation=self._inner_activation,\n",
    "                            kernel_initializer=\"variance_scaling\",\n",
    "                            name=\"layer_%d\" % i))\n",
    "\n",
    "        self.output_dropout = tf.keras.layers.Dropout(rate=self._dropout_rate)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "                \"vocab_size\":\n",
    "                        self._vocab_size,\n",
    "                \"num_layers\":\n",
    "                        self._num_layers,\n",
    "                \"hidden_size\":\n",
    "                        self._hidden_size,\n",
    "                \"num_attention_heads\":\n",
    "                        self._num_attention_heads,\n",
    "                \"head_size\":\n",
    "                        self._head_size,\n",
    "                \"inner_size\":\n",
    "                        self._inner_size,\n",
    "                \"dropout_rate\":\n",
    "                        self._dropout_rate,\n",
    "                \"attention_dropout_rate\":\n",
    "                        self._attention_dropout_rate,\n",
    "                \"initializer\":\n",
    "                        self._initializer,\n",
    "                \"tie_attention_biases\":\n",
    "                        self._tie_attention_biases,\n",
    "                \"memory_length\":\n",
    "                        self._memory_length,\n",
    "                \"reuse_length\":\n",
    "                        self._reuse_length,\n",
    "                \"inner_activation\":\n",
    "                        self._inner_activation,\n",
    "        }\n",
    "        base_config = super(TransformerXL, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "# content_stream: `Tensor`, the input content stream. This is the standard\n",
    "# input to Transformer XL and is commonly referred to as `h` in XLNet.\n",
    "# relative_position_encoding: Relative positional encoding `Tensor` of shape\n",
    "# `[B, L, dim]`.\n",
    "# state: Optional `Tensor` of shape `[B, M, E]`, where M is the length of\n",
    "# the state or memory. If passed, this is also attended over as in\n",
    "# Transformer XL.\n",
    "# content_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to content attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# query_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to query attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# target_mapping: Optional `Tensor` representing the target mapping when\n",
    "# calculating query attention.\n",
    "# Returns:\n",
    "# A tuple consisting of the attention output and the list of cached memory\n",
    "# states.\n",
    "# The attention output is `content_attention`\n",
    "    def call(self,\n",
    "             content_stream,\n",
    "             relative_position_encoding,\n",
    "             state=None,\n",
    "             content_attention_mask=None,\n",
    "             query_attention_mask=None,\n",
    "             target_mapping=None):\n",
    "        \n",
    "        new_mems = []\n",
    "\n",
    "        if state is None:\n",
    "            state = [None] * self._num_layers\n",
    "        for i in range(self._num_layers):\n",
    "            # cache new mems\n",
    "            new_mems.append(\n",
    "                    _cache_memory(content_stream, state[i],\n",
    "                                                self._memory_length, self._reuse_length))\n",
    "            \n",
    "            if self._tie_attention_biases:\n",
    "                content_attention_bias = self.content_attention_bias\n",
    "            else:\n",
    "                content_attention_bias = self.content_attention_bias[i]\n",
    "                \n",
    "            if self._tie_attention_biases:\n",
    "                positional_attention_bias = self.positional_attention_bias\n",
    "            else:\n",
    "                positional_attention_bias = self.positional_attention_bias[i]\n",
    "\n",
    "            transformer_xl_layer = self.transformer_xl_layers[i]\n",
    "            \n",
    "            transformer_xl_output = transformer_xl_layer(\n",
    "                    content_stream=content_stream,\n",
    "                    content_attention_bias=content_attention_bias,\n",
    "                    positional_attention_bias=positional_attention_bias,\n",
    "                    relative_position_encoding=relative_position_encoding,\n",
    "                    state=state[i],\n",
    "                    content_attention_mask=content_attention_mask,\n",
    "                    query_attention_mask=query_attention_mask,\n",
    "                    target_mapping=target_mapping)\n",
    "            content_stream = transformer_xl_output[\"content_attention\"]\n",
    "\n",
    "        output_stream = content_stream\n",
    "        return output_stream, new_mems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "855d43dd-e182-4c2a-961a-ff9781141f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XlModel(keras.Model):\n",
    "    def __init__(self,  embed_dim, encoder_shape, target_shape, vocab_size, num_layers, hidden_size, num_attention_heads, maxlen, memory_length, reuse_length, head_size, inner_size, dropout_rate, attention_dropout_rate, initializer):\n",
    "        super(XlModel, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_attention_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.maxlen = maxlen\n",
    "        self.memory_length = memory_length\n",
    "        \n",
    "        self.embedding_layer = MaskedTokenAndPositionEmbedding(maxlen=maxlen, vocab_size=vocab_size, embed_dim=embed_dim)\n",
    "        \n",
    "        self.rel_embeddings = RelativePositionEmbedding(embed_dim)\n",
    "        \n",
    "        self.decoder_embeddings = MaskedTokenAndPositionEmbedding(maxlen=pre_y_train.shape[1], vocab_size=vocab_size, embed_dim=self.embed_dim)\n",
    "\n",
    "        self.transformer_xl = TransformerXL(\n",
    "                vocab_size=vocab_size,\n",
    "                num_layers=num_layers,\n",
    "                hidden_size=hidden_size,\n",
    "                num_attention_heads=num_attention_heads,\n",
    "                maxlen=maxlen,\n",
    "                embed_dim=embed_dim,\n",
    "                memory_length=memory_length,\n",
    "                reuse_length=reuse_length,\n",
    "                head_size=head_size,\n",
    "                inner_size=inner_size,\n",
    "                dropout_rate=dropout_rate,\n",
    "                attention_dropout_rate=attention_dropout_rate,\n",
    "                initializer=initializer, \n",
    "            )\n",
    "        \n",
    "        self.decoder = self.create_decoder(encoder_shape, target_shape)\n",
    "        \n",
    "    def create_decoder(self, encoder_shape, target_shape):\n",
    "        y = x = keras.layers.Input(shape=target_shape)\n",
    "        y = self.decoder_embeddings(y)\n",
    "        \n",
    "        c = keras.layers.Input(shape=encoder_shape)\n",
    "    \n",
    "        for i in range(4):\n",
    "            y = MaskedTransformerBlock(embed_dim=self.embed_dim, num_heads=self.num_heads, ff_dim=self.hidden_size)([y,c])\n",
    "\n",
    "        y = keras.layers.Dense(len(i_to_c_pandp), name=\"hi\")(y)    \n",
    "        decoder = keras.Model([x, c],y,name=\"Decoder\")\n",
    "\n",
    "        return decoder    \n",
    "    \n",
    "    def call(self, x, mems, pre_y_train, training=None):        \n",
    "        embeddings = self.embedding_layer(x)\n",
    "        \n",
    "        \n",
    "        if mems[0] is not None:\n",
    "            rel_embeddings = self.rel_embeddings(tf.concat((embeddings,mems[0]), axis=1))\n",
    "        else:\n",
    "            rel_embeddings = self.rel_embeddings(embeddings)\n",
    "        \n",
    "        encoder, mems = self.transformer_xl(content_stream=embeddings, relative_position_encoding=rel_embeddings, state=mems)\n",
    "                    \n",
    "        model_output = self.decoder(([pre_y_train, encoder]), training=training)\n",
    "        \n",
    "        return model_output, mems\n",
    "    \n",
    "    def predictNTF(self, j, x, pre_y_train, post_y_train):\n",
    "        embeddings = self.embedding_layer(x)\n",
    "        mems = [*tf.zeros((self.num_layers, x.shape[0], self.memory_length, self.embed_dim))]\n",
    "        rel_embeddings = self.rel_embeddings(tf.concat((embeddings,mems[0]), axis=1))\n",
    "        context, mems = self.transformer_xl(content_stream=embeddings, relative_position_encoding=rel_embeddings, state=mems)\n",
    "        \n",
    "        token = np.zeros((x.shape[0], self.maxlen-1))\n",
    "        token[:,0] = 1 \n",
    "        \n",
    "        for i in range(post_y_train.shape[1]-1):\n",
    "            result = self.decoder.predict([token,context[:x.shape[0]]]).argmax(-1)\n",
    "            if np.all(result[:,i] == 2):\n",
    "                break \n",
    "                token[:,i+1] = result[:,i]\n",
    "                \n",
    "        result = self.decoder.predict([token,context])\n",
    "        result = result.argmax(-1)\n",
    "        \n",
    "        decoded = decode_seq(result[j],i_to_c_pandp)\n",
    "        target = decode_seq(pre_y_train[j],i_to_c_pandp)\n",
    "        \n",
    "        #accuracy = MaskedSparseCategoricalAccuracy(post_y_train, result).numpy()*100.0\n",
    "        accuracy = 5\n",
    "        \n",
    "        return decoded, target, accuracy,  mems\n",
    "    \n",
    "    \n",
    "    def predictTF(self, j, x, pre_y_train):\n",
    "        embeddings = self.embedding_layer(x)\n",
    "        mems = [*tf.zeros((self.num_layers, x.shape[0], self.memory_length, self.embed_dim))]\n",
    "        rel_embeddings = self.rel_embeddings(tf.concat((embeddings,mems[0]), axis=1))\n",
    "        context, mems = self.transformer_xl(content_stream=embeddings, relative_position_encoding=rel_embeddings, state=mems)\n",
    "        \n",
    "        result = self.decoder.predict([pre_y_train, context[:x.shape[0]]]).argmax(-1)\n",
    "        \n",
    "        decoded = decode_seq(result[j],i_to_c_pandp)\n",
    "        target = decode_seq(post_y_train[j],i_to_c_pandp)\n",
    "        \n",
    "        #accuracy = MaskedSparseCategoricalAccuracy(post_y_train[:0], result[:0]).numpy()*100.0\n",
    "        accuracy = 5\n",
    "        \n",
    "        return decoded, target, accuracy, mems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2bc3ea7d-d4a1-4a1e-bf2a-7cd1c91430cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters \n",
    "embed_dim = 64\n",
    "encoder_shape = (x_train.shape[1], embed_dim)\n",
    "target_shape = pre_y_train.shape[1:]\n",
    "vocab_size = len(i_to_c_pandp)\n",
    "num_layers = 8\n",
    "hidden_size = 64\n",
    "num_attention_heads = 8\n",
    "maxlen = x_train.shape[1]\n",
    "memory_length = 0\n",
    "reuse_length = 0\n",
    "head_size = 32\n",
    "inner_size = 32\n",
    "dropout_rate = 0.0\n",
    "attention_dropout_rate = 0.0\n",
    "initializer = keras.initializers.RandomNormal(stddev=0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "821b0c4a-6d05-4867-a713-474bfdbff881",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():  \n",
    "    model = XlModel(embed_dim, encoder_shape, target_shape, vocab_size, num_layers, hidden_size, num_attention_heads, maxlen, memory_length, reuse_length, head_size, inner_size, dropout_rate, attention_dropout_rate, initializer)\n",
    "    model.compile(loss = MaskedSparseCategoricalCrossentropy, optimizer = keras.optimizers.Adam())\n",
    "    batch = x_train\n",
    "    batch_size = x_train.shape[0]\n",
    "    mems = [*tf.zeros((num_layers, batch_size, memory_length, embed_dim))]\n",
    "    output, mems = model(x_train, mems, pre_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "92b02390-c88e-40e1-9686-cb7fc2581e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"xl_model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " masked_token_and_position_e  multiple                 8448      \n",
      " mbedding_4 (MaskedTokenAndP                                     \n",
      " ositionEmbedding)                                               \n",
      "                                                                 \n",
      " relative_position_embedding  multiple                 0         \n",
      " _2 (RelativePositionEmbeddi                                     \n",
      " ng)                                                             \n",
      "                                                                 \n",
      " masked_token_and_position_e  (None, 73, 64)           8384      \n",
      " mbedding_5 (MaskedTokenAndP                                     \n",
      " ositionEmbedding)                                               \n",
      "                                                                 \n",
      " transformer_xl_2 (Transform  multiple                 691456    \n",
      " erXL)                                                           \n",
      "                                                                 \n",
      " Decoder (Functional)        (None, 73, 57)            1107769   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,807,673\n",
      "Trainable params: 1,807,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752fb718-67f6-44ff-aa6d-c889c0254b82",
   "metadata": {},
   "source": [
    "---\n",
    "# Custom Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6fafad8d-f020-49b0-8233-bcfe54b88309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_step(batch, mems):\n",
    "    x, y, y_post = batch\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        result, new_mems = model(x, mems, y, training=True)\n",
    "        loss = MaskedSparseCategoricalCrossentropy(y_post, result)\n",
    "        \n",
    "    accuracy = MaskedSparseCategoricalAccuracy(y_post, result)\n",
    "        \n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    return loss, accuracy, new_mems\n",
    "\n",
    "@tf.function()\n",
    "def dist_train_step(batch, mems):\n",
    "    losses, accuracy, new_mems = strategy.run(train_step, args=(batch, mems))\n",
    "    return strategy.reduce(tf.distribute.ReduceOp.SUM, losses, axis=None), accuracy, new_mems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5a6127f4-fb94-4bf2-b3cf-fae83ef0aa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "history_loss = []\n",
    "history_accuracy = []\n",
    "epochs = 270"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a260118e-c39d-4232-8ee2-11ae86d8c8f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 Loss: 0.13846959173679352 Accuracy = 0.9687706232070923"
     ]
    }
   ],
   "source": [
    "with strategy.scope():    \n",
    "    for epoch in range(epochs):\n",
    "        batch = (x_train, pre_y_train, post_y_train)\n",
    "\n",
    "        loss, accuracy, mems = dist_train_step(batch, mems)\n",
    "        \n",
    "        history_loss.append(loss)\n",
    "        history_accuracy.append(accuracy) \n",
    "        \n",
    "        print(f\"\\r{epoch+1}/{epochs} Loss: {loss} Accuracy = {accuracy}\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "724ebe19-73b6-4aeb-b506-f24d6722ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_lens = []\n",
    "final_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cc71d008-1f06-4d2b-a8b4-0ed7fb8603d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n",
      "0.965912\n"
     ]
    }
   ],
   "source": [
    "mem_lens = np.loadtxt(\"./Mem_Lens.txt\", dtype=np.float64)\n",
    "final_accuracies =  np.loadtxt(\"./Final_Accuracies.txt\", dtype=np.float64)\n",
    "\n",
    "print(mem_lens)\n",
    "print(final_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6b5293b4-e6a7-4fbb-a40a-512e965573c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30.  0.]\n",
      "[0.965912   0.96877062]\n"
     ]
    }
   ],
   "source": [
    "mem_lens = np.append(mem_lens, memory_length)\n",
    "final_accuracies = np.append(final_accuracies, accuracy)\n",
    "print(mem_lens)\n",
    "print(final_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2fd866ef-9445-40a7-9d7e-85071baf9b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./Mem_Lens.txt', (mem_lens), fmt='%f', delimiter='\\n')\n",
    "np.savetxt('./Final_Accuracies.txt', (final_accuracies), fmt='%f', delimiter='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3e2dabdb-0f8b-4c68-91a0-72d0dd47d056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.plot(history_accuracy)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(history_loss)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "#plt.title(\"Accuracy and Loss Over Time\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ba942651-37b0-465e-b3c0-5dd22621b61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(2)\n",
    "plt.subplot(222)\n",
    "plt.plot(mem_lens, final_accuracies)\n",
    "plt.xlabel('Memory Length')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "27464ed8-16ad-423c-acc5-d379bb4db2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terself, began scolding one of her daughters. \n",
      " herself, began scolding one of her daughters. \n",
      " 5\n"
     ]
    }
   ],
   "source": [
    "predictionTF, target, accuracyTF, mems = model.predictTF(0, x_train, pre_y_train)\n",
    "print(predictionTF, '\\n', target, '\\n', accuracyTF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "23dd8aee-994c-4f4c-8a18-69356b3e1321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \"aaaaaaaaaaaaahhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh \n",
      " \"But consider your daughters. Only think what an establishment it would \n",
      " 5\n"
     ]
    }
   ],
   "source": [
    "predictionNTF, target, accuracyNTF, mems = model.predictNTF(1, x_train[:20], pre_y_train, post_y_train)\n",
    "print('\\n', predictionNTF, '\\n', target, '\\n', accuracyNTF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae81ff1b-a864-468f-b5a2-9a9036102cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-07-07 16:58:38--  https://www.cs.mtsu.edu/~jphillips/courses/CSCI4850-5850/public/PandP_Jane_Austen.txt\n",
      "Resolving www.cs.mtsu.edu (www.cs.mtsu.edu)... 161.45.162.100\n",
      "Connecting to www.cs.mtsu.edu (www.cs.mtsu.edu)|161.45.162.100|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 683838 (668K) [text/plain]\n",
      "Saving to: ‘PandP_Jane_Austen.txt’\n",
      "\n",
      "PandP_Jane_Austen.t 100%[===================>] 667.81K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2022-07-07 16:58:38 (11.6 MB/s) - ‘PandP_Jane_Austen.txt’ saved [683838/683838]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10657, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!wget https://www.cs.mtsu.edu/~jphillips/courses/CSCI4850-5850/public/PandP_Jane_Austen.txt\n",
    "with open('PandP_Jane_Austen.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "# Paragraphs are separated by blank\n",
    "# lines -> just drop those lines...\n",
    "text = []\n",
    "for i in range(len(lines)):\n",
    "     if lines[i] != '':\n",
    "        text = text + [lines[i]]\n",
    "data = np.vstack([[text[0:-1]],[text[1:]]]).T\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22c1911c-092e-4a19-be17-008e0979c3fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq = 100\n",
    "split_point = 80\n",
    "data = data[0:n_seq]\n",
    "np.random.shuffle(data) # In-place modification\n",
    "max_length = np.max([len(i) for i in data.flatten()]) + 2 # Add start/stop\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf21399d-41c5-4dcd-9650-916337917cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_seq(x,mapping,max_length=0):\n",
    "    # String to integer\n",
    "    return [mapping['<START>']] + \\\n",
    "    [mapping[i] for i in list(x)] + \\\n",
    "    [mapping['<STOP>']] + \\\n",
    "    [0]*(max_length-len(list(x))-2)\n",
    "def decode_seq(x,mapping):\n",
    "    # Integer-to-string\n",
    "    try:\n",
    "        idx = list(x).index(2) # Stop token?\n",
    "    except:\n",
    "        idx = len(list(x)) # No stop token found\n",
    "    return ''.join([mapping[i] for i in list(x)[0:idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2289815-d2d7-4c5c-ba6f-ed36987d8a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_to_c_pandp = ['','<START>','<STOP>'] + list({char for word in data[:,0] for char in word})\n",
    "c_to_i_pandp = {i_to_c_pandp[i]:i for i in range(len(i_to_c_pandp))}\n",
    "i_to_c_pandp[1] = i_to_c_pandp[2] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9134d636-3ece-4bc9-b7bc-bb3816f249e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack([encode_seq(x,c_to_i_pandp,max_length) for x in data[:,0]])\n",
    "Y = np.vstack([encode_seq(x,c_to_i_pandp,max_length) for x in data[:,1]])\n",
    "x_train = X[:split_point]\n",
    "x_test = X[split_point:]\n",
    "pre_y_train = Y[:,0:-1][:split_point]\n",
    "pre_y_test = Y[:,0:-1][split_point:]\n",
    "post_y_train = Y[:,1:][:split_point]\n",
    "post_y_test = Y[:,1:][split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd72860-e163-4462-835e-2183147f6dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
