{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dcad43d-67f7-49b9-84ce-6d91a0f468a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e7cb9df-5746-44a3-998a-b671b284957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2be6ea2-bb5f-48df-beae-6f795f794aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../../deep-learning-dna\")\n",
    "sys.path.append(\"../../settransformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4251783d-3aa2-46a0-9674-66bc68f981a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import settransformer as stf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fda64db1-8058-4e15-b2d4-1a3e64f23fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "import math\n",
    "import string\n",
    "\n",
    "import settransformer as stf\n",
    "from common.models import dnabert\n",
    "from common import dna\n",
    "from lmdbm import Lmdb\n",
    "from common.data import DnaSequenceGenerator, DnaLabelType, DnaSampleGenerator, find_dbs\n",
    "import wandb\n",
    "\n",
    "import tf_utils as tfu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dfb3b1a-2db7-4463-a208-b5d29e20f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tfu.strategy.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af784b9-c556-43b7-b636-7431b6442deb",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18d6bf42-0914-487c-922f-b54f2712adee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<common.models.dnabert.DnaBertPretrainModel at 0x7fab03c7f0d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import pretrained model\n",
    "api = wandb.Api()\n",
    "model_path = api.artifact(\"sirdavidludwig/deep-learning-dna/dnabert-pretrain-ablation-dim:8dim\").download()\n",
    "pretrained_model = dnabert.DnaBertModel.load(model_path)\n",
    "pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc98b4a9-90f8-442f-8f7e-7633b6f117cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact dnasamples:latest, 4086.55MB. 1260 files... Done. 0:3:14.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./artifacts/dnasamples:v1/train/WS-AG-Apr2016_S85_L001_R1_001.db'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load datafiles\n",
    "dataset_path = api.artifact(\"sirdavidludwig/nachusa-dna/dnasamples:latest\").download()\n",
    "samples = find_dbs(dataset_path + '/train')\n",
    "samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "eb43eeea-56ac-4f7f-85f2-fed0c0a62014",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_files = 1260"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3063cd-5a38-40bc-8ff6-4bf6dd03f397",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a14bf26f-f2d8-4907-991e-7dcbbc68f206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Sample './artifacts/dnasamples:v1/train/Wes24-PCRblank2_S25_L001_R1_001.db' only contains 33 sequences. This sample will not be included.\n",
      "Warning: Sample './artifacts/dnasamples:v1/train/Wes7-PCRblank1_S8_L001_R1_001.db' only contains 149 sequences. This sample will not be included.\n"
     ]
    }
   ],
   "source": [
    "#Generate batches\n",
    "subsample_length = 700\n",
    "sequence_length = 150\n",
    "kmer = 3\n",
    "batch_size = 20\n",
    "batches_per_epoch = 128\n",
    "augument = True\n",
    "labels = DnaLabelType.SampleIds\n",
    "dataset = DnaSampleGenerator(samples=samples, subsample_length = subsample_length, sequence_length=sequence_length,kmer=kmer,batch_size=batch_size,batches_per_epoch=batches_per_epoch,augment=augument,labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21b047f-bb01-43e0-ae37-67898f50a710",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[5][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d08e02fa-0bf0-4f37-a598-904035fe3a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 700, 148)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6d6e6ff-9ad7-4bc5-a04e-e389540e847e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 148)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed4a097f-8848-4845-885f-10d2eba07bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0][0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1952a5-7de9-4cb2-a353-18af72629061",
   "metadata": {},
   "source": [
    "---\n",
    "# Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5928c7f-92ab-4952-ab8b-477e0e457959",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create 8 dimensional embeddings\n",
    "pretrained_encoder= dnabert.DnaBertEncoderModel(pretrained_model.base)\n",
    "pretrained_encoder.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "801585e7-5a87-47cf-b952-a049454fcd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = pretrained_encoder.predict(dataset[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09c83c29-3d54-4262-ab9d-ff6a4312ed5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09bc0bf8-221e-47b5-99f1-07a27c13d117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -6.9496713 ,   1.6210482 ,  -7.8239136 , ...,  -4.2330317 ,\n",
       "         -1.5559857 ,  -3.900096  ],\n",
       "       [ -8.390611  ,   1.6148111 ,  -4.0261803 , ...,  -8.997752  ,\n",
       "          1.2903745 ,   2.3432248 ],\n",
       "       [ -4.8556437 ,   2.844587  ,  -5.9141846 , ...,  -8.047518  ,\n",
       "          2.4954636 ,   6.3042874 ],\n",
       "       ...,\n",
       "       [ -0.11759591,   2.1230648 ,  -1.8058336 , ...,  -0.7617552 ,\n",
       "         12.643952  ,  -0.83640176],\n",
       "       [ -9.236514  ,   5.322989  ,  -5.334051  , ..., -11.693535  ,\n",
       "         -0.35886312,   3.0221832 ],\n",
       "       [  1.2127527 ,  -2.1050375 ,   1.8511745 , ...,  -3.7555826 ,\n",
       "          8.241108  ,   0.07893485]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45d5d65-b728-42de-b529-85643adef96d",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Vanilla Transformer Block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e840f62e-6e0f-4788-bc15-1a94ea5871ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads,\n",
    "        key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "        [keras.layers.Dense(ff_dim, activation=\"gelu\"),\n",
    "        keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c9f3e4-144c-440d-a96d-4a4586bb5f13",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Masked Transformer Block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a199f10-268d-4153-863c-2d430a4d2c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedTransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(MaskedTransformerBlock, self).__init__()\n",
    "        self.att1 = keras.layers.MultiHeadAttention(num_heads=num_heads,\n",
    "        key_dim=embed_dim)\n",
    "        self.att2 = keras.layers.MultiHeadAttention(num_heads=num_heads,\n",
    "        key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "        [keras.layers.Dense(ff_dim, activation=\"gelu\"),\n",
    "        keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "        self.dropout3 = keras.layers.Dropout(rate)\n",
    "    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n",
    "        i = tf.range(n_dest)[:, None]\n",
    "        j = tf.range(n_src)\n",
    "        m = i >= j - n_src + n_dest\n",
    "        mask = tf.cast(m, dtype)\n",
    "        mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "        mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "    def call(self, inputs, training):\n",
    "        input_shape = tf.shape(inputs[0])\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        mask = self.causal_attention_mask(batch_size,\n",
    "        seq_len, seq_len,\n",
    "        tf.bool)\n",
    "        attn_output1 = self.att1(inputs[0], inputs[0],\n",
    "        attention_mask = mask)\n",
    "        attn_output1 = self.dropout1(attn_output1, training=training)\n",
    "        out1 = self.layernorm1(inputs[0] + attn_output1)\n",
    "        attn_output2 = self.att2(out1, inputs[1])\n",
    "        attn_output2 = self.dropout2(attn_output2, training=training)\n",
    "        out2 = self.layernorm1(out1 + attn_output2)\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        return self.layernorm2(out2 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1631b9-3dbd-4750-95c4-1cb7f1996548",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Masked Position Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a27a7c63-8d17-4e49-bfd7-4e05d92eb915",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedTokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim, **kwargs):\n",
    "        super(MaskedTokenAndPositionEmbedding, self).__init__(**kwargs)\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size,\n",
    "        output_dim=embed_dim,\n",
    "        mask_zero=True)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen+1,\n",
    "        output_dim=embed_dim,\n",
    "        mask_zero=True)\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=1, limit=maxlen+1, delta=1)\n",
    "        positions = positions * tf.cast(tf.sign(x),tf.int32)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a01a48-a768-41b8-a4e5-13394af7c12c",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Custom Loss and Accuracy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "160d05e4-b851-4348-85df-508526fb154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "def MaskedSparseCategoricalCrossentropy(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "def MaskedSparseCategoricalAccuracy(real, pred):\n",
    "    accuracies = tf.equal(tf.cast(real,tf.int64), tf.argmax(pred, axis=2))\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b66c98-f8b5-4b80-9736-66a9769cc966",
   "metadata": {},
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9af4ac04-1eab-4d74-bfc0-716408efeed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_initializer(initializer):\n",
    "    if isinstance(initializer, tf.keras.initializers.Initializer):\n",
    "        return initializer.__class__.from_config(initializer.get_config())\n",
    "    return initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "aadfeaaa-08b0-4155-8381-328ac71f4c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape_list(tensor, expected_rank=None, name=None):\n",
    "    if expected_rank is not None:\n",
    "        assert_rank(tensor, expected_rank, name)\n",
    "\n",
    "    shape = tensor.shape.as_list()\n",
    "\n",
    "    non_static_indexes = []\n",
    "    for (index, dim) in enumerate(shape):\n",
    "        if dim is None:\n",
    "            non_static_indexes.append(index)\n",
    "\n",
    "    if not non_static_indexes:\n",
    "        return shape\n",
    "\n",
    "    dyn_shape = tf.shape(tensor)\n",
    "    for index in non_static_indexes:\n",
    "        shape[index] = dyn_shape[index]\n",
    "    return shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835c39dd-9fb5-49bf-8763-14ca694a1a35",
   "metadata": {},
   "source": [
    "---\n",
    "# Cache Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "da4cb5e4-2d26-4e40-a00f-3e71f470b273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_state: `Tensor`, the current state.\n",
    "# previous_state: `Tensor`, the previous state.\n",
    "# memory_length: `int`, the number of tokens to cache.\n",
    "# reuse_length: `int`, the number of tokens in the current batch to be cached\n",
    "#     and reused in the future.\n",
    "# Returns:  `Tensor`, representing the cached state with stopped gradients.\n",
    "def _cache_memory(current_state, previous_state, memory_length, reuse_length=0):\n",
    "    if memory_length is None or memory_length == 0:\n",
    "        return None\n",
    "    else:\n",
    "        if reuse_length > 0:\n",
    "            current_state = current_state[:, :reuse_length, :]\n",
    "\n",
    "        if previous_state is None:\n",
    "            new_mem = current_state[:, -memory_length:, :]\n",
    "        else:\n",
    "            new_mem = tf.concat(\n",
    "                    [previous_state, current_state], 1)[:, -memory_length:, :]\n",
    "\n",
    "    return tf.stop_gradient(new_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3126b9-a96e-4d6a-ae65-3c9579514cf1",
   "metadata": {},
   "source": [
    "---\n",
    "# MultiHead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "5b32faad-fd06-4448-b817-b3a1f5943aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query: Query `Tensor` of shape `[B, T, dim]`.\n",
    "# value: Value `Tensor` of shape `[B, S, dim]`.\n",
    "# content_attention_bias: Bias `Tensor` for content based attention of shape\n",
    "# `[num_heads, dim]`.\n",
    "# positional_attention_bias: Bias `Tensor` for position based attention of\n",
    "# shape `[num_heads, dim]`.\n",
    "# key: Optional key `Tensor` of shape `[B, S, dim]`. If not given, will use\n",
    "# `value` for both `key` and `value`, which is the most common case.\n",
    "# relative_position_encoding: Relative positional encoding `Tensor` of shape\n",
    "# `[B, L, dim]`.\n",
    "# state: Optional `Tensor` of shape `[B, M, E]` where M is the length of the\n",
    "# state or memory. If passed, this is also attended over as in Transformer\n",
    "# XL.\n",
    "# attention_mask: A boolean mask of shape `[B, T, S]` that prevents attention\n",
    "# to certain positions.\n",
    "class MultiHeadRelativeAttention(tf.keras.layers.MultiHeadAttention):\n",
    "    def __init__(self,\n",
    "                 kernel_initializer=\"variance_scaling\",\n",
    "                 **kwargs):\n",
    "        super().__init__(kernel_initializer=kernel_initializer,\n",
    "                                         **kwargs)\n",
    "\n",
    "    def _build_from_signature(self, query, value, key=None):\n",
    "        super(MultiHeadRelativeAttention, self)._build_from_signature(\n",
    "                query=query,\n",
    "                value=value,\n",
    "                key=key)\n",
    "        if hasattr(value, \"shape\"):\n",
    "            value_shape = tf.TensorShape(value.shape)\n",
    "        else:\n",
    "            value_shape = value\n",
    "        if key is None:\n",
    "            key_shape = value_shape\n",
    "        elif hasattr(key, \"shape\"):\n",
    "            key_shape = tf.TensorShape(key.shape)\n",
    "        else:\n",
    "            key_shape = key\n",
    "\n",
    "        common_kwargs = dict(\n",
    "                kernel_initializer=self._kernel_initializer,\n",
    "                bias_initializer=self._bias_initializer,\n",
    "                kernel_regularizer=self._kernel_regularizer,\n",
    "                bias_regularizer=self._bias_regularizer,\n",
    "                activity_regularizer=self._activity_regularizer,\n",
    "                kernel_constraint=self._kernel_constraint,\n",
    "                bias_constraint=self._bias_constraint)\n",
    "\n",
    "        with tf.init_scope():\n",
    "            einsum_equation, _, output_rank = _build_proj_equation(\n",
    "                    key_shape.rank - 1, bound_dims=1, output_dims=2)\n",
    "            self._encoding_dense = tf.keras.layers.experimental.EinsumDense(\n",
    "                    einsum_equation,\n",
    "                    output_shape=_get_output_shape(\n",
    "                        output_rank - 1,\n",
    "                        [self._num_heads, self._key_dim]),\n",
    "                        bias_axes=None,\n",
    "                        name=\"encoding\",\n",
    "                        **common_kwargs)\n",
    "\n",
    "# query: Projected query `Tensor` of shape `[B, T, N, key_dim]`.\n",
    "# key: Projected key `Tensor` of shape `[B, S + M, N, key_dim]`.\n",
    "# value: Projected value `Tensor` of shape `[B, S + M, N, key_dim]`.\n",
    "# position: Projected position `Tensor` of shape `[B, L, N, key_dim]`.\n",
    "# content_attention_bias: Trainable bias parameter added to the query head\n",
    "#     when calculating the content-based attention score.\n",
    "# positional_attention_bias: Trainable bias parameter added to the query\n",
    "#     head when calculating the position-based attention score.\n",
    "# attention_mask: (default None) Optional mask that is added to attention\n",
    "#     logits. If state is not None, the mask source sequence dimension should\n",
    "#     extend M.\n",
    "# Returns:\n",
    "# attention_output: Multi-headed output of attention computation of shape\n",
    "#     `[B, S, N, key_dim]`.\n",
    "    def compute_attention(\n",
    "        self,\n",
    "        query,\n",
    "        key,\n",
    "        value,\n",
    "        position,\n",
    "        content_attention_bias,\n",
    "        positional_attention_bias,\n",
    "        attention_mask=None\n",
    "    ):\n",
    "        #AC\n",
    "        content_attention = tf.einsum(self._dot_product_equation, key, query + content_attention_bias)\n",
    "        \n",
    "        #positional_attention = tf.einsum(self._dot_product_equation, position, query + positional_attention_bias)\n",
    "        \n",
    "        #BD\n",
    "       # positional_attention = _rel_shift(positional_attention, klen=tf.shape(content_attention)[3])\n",
    "    \n",
    "        positional_attention = None\n",
    "        \n",
    "        attention_sum = content_attention #+ positional_attention\n",
    "\n",
    "        attention_scores = tf.multiply(attention_sum, 1.0 / math.sqrt(float(self._key_dim)))\n",
    "\n",
    "        attention_scores = self._masked_softmax(attention_scores, attention_mask)\n",
    "\n",
    "        attention_output = self._dropout_layer(attention_scores)\n",
    "\n",
    "        attention_output = tf.einsum(self._combine_equation,\n",
    "                                                                 attention_output,\n",
    "                                                                 value)\n",
    "        return attention_output\n",
    "\n",
    "# * Number of heads (H): the number of attention heads.\n",
    "# * Value size (V): the size of each value embedding per head.\n",
    "# * Key size (K): the size of each key embedding per head. Equally, the size\n",
    "#     of each query embedding per head. Typically K <= V.\n",
    "# * Batch dimensions (B).\n",
    "# * Query (target) attention axes shape (T).\n",
    "# * Value (source) attention axes shape (S), the rank must match the target.\n",
    "# * Encoding length (L): The relative positional encoding length.\n",
    "# query: attention input.\n",
    "# value: attention input.\n",
    "# content_attention_bias: A trainable bias parameter added to the query head\n",
    "#     when calculating the content-based attention score.\n",
    "# positional_attention_bias: A trainable bias parameter added to the query\n",
    "#     head when calculating the position-based attention score.\n",
    "# key: attention input.\n",
    "# relative_position_encoding: relative positional encoding for key and\n",
    "#     value.\n",
    "# state: (default None) optional state. If passed, this is also attended\n",
    "#     over as in TransformerXL.\n",
    "# attention_mask: (default None) Optional mask that is added to attention\n",
    "#     logits. If state is not None, the mask source sequence dimension should\n",
    "#     extend M.\n",
    "# Returns:\n",
    "# attention_output: The result of the computation, of shape [B, T, E],\n",
    "#     where `T` is for target sequence shapes and `E` is the query input last\n",
    "#     dimension if `output_shape` is `None`. Otherwise, the multi-head outputs\n",
    "#     are projected to the shape specified by `output_shape`.\n",
    "    def call(self,\n",
    "             query,\n",
    "             value,\n",
    "             content_attention_bias,\n",
    "             positional_attention_bias,\n",
    "             key=None,\n",
    "             relative_position_encoding=None,\n",
    "             state=None,\n",
    "             attention_mask=None):\n",
    "        \n",
    "        if not self._built_from_signature:\n",
    "            self._build_from_signature(query, value, key=key)\n",
    "        if key is None:\n",
    "            key = value\n",
    "        if state is not None and state.shape.ndims > 1:\n",
    "            value = tf.concat([state, value], 1)\n",
    "            key = tf.concat([state, key], 1)\n",
    "\n",
    "        # `query` = [B, T, N ,H]\n",
    "        query = self._query_dense(query)\n",
    "\n",
    "        # `key` = [B, S + M, N, H]\n",
    "        key = self._key_dense(key)\n",
    "\n",
    "        # `value` = [B, S + M, N, H]\n",
    "        value = self._value_dense(value)\n",
    "\n",
    "        # # `position` = [B, L, N, H]\n",
    "        # position = self._encoding_dense(relative_position_encoding)\n",
    "        position = None\n",
    "        \n",
    "        \n",
    "        attention_output = self.compute_attention(\n",
    "                query=query,\n",
    "                key=key,\n",
    "                value=value,\n",
    "                position=position,\n",
    "                content_attention_bias=content_attention_bias,\n",
    "                positional_attention_bias=positional_attention_bias,\n",
    "                attention_mask=attention_mask)\n",
    "\n",
    "        # `attention_output` = [B, S, N, H]\n",
    "        attention_output = self._output_dense(attention_output)\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95abdafa-2c98-4c04-b9e8-aae23e53265e",
   "metadata": {},
   "source": [
    "---\n",
    "# Relative Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "cc8c99a8-3755-48f9-9704-c1dbd04e0ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _rel_shift(x, klen=-1):    \n",
    "#     x = tf.transpose(x, perm=[2, 3, 0, 1])\n",
    "#     x = tf.pad(x, [[0, 0], [1, 0], [0, 0], [0, 0]])\n",
    "#     x_size = tf.shape(x)\n",
    "#     x = tf.reshape(x, [x_size[1], x_size[0], x_size[2], x_size[3]])\n",
    "#     x = tf.slice(x, [1, 0, 0, 0], [-1, -1, -1, -1])\n",
    "#     x = tf.reshape(x, [x_size[0], x_size[1] - 1, x_size[2], x_size[3]])\n",
    "#     x = tf.transpose(x, perm=[2, 3, 0, 1])\n",
    "\n",
    "#     return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a026fd-2cbe-45b7-88f7-a3ffd3548806",
   "metadata": {},
   "source": [
    "---\n",
    "# Build Einsum Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "5102422c-82af-49d4-a8d5-5081262b21e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_CHR_IDX = string.ascii_lowercase\n",
    "\n",
    "# Builds an einsum equation for projections inside multi-head attention\n",
    "def _build_proj_equation(free_dims, bound_dims, output_dims):\n",
    "    input_str = \"\"\n",
    "    kernel_str = \"\"\n",
    "    output_str = \"\"\n",
    "    bias_axes = \"\"\n",
    "    letter_offset = 0\n",
    "    for i in range(free_dims):\n",
    "        char = _CHR_IDX[i + letter_offset]\n",
    "        input_str += char\n",
    "        output_str += char\n",
    "\n",
    "    letter_offset += free_dims\n",
    "    for i in range(bound_dims):\n",
    "        char = _CHR_IDX[i + letter_offset]\n",
    "        input_str += char\n",
    "        kernel_str += char\n",
    "\n",
    "    letter_offset += bound_dims\n",
    "    for i in range(output_dims):\n",
    "        char = _CHR_IDX[i + letter_offset]\n",
    "        kernel_str += char\n",
    "        output_str += char\n",
    "        bias_axes += char\n",
    "    equation = \"%s,%s->%s\" % (input_str, kernel_str, output_str)\n",
    "\n",
    "    return equation, bias_axes, len(output_str)\n",
    "\n",
    "\n",
    "def _get_output_shape(output_rank, known_last_dims):\n",
    "    return [None] * (output_rank - len(known_last_dims)) + list(known_last_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5343f3e3-8536-46d5-9d1b-28f8c3f7908b",
   "metadata": {},
   "source": [
    "---\n",
    "# Relative Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a5f5eeac-4c07-4796-8281-f5c039fe7c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_size: Size of the hidden layer.\n",
    "# min_timescale: Minimum scale that will be applied at each position\n",
    "# max_timescale: Maximum scale that will be applied at each position.\n",
    "# class RelativePositionEmbedding(tf.keras.layers.Layer):\n",
    "\n",
    "#     def __init__(self,\n",
    "#                  hidden_size: int,\n",
    "#                  min_timescale: float = 1.0,\n",
    "#                  max_timescale: float = 1.0e4,\n",
    "#                  **kwargs):\n",
    "        \n",
    "#         if \"dtype\" not in kwargs:\n",
    "#             kwargs[\"dtype\"] = \"float32\"\n",
    "\n",
    "#         super().__init__(**kwargs)\n",
    "#         self._hidden_size = hidden_size\n",
    "#         self._min_timescale = min_timescale\n",
    "#         self._max_timescale = max_timescale\n",
    "\n",
    "#     def get_config(self):\n",
    "#         config = {\n",
    "#                 \"hidden_size\": self._hidden_size,\n",
    "#                 \"min_timescale\": self._min_timescale,\n",
    "#                 \"max_timescale\": self._max_timescale,\n",
    "#         }\n",
    "#         base_config = super(RelativePositionEmbedding, self).get_config()\n",
    "#         return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "# # inputs: An tensor whose second dimension will be used as `length`. If\n",
    "# # `None`, the other `length` argument must be specified.\n",
    "# # length: An optional integer specifying the number of positions. If both\n",
    "# # `inputs` and `length` are spcified, `length` must be equal to the second\n",
    "# # dimension of `inputs`.\n",
    "# # Returns:\n",
    "# # A tensor in shape of `(length, hidden_size)`.\n",
    "#     def call(self, inputs, length=None):\n",
    "#         if inputs is None and length is None:\n",
    "#             raise ValueError(\"If inputs is None, `length` must be set in \"\n",
    "#                                              \"RelativePositionEmbedding().\")\n",
    "#         if inputs is not None:\n",
    "#             input_shape = get_shape_list(inputs)\n",
    "#             if length is not None and length != input_shape[1]:\n",
    "#                 raise ValueError(\n",
    "#                         \"If inputs is not None, `length` must equal to input_shape[1].\")\n",
    "#             length = input_shape[1]\n",
    "#         position = tf.cast(tf.range(length), tf.float32)\n",
    "#         num_timescales = self._hidden_size // 2\n",
    "#         min_timescale, max_timescale = self._min_timescale, self._max_timescale\n",
    "#         log_timescale_increment = (\n",
    "#                 math.log(float(max_timescale) / float(min_timescale)) /\n",
    "#                 (tf.cast(num_timescales, tf.float32) - 1))\n",
    "#         inv_timescales = min_timescale * tf.exp(\n",
    "#                 tf.cast(tf.range(num_timescales), tf.float32) *\n",
    "#                 -log_timescale_increment)\n",
    "#         scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(\n",
    "#                 inv_timescales, 0)\n",
    "#         position_embeddings = tf.concat(\n",
    "#                 [tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n",
    "#         position_embeddings = tf.expand_dims(position_embeddings, axis=0)\n",
    "#         return tf.tile(position_embeddings, (tf.shape(inputs)[0], 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "fdbdfc2d-f593-4466-a981-c8e7b3b06e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size: The size of the token vocabulary.\n",
    "# hidden_size: The size of the transformer hidden layers.\n",
    "# num_attention_heads: The number of attention heads.\n",
    "# head_size: The dimension size of each attention head.\n",
    "# inner_size: The inner size for the transformer layers.\n",
    "# dropout_rate: Dropout rate for the output of this layer.\n",
    "# attention_dropout_rate: Dropout rate on attention probabilities.\n",
    "# norm_epsilon: Epsilon value to initialize normalization layers.\n",
    "# inner_activation: The activation to use for the inner\n",
    "#     FFN layers.\n",
    "# kernel_initializer: Initializer for dense layer kernels.\n",
    "# inner_dropout: Dropout probability for the inner dropoutlay er.\n",
    "class TransformerXLBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 hidden_size,\n",
    "                 num_attention_heads,\n",
    "                 head_size,\n",
    "                 inner_size,\n",
    "                 dropout_rate,\n",
    "                 attention_dropout_rate,\n",
    "                 norm_epsilon=1e-12,\n",
    "                 inner_activation=\"relu\",\n",
    "                 kernel_initializer=\"variance_scaling\",\n",
    "                 inner_dropout=0.0,\n",
    "                 **kwargs):\n",
    "        \"\"\"Initializes TransformerXLBlock layer.\"\"\"\n",
    "\n",
    "        super(TransformerXLBlock, self).__init__(**kwargs)\n",
    "        self._vocab_size = vocab_size\n",
    "        self._num_heads = num_attention_heads\n",
    "        self._head_size = head_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._inner_size = inner_size\n",
    "        self._dropout_rate = dropout_rate\n",
    "        self._attention_dropout_rate = attention_dropout_rate\n",
    "        self._inner_activation = inner_activation\n",
    "        self._norm_epsilon = norm_epsilon\n",
    "        self._kernel_initializer = kernel_initializer\n",
    "        self._inner_dropout = inner_dropout\n",
    "        self._attention_layer_type = MultiHeadRelativeAttention\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_tensor = input_shape[0] if len(input_shape) == 2 else input_shape\n",
    "        input_tensor_shape = tf.TensorShape(input_tensor)\n",
    "        if len(input_tensor_shape.as_list()) != 3:\n",
    "            raise ValueError(\"TransformerLayer expects a three-dimensional input of \"\n",
    "                                             \"shape [batch, sequence, width].\")\n",
    "        batch_size, sequence_length, hidden_size = input_tensor_shape\n",
    "\n",
    "        if len(input_shape) == 2:\n",
    "            mask_tensor_shape = tf.TensorShape(input_shape[1])\n",
    "            expected_mask_tensor_shape = tf.TensorShape(\n",
    "                    [batch_size, sequence_length, sequence_length])\n",
    "            if not expected_mask_tensor_shape.is_compatible_with(mask_tensor_shape):\n",
    "                raise ValueError(\"When passing a mask tensor to TransformerXLBlock, \"\n",
    "                                                 \"the mask tensor must be of shape [batch, \"\n",
    "                                                 \"sequence_length, sequence_length] (here %s). Got a \"\n",
    "                                                 \"mask tensor of shape %s.\" %\n",
    "                                                 (expected_mask_tensor_shape, mask_tensor_shape))\n",
    "        if hidden_size % self._num_heads != 0:\n",
    "            raise ValueError(\n",
    "                    \"The input size (%d) is not a multiple of the number of attention \"\n",
    "                    \"heads (%d)\" % (hidden_size, self._num_heads))\n",
    "        self._attention_layer = self._attention_layer_type(\n",
    "                num_heads=self._num_heads,\n",
    "                key_dim=self._head_size,\n",
    "                value_dim=self._head_size,\n",
    "                dropout=self._attention_dropout_rate,\n",
    "                use_bias=False,\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer),\n",
    "                name=\"rel_attn\")\n",
    "        self._attention_dropout = tf.keras.layers.Dropout(\n",
    "                rate=self._attention_dropout_rate)\n",
    "        self._attention_layer_norm = tf.keras.layers.LayerNormalization(\n",
    "                name=\"self_attention_layer_norm\",\n",
    "                axis=-1,\n",
    "                epsilon=self._norm_epsilon,\n",
    "                dtype=tf.float32)\n",
    "        self._inner_dense = tf.keras.layers.experimental.EinsumDense(\n",
    "                \"abc,cd->abd\",\n",
    "                output_shape=(None, self._inner_size),\n",
    "                bias_axes=\"d\",\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer),\n",
    "                name=\"inner\")\n",
    "\n",
    "        self._inner_activation_layer = tf.keras.layers.Activation(\n",
    "                self._inner_activation)\n",
    "        self._inner_dropout_layer = tf.keras.layers.Dropout(\n",
    "                rate=self._inner_dropout)\n",
    "        self._output_dense = tf.keras.layers.experimental.EinsumDense(\n",
    "                \"abc,cd->abd\",\n",
    "                output_shape=(None, hidden_size),\n",
    "                bias_axes=\"d\",\n",
    "                name=\"output\",\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer))\n",
    "        self._output_dropout = tf.keras.layers.Dropout(rate=self._dropout_rate)\n",
    "        self._output_layer_norm = tf.keras.layers.LayerNormalization(\n",
    "                name=\"output_layer_norm\",\n",
    "                axis=-1,\n",
    "                epsilon=self._norm_epsilon)\n",
    "\n",
    "        \n",
    "        super(TransformerXLBlock, self).build(input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "                \"vocab_size\":\n",
    "                        self._vocab_size,\n",
    "                \"hidden_size\":\n",
    "                        self._hidden_size,\n",
    "                \"num_attention_heads\":\n",
    "                        self._num_heads,\n",
    "                \"head_size\":\n",
    "                        self._head_size,\n",
    "                \"inner_size\":\n",
    "                        self._inner_size,\n",
    "                \"dropout_rate\":\n",
    "                        self._dropout_rate,\n",
    "                \"attention_dropout_rate\":\n",
    "                        self._attention_dropout_rate,\n",
    "                \"norm_epsilon\":\n",
    "                        self._norm_epsilon,\n",
    "                \"inner_activation\":\n",
    "                        self._inner_activation,\n",
    "                \"kernel_initializer\":\n",
    "                        self._kernel_initializer,\n",
    "                \"inner_dropout\":\n",
    "                        self._inner_dropout,\n",
    "        }\n",
    "        base_config = super(TransformerXLBlock, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    \n",
    "    # content_stream: `Tensor`, the input content stream. This is the standard\n",
    "    # input to Transformer XL and is commonly referred to as `h` in XLNet.\n",
    "    # content_attention_bias: Bias `Tensor` for content based attention of shape\n",
    "    # `[num_heads, dim]`.\n",
    "    # positional_attention_bias: Bias `Tensor` for position based attention of\n",
    "    # shape `[num_heads, dim]`.\n",
    "    # relative_position_encoding: Relative positional encoding `Tensor` of shape\n",
    "    # `[B, L, dim]`.\n",
    "    # state: Optional `Tensor` of shape `[B, M, E]`, where M is the length of\n",
    "    # the state or memory. If passed, this is also attended over as in\n",
    "    # Transformer XL.\n",
    "    # content_attention_mask: Optional `Tensor` representing the mask that is\n",
    "    # added to content attention logits. If state is not None, the mask source\n",
    "    # sequence dimension should extend M.\n",
    "    # query_attention_mask: Optional `Tensor` representing the mask that is\n",
    "    # added to query attention logits. If state is not None, the mask source\n",
    "    # sequence dimension should extend M.\n",
    "    # target_mapping: Optional `Tensor` representing the target mapping when\n",
    "    # calculating query attention.\n",
    "    # Returns:\n",
    "    # A `dict` object, containing the key value pairs for `content_attention`\n",
    "    def call(self,\n",
    "             content_stream,\n",
    "             content_attention_bias,\n",
    "             positional_attention_bias = None,\n",
    "             relative_position_encoding=None,\n",
    "             state=None,\n",
    "             content_attention_mask=None,\n",
    "             query_attention_mask=None,\n",
    "             target_mapping=None):\n",
    "\n",
    "        attention_kwargs = dict(\n",
    "                query=content_stream,\n",
    "                value=content_stream,\n",
    "                key=content_stream,\n",
    "                attention_mask=content_attention_mask)\n",
    "\n",
    "        common_attention_kwargs = dict(\n",
    "                content_attention_bias=content_attention_bias,\n",
    "                relative_position_encoding=relative_position_encoding,\n",
    "                positional_attention_bias=positional_attention_bias,\n",
    "                state=state)\n",
    "\n",
    "        attention_kwargs.update(common_attention_kwargs)\n",
    "        attention_output = self._attention_layer(**attention_kwargs)\n",
    "\n",
    "        attention_stream = attention_output\n",
    "        input_stream = content_stream\n",
    "        attention_key = \"content_attention\"\n",
    "        attention_output = {}\n",
    "        \n",
    "        attention_stream = self._attention_dropout(attention_stream)\n",
    "        attention_stream = self._attention_layer_norm(\n",
    "                attention_stream + input_stream)\n",
    "        inner_output = self._inner_dense(attention_stream)\n",
    "        inner_output = self._inner_activation_layer(\n",
    "                inner_output)\n",
    "        inner_output = self._inner_dropout_layer(\n",
    "                inner_output)\n",
    "        layer_output = self._output_dense(inner_output)\n",
    "        layer_output = self._output_dropout(layer_output)\n",
    "        layer_output = self._output_layer_norm(layer_output + attention_stream)\n",
    "        attention_output[attention_key] = layer_output\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c32bb5-4520-4802-a8f0-5f7849905a83",
   "metadata": {},
   "source": [
    "---\n",
    "# TransformerXL Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "7245b7dd-a3b2-464a-9f5b-7efd4a9d0ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size: The size of the token vocabulary.\n",
    "# hidden_size: The size of the transformer hidden layers.\n",
    "# num_attention_heads: The number of attention heads.\n",
    "# head_size: The dimension size of each attention head.\n",
    "# inner_size: The inner size for the transformer layers.\n",
    "# dropout_rate: Dropout rate for the output of this layer.\n",
    "# attention_dropout_rate: Dropout rate on attention probabilities.\n",
    "# norm_epsilon: Epsilon value to initialize normalization layers.\n",
    "# inner_activation: The activation to use for the inner\n",
    "#     FFN layers.\n",
    "# kernel_initializer: Initializer for dense layer kernels.\n",
    "# inner_dropout: Dropout probability for the inner dropout\n",
    "#     layer.\n",
    "class TransformerXLBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 hidden_size,\n",
    "                 num_attention_heads,\n",
    "                 head_size,\n",
    "                 inner_size,\n",
    "                 dropout_rate,\n",
    "                 attention_dropout_rate,\n",
    "                 norm_epsilon=1e-12,\n",
    "                 inner_activation=\"relu\",\n",
    "                 kernel_initializer=\"variance_scaling\",\n",
    "                 inner_dropout=0.0,\n",
    "                 **kwargs):\n",
    "        \"\"\"Initializes TransformerXLBlock layer.\"\"\"\n",
    "\n",
    "        super(TransformerXLBlock, self).__init__(**kwargs)\n",
    "        self._vocab_size = vocab_size\n",
    "        self._num_heads = num_attention_heads\n",
    "        self._head_size = head_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._inner_size = inner_size\n",
    "        self._dropout_rate = dropout_rate\n",
    "        self._attention_dropout_rate = attention_dropout_rate\n",
    "        self._inner_activation = inner_activation\n",
    "        self._norm_epsilon = norm_epsilon\n",
    "        self._kernel_initializer = kernel_initializer\n",
    "        self._inner_dropout = inner_dropout\n",
    "        self._attention_layer_type = MultiHeadRelativeAttention\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_tensor = input_shape[0] if len(input_shape) == 2 else input_shape\n",
    "        input_tensor_shape = tf.TensorShape(input_tensor)\n",
    "        if len(input_tensor_shape.as_list()) != 3:\n",
    "            raise ValueError(\"TransformerLayer expects a three-dimensional input of \"\n",
    "                                             \"shape [batch, sequence, width].\")\n",
    "        batch_size, sequence_length, hidden_size = input_tensor_shape\n",
    "\n",
    "        if len(input_shape) == 2:\n",
    "            mask_tensor_shape = tf.TensorShape(input_shape[1])\n",
    "            expected_mask_tensor_shape = tf.TensorShape(\n",
    "                    [batch_size, sequence_length, sequence_length])\n",
    "            if not expected_mask_tensor_shape.is_compatible_with(mask_tensor_shape):\n",
    "                raise ValueError(\"When passing a mask tensor to TransformerXLBlock, \"\n",
    "                                                 \"the mask tensor must be of shape [batch, \"\n",
    "                                                 \"sequence_length, sequence_length] (here %s). Got a \"\n",
    "                                                 \"mask tensor of shape %s.\" %\n",
    "                                                 (expected_mask_tensor_shape, mask_tensor_shape))\n",
    "        if hidden_size % self._num_heads != 0:\n",
    "            raise ValueError(\n",
    "                    \"The input size (%d) is not a multiple of the number of attention \"\n",
    "                    \"heads (%d)\" % (hidden_size, self._num_heads))\n",
    "        self._attention_layer = self._attention_layer_type(\n",
    "                num_heads=self._num_heads,\n",
    "                key_dim=self._head_size,\n",
    "                value_dim=self._head_size,\n",
    "                dropout=self._attention_dropout_rate,\n",
    "                use_bias=False,\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer),\n",
    "                name=\"rel_attn\")\n",
    "        self._attention_dropout = tf.keras.layers.Dropout(\n",
    "                rate=self._attention_dropout_rate)\n",
    "        self._attention_layer_norm = tf.keras.layers.LayerNormalization(\n",
    "                name=\"self_attention_layer_norm\",\n",
    "                axis=-1,\n",
    "                epsilon=self._norm_epsilon,\n",
    "                dtype=tf.float32)\n",
    "        self._inner_dense = tf.keras.layers.experimental.EinsumDense(\n",
    "                \"abc,cd->abd\",\n",
    "                output_shape=(None, self._inner_size),\n",
    "                bias_axes=\"d\",\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer),\n",
    "                name=\"inner\")\n",
    "\n",
    "        self._inner_activation_layer = tf.keras.layers.Activation(\n",
    "                self._inner_activation)\n",
    "        self._inner_dropout_layer = tf.keras.layers.Dropout(\n",
    "                rate=self._inner_dropout)\n",
    "        self._output_dense = tf.keras.layers.experimental.EinsumDense(\n",
    "                \"abc,cd->abd\",\n",
    "                output_shape=(None, hidden_size),\n",
    "                bias_axes=\"d\",\n",
    "                name=\"output\",\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer))\n",
    "        self._output_dropout = tf.keras.layers.Dropout(rate=self._dropout_rate)\n",
    "        self._output_layer_norm = tf.keras.layers.LayerNormalization(\n",
    "                name=\"output_layer_norm\",\n",
    "                axis=-1,\n",
    "                epsilon=self._norm_epsilon)\n",
    "\n",
    "        super(TransformerXLBlock, self).build(input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "                \"vocab_size\":\n",
    "                        self._vocab_size,\n",
    "                \"hidden_size\":\n",
    "                        self._hidden_size,\n",
    "                \"num_attention_heads\":\n",
    "                        self._num_heads,\n",
    "                \"head_size\":\n",
    "                        self._head_size,\n",
    "                \"inner_size\":\n",
    "                        self._inner_size,\n",
    "                \"dropout_rate\":\n",
    "                        self._dropout_rate,\n",
    "                \"attention_dropout_rate\":\n",
    "                        self._attention_dropout_rate,\n",
    "                \"norm_epsilon\":\n",
    "                        self._norm_epsilon,\n",
    "                \"inner_activation\":\n",
    "                        self._inner_activation,\n",
    "                \"kernel_initializer\":\n",
    "                        self._kernel_initializer,\n",
    "                \"inner_dropout\":\n",
    "                        self._inner_dropout,\n",
    "        }\n",
    "        base_config = super(TransformerXLBlock, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "# content_stream: `Tensor`, the input content stream. This is the standard\n",
    "# input to Transformer XL and is commonly referred to as `h` in XLNet.\n",
    "# content_attention_bias: Bias `Tensor` for content based attention of shape\n",
    "# `[num_heads, dim]`.\n",
    "# positional_attention_bias: Bias `Tensor` for position based attention of\n",
    "# shape `[num_heads, dim]`.\n",
    "# relative_position_encoding: Relative positional encoding `Tensor` of shape\n",
    "# `[B, L, dim]`.\n",
    "# state: Optional `Tensor` of shape `[B, M, E]`, where M is the length of\n",
    "# the state or memory. If passed, this is also attended over as in\n",
    "# Transformer XL.\n",
    "# content_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to content attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# query_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to query attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# target_mapping: Optional `Tensor` representing the target mapping when\n",
    "# calculating query attention.\n",
    "# Returns:\n",
    "# A `dict` object, containing the key value pairs for `content_attention`\n",
    "    def call(self,\n",
    "             content_stream,\n",
    "             content_attention_bias,\n",
    "             positional_attention_bias,\n",
    "             relative_position_encoding=None,\n",
    "             state=None,\n",
    "             content_attention_mask=None,\n",
    "             query_attention_mask=None,\n",
    "             target_mapping=None):\n",
    "        \n",
    "        attention_kwargs = dict(\n",
    "                query=content_stream,\n",
    "                value=content_stream,\n",
    "                key=content_stream,\n",
    "                attention_mask=content_attention_mask)\n",
    "\n",
    "        common_attention_kwargs = dict(\n",
    "                content_attention_bias=content_attention_bias,\n",
    "                relative_position_encoding=relative_position_encoding,\n",
    "                positional_attention_bias=positional_attention_bias,\n",
    "                state=state)\n",
    "\n",
    "        attention_kwargs.update(common_attention_kwargs)\n",
    "        attention_output = self._attention_layer(**attention_kwargs)\n",
    "\n",
    "        attention_stream = attention_output\n",
    "        input_stream = content_stream\n",
    "        attention_key = \"content_attention\"\n",
    "        attention_output = {}\n",
    "        \n",
    "        attention_stream = self._attention_dropout(attention_stream)\n",
    "        attention_stream = self._attention_layer_norm(attention_stream + input_stream)\n",
    "        inner_output = self._inner_dense(attention_stream)\n",
    "        inner_output = self._inner_activation_layer(\n",
    "                inner_output)\n",
    "        inner_output = self._inner_dropout_layer(\n",
    "                inner_output)\n",
    "        layer_output = self._output_dense(inner_output)\n",
    "        layer_output = self._output_dropout(layer_output)\n",
    "        layer_output = self._output_layer_norm(layer_output + attention_stream)\n",
    "        attention_output[attention_key] = layer_output\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8749ec30-1281-4f9d-8baa-54b890fb7949",
   "metadata": {},
   "source": [
    "---\n",
    "# TransformerXL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "019ca25d-e899-4e17-8f41-dfdc3d9bdee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_layers: The number of layers.\n",
    "# hidden_size: The hidden size.\n",
    "# num_attention_heads: The number of attention heads.\n",
    "# head_size: The dimension size of each attention head.\n",
    "# inner_size: The hidden size in feed-forward layers.\n",
    "# dropout_rate: Dropout rate used in each Transformer XL block.\n",
    "# attention_dropout_rate: Dropout rate on attention probabilities.\n",
    "# initializer: The initializer to use for attention biases.\n",
    "# tie_attention_biases: Whether or not to tie biases together. If `True`, then\n",
    "# each Transformer XL block shares the same trainable attention bias. If\n",
    "# `False`, then each block has its own attention bias. This is usually set\n",
    "# to `True`.\n",
    "# memory_length: The number of tokens to cache.\n",
    "# reuse_length: The number of tokens in the current batch to be cached\n",
    "# and reused in the future.\n",
    "# inner_activation: The activation to use in the inner layers\n",
    "# for Transformer XL blocks. Typically \"relu\" or \"gelu\".\n",
    "class TransformerXL(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 num_layers,\n",
    "                 hidden_size,\n",
    "                 maxlen,\n",
    "                 embed_dim,\n",
    "                 num_attention_heads,\n",
    "                 head_size,\n",
    "                 inner_size,\n",
    "                 dropout_rate,\n",
    "                 attention_dropout_rate,\n",
    "                 initializer,\n",
    "                 tie_attention_biases=True,\n",
    "                 memory_length=None,\n",
    "                 reuse_length=None,\n",
    "                 inner_activation=\"relu\",\n",
    "                 **kwargs):\n",
    "        super(TransformerXL, self).__init__(**kwargs)\n",
    "\n",
    "        self._vocab_size = vocab_size\n",
    "        self._initializer = initializer\n",
    "        self._num_layers = num_layers\n",
    "        self._hidden_size = hidden_size\n",
    "        self._num_attention_heads = num_attention_heads\n",
    "        self._head_size = head_size\n",
    "        self._inner_size = inner_size\n",
    "        self._inner_activation = inner_activation\n",
    "        self._dropout_rate = dropout_rate\n",
    "        self._attention_dropout_rate = attention_dropout_rate\n",
    "        self._tie_attention_biases = tie_attention_biases\n",
    "\n",
    "        self._memory_length = memory_length\n",
    "        self._reuse_length = reuse_length\n",
    "\n",
    "        if self._tie_attention_biases:\n",
    "            attention_bias_shape = [self._num_attention_heads, self._head_size]\n",
    "        else:\n",
    "            attention_bias_shape = [self._num_layers, self._num_attention_heads, self._head_size]\n",
    "\n",
    "        self.content_attention_bias = self.add_weight(\n",
    "                \"content_attention_bias\",\n",
    "                shape=attention_bias_shape,\n",
    "                dtype=tf.float32,\n",
    "                initializer=clone_initializer(self._initializer))\n",
    "        # self.positional_attention_bias = self.add_weight(\n",
    "        #         \"positional_attention_bias\",\n",
    "        #         shape=attention_bias_shape,\n",
    "        #         dtype=tf.float32,\n",
    "        #         initializer=clone_initializer(self._initializer))\n",
    "\n",
    "        self.transformer_xl_layers = []\n",
    "        for i in range(self._num_layers):\n",
    "            self.transformer_xl_layers.append(\n",
    "                    TransformerXLBlock(\n",
    "                            vocab_size=self._vocab_size,\n",
    "                            hidden_size=self._head_size * self._num_attention_heads,\n",
    "                            num_attention_heads=self._num_attention_heads,\n",
    "                            head_size=self._head_size,\n",
    "                            inner_size=self._inner_size,\n",
    "                            dropout_rate=self._dropout_rate,\n",
    "                            attention_dropout_rate=self._attention_dropout_rate,\n",
    "                            norm_epsilon=1e-12,\n",
    "                            inner_activation=self._inner_activation,\n",
    "                            kernel_initializer=\"variance_scaling\",\n",
    "                            name=\"layer_%d\" % i))\n",
    "\n",
    "        self.output_dropout = tf.keras.layers.Dropout(rate=self._dropout_rate)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "                \"vocab_size\":\n",
    "                        self._vocab_size,\n",
    "                \"num_layers\":\n",
    "                        self._num_layers,\n",
    "                \"hidden_size\":\n",
    "                        self._hidden_size,\n",
    "                \"num_attention_heads\":\n",
    "                        self._num_attention_heads,\n",
    "                \"head_size\":\n",
    "                        self._head_size,\n",
    "                \"inner_size\":\n",
    "                        self._inner_size,\n",
    "                \"dropout_rate\":\n",
    "                        self._dropout_rate,\n",
    "                \"attention_dropout_rate\":\n",
    "                        self._attention_dropout_rate,\n",
    "                \"initializer\":\n",
    "                        self._initializer,\n",
    "                \"tie_attention_biases\":\n",
    "                        self._tie_attention_biases,\n",
    "                \"memory_length\":\n",
    "                        self._memory_length,\n",
    "                \"reuse_length\":\n",
    "                        self._reuse_length,\n",
    "                \"inner_activation\":\n",
    "                        self._inner_activation,\n",
    "        }\n",
    "        base_config = super(TransformerXL, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "# content_stream: `Tensor`, the input content stream. This is the standard\n",
    "# input to Transformer XL and is commonly referred to as `h` in XLNet.\n",
    "# relative_position_encoding: Relative positional encoding `Tensor` of shape\n",
    "# `[B, L, dim]`.\n",
    "# state: Optional `Tensor` of shape `[B, M, E]`, where M is the length of\n",
    "# the state or memory. If passed, this is also attended over as in\n",
    "# Transformer XL.\n",
    "# content_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to content attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# query_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to query attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# target_mapping: Optional `Tensor` representing the target mapping when\n",
    "# calculating query attention.\n",
    "# Returns:\n",
    "# A tuple consisting of the attention output and the list of cached memory\n",
    "# states.\n",
    "# The attention output is `content_attention`\n",
    "    def call(self,\n",
    "             content_stream,\n",
    "             relative_position_encoding,\n",
    "             state=None,\n",
    "             content_attention_mask=None,\n",
    "             query_attention_mask=None,\n",
    "             target_mapping=None):\n",
    "        \n",
    "        new_mems = []\n",
    "\n",
    "        if state is None:\n",
    "            state = [None] * self._num_layers\n",
    "        for i in range(self._num_layers):\n",
    "            # cache new mems\n",
    "            new_mems.append(\n",
    "                    _cache_memory(content_stream, state[i],\n",
    "                                                self._memory_length, self._reuse_length))\n",
    "            \n",
    "            if self._tie_attention_biases:\n",
    "                content_attention_bias = self.content_attention_bias\n",
    "            else:\n",
    "                content_attention_bias = self.content_attention_bias[i]\n",
    "                \n",
    "            # if self._tie_attention_biases:\n",
    "            #     positional_attention_bias = self.positional_attention_bias\n",
    "            # else:\n",
    "            #     positional_attention_bias = self.positional_attention_bias[i]\n",
    "            positional_attention_bias = None\n",
    "            \n",
    "\n",
    "            transformer_xl_layer = self.transformer_xl_layers[i]\n",
    "            \n",
    "            transformer_xl_output = transformer_xl_layer(\n",
    "                    content_stream=content_stream,\n",
    "                    content_attention_bias=content_attention_bias,\n",
    "                    positional_attention_bias=positional_attention_bias,\n",
    "                    relative_position_encoding=relative_position_encoding,\n",
    "                    state=state[i],\n",
    "                    content_attention_mask=content_attention_mask,\n",
    "                    query_attention_mask=query_attention_mask,\n",
    "                    target_mapping=target_mapping)\n",
    "            content_stream = transformer_xl_output[\"content_attention\"]\n",
    "\n",
    "        output_stream = content_stream\n",
    "        return output_stream, new_mems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5272e222-38aa-4639-9b1d-5949af299637",
   "metadata": {},
   "source": [
    "---\n",
    "# Dna Embeddings Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8eb2673b-5fd4-4531-a4d4-dbccf3e72664",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dna_Embeddings_Model(keras.Model):\n",
    "    def __init__(self, seq_len, encoder, embed_dim):\n",
    "        super(Dna_Embeddings_Model, self).__init__()\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.encoder = encoder\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # self.input_layer = keras.layers.Input((None,seq_len))\n",
    "        # self.time_distributed_layer = keras.layers.TimeDistributed(encoder)\n",
    "        # self.dense_layer = keras.layers.Dense(embed_dim)\n",
    "    \n",
    "    def call(self, dataset, training=None):\n",
    "#         input_embeddings = self.input_layer\n",
    "#         distributed_embeddings = self.time_distributed_layer(input_embeddings)\n",
    "#         output_embeddings = self.dense_layer(distributed_embeddings)\n",
    "        \n",
    "#         model = keras.Model(input_embeddings, output_embeddings)\n",
    "              \n",
    "#         return model\n",
    "        return None\n",
    "    \n",
    "    def create_model(self):\n",
    "        input_layer = keras.layers.Input((None, self.seq_len))\n",
    "        time_distributed_layer = keras.layers.TimeDistributed(self.encoder)(input_layer)\n",
    "        dense_layer = keras.layers.Dense(self.embed_dim)(time_distributed_layer)\n",
    "        \n",
    "        model = keras.Model(input_layer, dense_layer)\n",
    "        \n",
    "        return model        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "aef5b38e-1684-444d-b797-e9afec402735",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper Parameters \n",
    "seq_len = 148\n",
    "encoder = pretrained_encoder\n",
    "embed_dim = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122da068-2c47-4e0b-8f2b-93bf591efad2",
   "metadata": {},
   "source": [
    "---\n",
    "# Test DNA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "9d9ac12d-4d2c-4083-a87b-74c9b8f1658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():  \n",
    "    dna_model = Dna_Embeddings_Model(seq_len, encoder, embed_dim)\n",
    "    dna_model = dna_model.create_model()\n",
    "    #dna_model.compile(optimizer=keras.optimizers.Adam(1e-4),loss=keras.losses.SparseCategoricalCrossentropy(from_logits = True), metrics = [keras.metrics.sparse_categorical_accuracy])\n",
    "    \n",
    "    \n",
    "    batch = dataset[0][0]\n",
    "    batch_size = dataset[0][0][0].shape\n",
    "    dna_embeddings = dna_model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a6fd7fdd-475f-4a8f-a807-9d3dc2c66e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([20, 700, 32])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dna_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "4706eece-2720-43d7-9fa3-6dc495af1862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20, 700, 32), dtype=float32, numpy=\n",
       "array([[[-3.9879537 ,  3.8753116 ,  3.8554542 , ...,  1.5293021 ,\n",
       "         -1.8139416 , -1.2552106 ],\n",
       "        [ 1.4834095 , -0.09260456,  0.46363428, ...,  3.49958   ,\n",
       "         -1.0442907 , -0.04607025],\n",
       "        [ 2.9982667 , -0.34751317, -1.2545336 , ...,  5.6417484 ,\n",
       "          1.8454205 , -0.64657986],\n",
       "        ...,\n",
       "        [ 1.6055295 ,  7.0958934 ,  7.257243  , ..., -3.9404395 ,\n",
       "          0.13447534,  8.348375  ],\n",
       "        [ 1.8501576 , -0.2726029 , -1.7018017 , ...,  6.220066  ,\n",
       "         -2.2616332 ,  0.34388867],\n",
       "        [ 3.6608975 ,  2.162143  ,  3.5645483 , ..., -4.9542446 ,\n",
       "         -0.6773416 ,  6.498264  ]],\n",
       "\n",
       "       [[ 4.423611  , -2.212044  , -3.5878837 , ...,  8.262689  ,\n",
       "         -0.76589817, -3.2996655 ],\n",
       "        [ 6.5634246 , -4.7361717 , -5.228298  , ...,  8.327922  ,\n",
       "         -7.438349  , -1.7114404 ],\n",
       "        [ 3.6485624 , -2.8724189 , -3.4627936 , ...,  7.11956   ,\n",
       "         -1.9525318 , -3.7502909 ],\n",
       "        ...,\n",
       "        [ 4.690465  , -4.5202584 , -5.5805836 , ...,  8.467158  ,\n",
       "         -6.591874  , -3.5142503 ],\n",
       "        [ 3.7335143 , -0.48385435, -2.0848057 , ...,  3.6007943 ,\n",
       "         -1.0441426 ,  2.1714642 ],\n",
       "        [ 6.1449523 , -4.6330786 , -5.285731  , ..., 11.146946  ,\n",
       "         -3.3190846 , -5.2165236 ]],\n",
       "\n",
       "       [[ 0.1828613 ,  1.4262923 ,  2.7651374 , ...,  3.864267  ,\n",
       "         -2.2162294 , -2.3557465 ],\n",
       "        [ 1.8272814 ,  0.64059794, -1.9921914 , ...,  3.238626  ,\n",
       "         -1.091549  ,  2.7160726 ],\n",
       "        [ 3.2124798 ,  0.4859018 ,  0.6615913 , ..., -2.3747716 ,\n",
       "          0.6077062 ,  3.8065841 ],\n",
       "        ...,\n",
       "        [ 2.2388604 ,  0.91210014,  0.2639048 , ...,  3.734243  ,\n",
       "         -2.8934753 ,  2.5511248 ],\n",
       "        [-5.946843  ,  5.149717  ,  6.050972  , ...,  0.9812737 ,\n",
       "         -2.816333  , -1.8716618 ],\n",
       "        [ 2.1360776 ,  2.6727934 ,  1.9890153 , ..., -0.6467366 ,\n",
       "         -0.8566817 ,  4.504888  ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.363907  ,  1.6900145 ,  3.0664737 , ...,  2.118225  ,\n",
       "         -2.0880966 , -2.097623  ],\n",
       "        [ 1.7494512 ,  1.0415485 ,  4.1427474 , ...,  2.3227384 ,\n",
       "          1.8159841 , -0.3668471 ],\n",
       "        [ 3.5190253 ,  1.9914019 ,  4.0034623 , ..., -6.752971  ,\n",
       "          0.29170343,  5.1811266 ],\n",
       "        ...,\n",
       "        [-3.1705582 ,  3.029133  ,  2.7763286 , ...,  2.4793167 ,\n",
       "         -3.2658503 , -2.8507273 ],\n",
       "        [ 4.138442  , -1.4236834 ,  1.7968932 , ...,  2.741627  ,\n",
       "          2.0259664 , -0.7210034 ],\n",
       "        [-4.8421335 ,  4.9978814 ,  6.9581237 , ...,  2.472277  ,\n",
       "         -1.9122005 , -2.4762008 ]],\n",
       "\n",
       "       [[ 5.791703  , -4.508716  , -5.1939387 , ...,  8.509528  ,\n",
       "         -7.605911  , -1.700171  ],\n",
       "        [ 3.8485234 , -0.06665768, -1.3197964 , ...,  0.39875272,\n",
       "         -5.6914334 ,  3.4856834 ],\n",
       "        [ 2.8148208 ,  2.4549875 ,  0.5396104 , ..., -2.3582335 ,\n",
       "         -5.312919  , 10.145656  ],\n",
       "        ...,\n",
       "        [ 4.8406386 , -4.5946875 , -3.9190156 , ...,  5.4475265 ,\n",
       "         -7.266304  , -2.2885795 ],\n",
       "        [ 6.8177834 , -4.901842  , -4.5989757 , ...,  7.2714047 ,\n",
       "         -1.7716811 , -2.2890854 ],\n",
       "        [ 3.8498702 , -3.1803384 , -3.9144466 , ...,  7.3115315 ,\n",
       "         -4.4707594 , -2.7561479 ]],\n",
       "\n",
       "       [[ 0.06606732,  0.9179598 ,  1.0718745 , ...,  2.2403946 ,\n",
       "         -1.4587516 , -0.06401774],\n",
       "        [ 1.7525846 ,  6.827737  ,  8.179452  , ..., -5.529333  ,\n",
       "         -0.31410637,  7.672475  ],\n",
       "        [ 2.9679391 , -1.0820372 , -1.0788169 , ...,  4.5983634 ,\n",
       "          1.0320083 , -0.88381696],\n",
       "        ...,\n",
       "        [ 3.0498185 , -0.44709253, -1.008706  , ...,  2.2730422 ,\n",
       "          0.37119925,  1.046691  ],\n",
       "        [ 2.2770317 , -0.27166247, -0.09635425, ...,  4.939694  ,\n",
       "          1.652408  , -1.0917805 ],\n",
       "        [ 4.346285  ,  4.8573036 ,  4.0595465 , ..., -2.235218  ,\n",
       "          0.8401723 ,  6.982089  ]]], dtype=float32)>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dna_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16bdc19-dcd6-4a89-8488-2d697a6925a7",
   "metadata": {},
   "source": [
    "---\n",
    "# XL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "855d43dd-e182-4c2a-961a-ff9781141f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XlModel(keras.Model):\n",
    "    def __init__(self, output_shape, embed_dim, num_layers, hidden_size, num_attention_heads, maxlen, memory_length, reuse_length, \n",
    "                                        head_size, inner_size, dropout_rate, attention_dropout_rate, initializer, vocab_size = None):\n",
    "        super(XlModel, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_attention_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.maxlen = maxlen\n",
    "        self.memory_length = memory_length\n",
    "        \n",
    "        self.transformer_xl = TransformerXL(\n",
    "                vocab_size=vocab_size,\n",
    "                num_layers=num_layers,\n",
    "                hidden_size=hidden_size,\n",
    "                num_attention_heads=num_attention_heads,\n",
    "                maxlen=maxlen,\n",
    "                embed_dim=embed_dim,\n",
    "                memory_length=memory_length,\n",
    "                reuse_length=reuse_length,\n",
    "                head_size=head_size,\n",
    "                inner_size=inner_size,\n",
    "                dropout_rate=dropout_rate,\n",
    "                attention_dropout_rate=attention_dropout_rate,\n",
    "                initializer=initializer\n",
    "            )\n",
    "  \n",
    "        self.pooling_layer = stf.PoolingByMultiHeadAttention(num_seeds=1,embed_dim=embed_dim,num_heads=1,use_layernorm=use_layernorm,pre_layernorm=pre_layernorm,\n",
    "                                                 use_keras_mha=use_keras_mha,is_final_block=True)\n",
    "\n",
    "        self.output_layer = keras.layers.Dense(output_shape, activation=keras.activations.sigmoid)\n",
    "\n",
    "\n",
    "    def call(self, x, mems, training=None):       \n",
    "      \n",
    "        encoder, mems = self.transformer_xl(content_stream=x, relative_position_encoding=None, state=mems)\n",
    "        \n",
    "        pooled = self.pooling_layer(encoder)\n",
    "    \n",
    "        classification = self.output_layer(pooled)\n",
    "        \n",
    "        return classification, mems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "2bc3ea7d-d4a1-4a1e-bf2a-7cd1c91430cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper Parameters \n",
    "output_shape = max_files\n",
    "embed_dim = 32\n",
    "num_layers = 12\n",
    "hidden_size = 64\n",
    "num_attention_heads = 8\n",
    "maxlen = seq_len\n",
    "memory_length = 0\n",
    "reuse_length = 0\n",
    "head_size = 64\n",
    "inner_size = 64\n",
    "dropout_rate = 0.0\n",
    "attention_dropout_rate = 0.0\n",
    "initializer = keras.initializers.RandomNormal(stddev=0.1) \n",
    "vocab_size = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494843f5-ba41-48a2-9030-79df47617479",
   "metadata": {},
   "source": [
    "---\n",
    "# Create DNA XL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "1ed988c2-4dd3-4309-bc0e-1168b3a2ffcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.433057249 0.556360364 0.579769313 ... 0.604828954 0.637039959 0.519640565]]\n",
      "\n",
      " [[0.511237621 0.497999549 0.613589764 ... 0.58514446 0.58947593 0.536577106]]\n",
      "\n",
      " [[0.434808761 0.554745793 0.580063283 ... 0.60638392 0.63962239 0.51991272]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.43073225 0.554157257 0.580819607 ... 0.606903076 0.640633702 0.519960403]]\n",
      "\n",
      " [[0.518595695 0.493677795 0.604711652 ... 0.582517385 0.578340471 0.539728761]]\n",
      "\n",
      " [[0.430938661 0.554437518 0.57796979 ... 0.605135798 0.638500571 0.520724714]]]\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():  \n",
    "    #Dna Model \n",
    "    dna_model = Dna_Embeddings_Model(seq_len, encoder, embed_dim)\n",
    "    dna_model = dna_model.create_model()\n",
    "    #dna_model.compile(optimizer=keras.optimizers.Adam(1e-4),loss=keras.losses.SparseCategoricalCrossentropy(from_logits = True), metrics = [keras.metrics.sparse_categorical_accuracy])\n",
    "\n",
    "    batch_dna = dataset[0][0]\n",
    "    dna_embeddings = dna_model(batch_dna)\n",
    "    \n",
    "    #XL Model\n",
    "    model = XlModel(output_shape, embed_dim, num_layers, hidden_size, num_attention_heads, maxlen, memory_length, reuse_length, head_size, inner_size, dropout_rate, attention_dropout_rate, initializer, vocab_size = None)\n",
    "    model.compile(loss = keras.losses.CategoricalCrossentropy , optimizer = keras.optimizers.Adam())\n",
    "    \n",
    "    batch_xl = dna_embeddings\n",
    "    batch_size = dataset[0][0].shape[0] #20\n",
    "    \n",
    "    mems = [*tf.zeros((num_layers, batch_size, memory_length, embed_dim))]\n",
    "    \n",
    "    output, mems = model(dna_embeddings, mems)\n",
    "    \n",
    "    tf.print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "1a070d87-e26a-4670-be67-2baa5ec5d8ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([20, 1, 1260])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "17e66b19-b53d-428d-af50-faf8b078d749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30., 27., 11., 56., 32., 36., 56., 32., 36., 57., 37., 63., 65.,\n",
       "       75.,  3., 15., 76.,  7., 38., 65., 77., 12., 63., 67., 87., 61.,\n",
       "       55., 25.,  2., 11., 57., 38., 68., 92., 88., 66., 81., 32., 37.,\n",
       "       60., 53., 18., 93., 90., 75.,  3., 17., 87., 62., 61., 57., 38.,\n",
       "       65., 75.,  0.,  2., 10., 52., 11., 57., 36., 57., 36., 55., 27.,\n",
       "       12., 61., 57., 37., 63., 66., 82., 38., 68., 91., 80., 25.,  2.,\n",
       "       13., 66., 82., 36., 57., 38., 67., 88., 67., 85., 50.,  0.,  2.,\n",
       "       11., 56., 31., 31., 31., 32., 37., 61., 58., 41., 80., 25.,  1.,\n",
       "        8., 42., 87., 62., 62., 60., 52., 12., 62., 63., 66., 80., 26.,\n",
       "        7., 36., 57., 35., 53., 15., 76.,  8., 42., 85., 53., 16., 82.,\n",
       "       35., 51.,  8., 41., 82., 35., 50.,  2., 12., 61., 55., 27., 12.,\n",
       "       60., 52., 10., 52., 12.])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "4a814efb-ef84-4c55-8a06-8dd3fadd540f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([127,  77, 163,  41,  77,  40, 101, 134, 162,  81, 143, 117,  99,\n",
       "       190,  54, 188, 131, 155,  19, 159], dtype=int32)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "94a09406-3472-4035-8061-e31f9ac9a142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1260,), dtype=float32, numpy=\n",
       "array([0.43305725, 0.55636036, 0.5797693 , ..., 0.60482895, 0.63703996,\n",
       "       0.51964056], dtype=float32)>"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "a993c95e-bb05-4376-896d-765b9be9d599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max: 0.67633617\n",
      "(array([771]),)\n"
     ]
    }
   ],
   "source": [
    "print('Max:', np.amax(output[0][0]))\n",
    "print(np.where(output[0][0] == np.amax(output[0][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "92b02390-c88e-40e1-9686-cb7fc2581e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"xl_model_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transformer_xl_18 (Transfor  multiple                 838784    \n",
      " merXL)                                                          \n",
      "                                                                 \n",
      " pooling_by_multi_head_atten  multiple                 5312      \n",
      " tion_18 (PoolingByMultiHead                                     \n",
      " Attention)                                                      \n",
      "                                                                 \n",
      " dense_168 (Dense)           multiple                  41580     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 885,676\n",
      "Trainable params: 885,676\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752fb718-67f6-44ff-aa6d-c889c0254b82",
   "metadata": {},
   "source": [
    "---\n",
    "# Custom Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fafad8d-f020-49b0-8233-bcfe54b88309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(batch, mems):\n",
    "    x, y = batch\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        result, new_mems = model(x, mems, y, training=True)\n",
    "        loss = MaskedSparseCategoricalCrossentropy(y_post, result)\n",
    "        \n",
    "    accuracy = keras.metrics(SparseCategoricalAccuracy(y, result)\n",
    "        \n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    return loss, accuracy, new_mems\n",
    "\n",
    "@tf.function()\n",
    "def dist_train_step(batch, mems):\n",
    "    losses, accuracy, new_mems = strategy.run(train_step, args=(batch, mems))\n",
    "    return strategy.reduce(tf.distribute.ReduceOp.SUM, losses, axis=None), accuracy, new_mems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5a6127f4-fb94-4bf2-b3cf-fae83ef0aa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "history_loss = []\n",
    "history_accuracy = []\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a260118e-c39d-4232-8ee2-11ae86d8c8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/200 Loss: 0.42261186242103577 Accuracy = 0.9020711183547974"
     ]
    }
   ],
   "source": [
    "with strategy.scope():    \n",
    "    for epoch in range(epochs):\n",
    "        batch = (x_train, pre_y_train, post_y_train)\n",
    "\n",
    "        loss, accuracy, mems = dist_train_step(batch, mems)\n",
    "        \n",
    "        history_loss.append(loss)\n",
    "        history_accuracy.append(accuracy) \n",
    "        \n",
    "        print(f\"\\r{epoch+1}/{epochs} Loss: {loss} Accuracy = {accuracy}\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23810b3f-a9d8-4a7a-a967-130ab85ace1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.subplot(211)\n",
    "plt.plot(history_accuracy)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.subplot(212)\n",
    "plt.plot(history_loss)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
