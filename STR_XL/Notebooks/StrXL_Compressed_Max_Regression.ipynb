{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad2c14f8-8dd8-459f-81fe-a6708e6a8355",
   "metadata": {},
   "source": [
    "---\n",
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dcad43d-67f7-49b9-84ce-6d91a0f468a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e7cb9df-5746-44a3-998a-b671b284957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c507a85-fe6f-4d3c-ae1f-16914fdec693",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../../../deep-learning-dna\")\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../../deep-learning-dna/common\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31b9c98a-b4ee-4667-89a3-5f04549f7d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fda64db1-8058-4e15-b2d4-1a3e64f23fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import math\n",
    "import string\n",
    "\n",
    "from Attention import Set_Transformer\n",
    "from common.models import dnabert\n",
    "from common import dna\n",
    "from lmdbm import Lmdb\n",
    "from common.data import DnaSequenceGenerator, DnaLabelType, DnaSampleGenerator, find_dbs\n",
    "import wandb\n",
    "\n",
    "import tf_utils as tfu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dfb3b1a-2db7-4463-a208-b5d29e20f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tfu.devices.select_gpu(0, use_dynamic_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54593a0f-b5b4-401e-91e2-8c1a1598432e",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65e83dc7-9349-4aab-b240-1e0d39988513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(batch_size, length=5):\n",
    "    x = np.random.randint(1, 100, (batch_size, length))\n",
    "    y = np.max(x, axis=1)\n",
    "    return x, y # (batch_size, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f47ff082-2d44-4467-872b-c925150812b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = gen_data(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df9ded01-96fb-4df2-b52f-429f623ab30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5) (3,)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0adef207-d041-409c-9f7b-f57799812e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[99, 85, 49, 44,  2],\n",
       "       [81, 40, 24, 44, 92],\n",
       "       [13, 74, 34, 36, 65]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df941e9f-16d1-4d9d-a6ec-bec7469d2fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([99, 92, 74])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd855356-4e5a-4047-ae81-323277b11b44",
   "metadata": {
    "tags": []
   },
   "source": [
    "--- \n",
    "# Batch Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835c39dd-9fb5-49bf-8763-14ca694a1a35",
   "metadata": {},
   "source": [
    "---\n",
    "# Compress Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ac5b57b-b6a7-4aea-bd67-6d2aae1310c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compress_Memory(keras.layers.Layer):\n",
    "    def __init__(self, num_seeds, embed_dim, num_heads, use_layernorm, pre_layernorm, use_keras_mha):\n",
    "        super(Compress_Memory, self).__init__()\n",
    "        \n",
    "        self.num_seeds = num_seeds\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.use_layernorm = use_layernorm\n",
    "        self.pre_layernorm = pre_layernorm\n",
    "        self.use_keras_mha = use_keras_mha\n",
    "        \n",
    "        self.compresser = Set_Transformer.PoolingByMultiHeadAttention(num_seeds=self.num_seeds,embed_dim=self.embed_dim,num_heads=self.num_heads,use_layernorm=self.use_layernorm,pre_layernorm=self.pre_layernorm, use_keras_mha=self.use_keras_mha, is_final_block=True)\n",
    "        \n",
    "    def call(self, current_state, previous_state):\n",
    "        if previous_state is None:\n",
    "            new_mem = self.compresser(current_state)\n",
    "        else:\n",
    "            new_mem = self.compresser(tf.concat([previous_state, current_state], 1))\n",
    "\n",
    "        return tf.stop_gradient(new_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb50453f-628c-412e-942b-d486d01c6b8a",
   "metadata": {},
   "source": [
    "---\n",
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bf9ef79-9ede-4271-8542-223cdd1e11ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(keras.layers.Layer):\n",
    "    def __init__(self, num_induce, embed_dim, num_heads, use_layernorm, pre_layernorm, use_keras_mha):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.num_induce = num_induce\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.use_layernorm = use_layernorm\n",
    "        self.pre_layernorm = pre_layernorm\n",
    "        self.use_keras_mha = use_keras_mha\n",
    "        \n",
    "        if self.num_induce == 0:       \n",
    "            self.attention = (Set_Transformer.SetAttentionBlock(embed_dim=self.embed_dim, num_heads=self.num_heads, use_layernorm=self.use_layernorm,pre_layernorm=self.pre_layernorm,use_keras_mha=self.use_keras_mha))\n",
    "        else:\n",
    "            self.attention = Set_Transformer.InducedSetAttentionBlock(embed_dim=self.embed_dim, num_heads=self.num_heads, num_induce=self.num_induce, use_layernorm=self.use_layernorm, pre_layernorm=self.pre_layernorm, use_keras_mha=self.use_keras_mha)\n",
    "\n",
    "    def call(self, data, mems=None):\n",
    "            attention = self.attention([data, mems])\n",
    "                \n",
    "            return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c32bb5-4520-4802-a8f0-5f7849905a83",
   "metadata": {},
   "source": [
    "---\n",
    "# XL Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7245b7dd-a3b2-464a-9f5b-7efd4a9d0ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerXLBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 num_induce, \n",
    "                 embed_dim,\n",
    "                 num_heads,\n",
    "                 use_layernorm,\n",
    "                 pre_layernorm,\n",
    "                 use_keras_mha,):\n",
    "\n",
    "        super(TransformerXLBlock, self).__init__()\n",
    "        \n",
    "        self.num_induce = num_induce\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.use_layernorm = use_layernorm\n",
    "        self.pre_layernorm = pre_layernorm\n",
    "        self.use_keras_mha = use_keras_mha\n",
    "        \n",
    "        self.attention = Attention\n",
    "        \n",
    "        self.attention_layer = self.attention(self.num_induce, self.embed_dim, self.num_heads, self.use_layernorm, self.pre_layernorm, self.use_keras_mha)\n",
    "\n",
    "   \n",
    "    def call(self,\n",
    "             content_stream,\n",
    "             state=None):\n",
    "        \n",
    "        attention_output = self.attention_layer(content_stream, state)\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8749ec30-1281-4f9d-8baa-54b890fb7949",
   "metadata": {},
   "source": [
    "---\n",
    "# Transformer XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "019ca25d-e899-4e17-8f41-dfdc3d9bdee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerXL(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 mem_switched, \n",
    "                 num_seeds,\n",
    "                 num_layers,\n",
    "                 num_induce,\n",
    "                 embed_dim,\n",
    "                 num_heads,\n",
    "                 dropout_rate,\n",
    "                 use_layernorm=True,\n",
    "                 pre_layernorm=True, \n",
    "                 use_keras_mha=True):\n",
    "        \n",
    "        super(TransformerXL, self).__init__()\n",
    "\n",
    "        self.mem_switched = mem_switched\n",
    "        self.num_seeds = num_seeds\n",
    "        self.num_layers = num_layers\n",
    "        self.num_induce = num_induce\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_layernorm = use_layernorm\n",
    "        self.pre_layernorm = pre_layernorm\n",
    "        self.use_keras_mha = use_keras_mha\n",
    "\n",
    "        self.compresser = Compress_Memory\n",
    "        \n",
    "        self.compress_mems = self.compresser(self.num_seeds,\n",
    "                                        self.embed_dim,\n",
    "                                        self.num_heads,\n",
    "                                        self.use_layernorm,\n",
    "                                        self.pre_layernorm, \n",
    "                                        self.use_keras_mha)\n",
    "        \n",
    "        self.transformer_xl_layers = []\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            self.transformer_xl_layers.append(\n",
    "                    TransformerXLBlock(self.num_induce,\n",
    "                                        self.embed_dim,\n",
    "                                        self.num_heads,\n",
    "                                        self.use_layernorm,\n",
    "                                        self.pre_layernorm, \n",
    "                                        self.use_keras_mha))\n",
    "\n",
    "        self.output_dropout = tf.keras.layers.Dropout(rate=self.dropout_rate)\n",
    "\n",
    "    def call(self,\n",
    "             content_stream,\n",
    "             state=None):\n",
    "        \n",
    "        new_mems = []\n",
    "\n",
    "        if state is None:\n",
    "            state = [None] * self.num_layers\n",
    "            \n",
    "        for i in range(self.num_layers):\n",
    "            if self.mem_switched == False:\n",
    "                new_mems.append(self.compress_mems(content_stream, state[i]))\n",
    "            \n",
    "            transformer_xl_layer = self.transformer_xl_layers[i]\n",
    "            \n",
    "            transformer_xl_output = transformer_xl_layer(content_stream=content_stream,\n",
    "                                                        state=state[i])\n",
    "            \n",
    "            content_stream = self.output_dropout(transformer_xl_output)\n",
    "            \n",
    "            if self.mem_switched == True:\n",
    "                new_mems.append(self.compress_mems(content_stream, state[i]))\n",
    "\n",
    "        output_stream = content_stream\n",
    "        return output_stream, new_mems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a127a4b5-15e3-46e3-838b-a5e18e4d8dc8",
   "metadata": {},
   "source": [
    "---\n",
    "# Xl Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "855d43dd-e182-4c2a-961a-ff9781141f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XlModel(keras.Model):\n",
    "    def __init__(self, mem_switched, num_seeds_mems, max_files, encoder, block_size, max_set_len, num_induce, embed_dim, num_layers, num_heads, dropout_rate, num_seeds_output, use_layernorm, pre_layernorm, use_keras_mha):\n",
    "        super(XlModel, self).__init__()\n",
    "        \n",
    "        self.mem_switched = mem_switched\n",
    "        self.num_seeds_mems = num_seeds_mems\n",
    "        self.max_files = max_files\n",
    "        self.encoder = encoder\n",
    "        self.block_size = block_size\n",
    "        self.max_set_len = max_set_len\n",
    "        self.num_induce = num_induce\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_seeds_output = num_seeds_output\n",
    "        self.use_layernorm = use_layernorm\n",
    "        self.pre_layernorm = pre_layernorm\n",
    "        self.use_keras_mha = use_keras_mha\n",
    "        \n",
    "\n",
    "        self.linear_layer = keras.layers.Dense(self.embed_dim)\n",
    "        \n",
    "        self.transformer_xl = TransformerXL(self.mem_switched,\n",
    "                                            self.num_seeds_mems,\n",
    "                                            self.num_layers,\n",
    "                                             self.num_induce,\n",
    "                                             self.embed_dim,\n",
    "                                             self.num_heads,\n",
    "                                             self.dropout_rate,\n",
    "                                             self.use_layernorm,\n",
    "                                             self.pre_layernorm,\n",
    "                                             self.use_keras_mha)\n",
    "        \n",
    "\n",
    "        self.pooling_layer = Set_Transformer.PoolingByMultiHeadAttention(num_seeds=self.num_seeds_output,embed_dim=self.embed_dim,num_heads=self.num_heads,use_layernorm=self.use_layernorm,pre_layernorm=self.pre_layernorm, use_keras_mha=self.use_keras_mha, is_final_block=True)\n",
    "    \n",
    "        self.dropout_layer = keras.layers.Dropout(.5)\n",
    "    \n",
    "        self.dense_layer = keras.layers.Dense(1) \n",
    "        \n",
    "    \n",
    "    def call(self, x, training=None):        \n",
    " \n",
    "        mems = tf.zeros((self.num_layers, tf.shape(x)[0], self.num_seeds_mems, self.embed_dim))\n",
    "    \n",
    "        x = tf.expand_dims(x, axis=2)\n",
    "        \n",
    "        linear_transform = self.linear_layer(x)\n",
    "\n",
    "        for i in range(0, self.max_set_len, self.block_size):\n",
    "            block = linear_transform[:,i:i+self.block_size]\n",
    "            \n",
    "            output, mems = self.transformer_xl(content_stream=block, state=mems)\n",
    "            \n",
    "        pooling = self.pooling_layer(output)\n",
    "        \n",
    "        dropout = self.dropout_layer(pooling)\n",
    "\n",
    "        dense = self.dense_layer(dropout)\n",
    "\n",
    "        output = tf.reshape(dense, tf.shape(dense)[:2])    \n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3469de1-250d-4c14-bb79-a4e74f0fe93e",
   "metadata": {},
   "source": [
    "---\n",
    "# Xl Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2bc3ea7d-d4a1-4a1e-bf2a-7cd1c91430cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xl Parameters\n",
    "encoder = 0\n",
    "max_files = 0\n",
    "mem_switched = False\n",
    "num_seeds_mem = 200\n",
    "num_induce = 0\n",
    "embed_dim = 64\n",
    "num_layers = 4\n",
    "num_heads = 4\n",
    "dropout_rate = 0.01\n",
    "num_seeds_output = 1\n",
    "use_layernorm = True\n",
    "pre_layernorm = True\n",
    "use_keras_mha = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89649663-7d18-48bc-8678-ce5f093908cc",
   "metadata": {},
   "source": [
    "---\n",
    "# Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1815c6f5-f189-4cf5-8e03-e9401fdf885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 20\n",
    "length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9efa4a61-2c5a-4db8-a5a1-8d93c1c1bd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = gen_data(batch_size=10, length=length)\n",
    "vx, vy = gen_data(batch_size=10, length=length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d2ab7b08-d208-4fb7-8ef8-353d7fe6afbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_set_len = length\n",
    "set_len = length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "821b0c4a-6d05-4867-a713-474bfdbff881",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XlModel(mem_switched, num_seeds_mem, max_files, encoder, block_size, max_set_len, num_induce, embed_dim, num_layers, num_heads, dropout_rate, num_seeds_output, use_layernorm, pre_layernorm, use_keras_mha)\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-3),loss=keras.losses.MeanAbsoluteError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7ddcc096-cb9a-4c19-a838-0aa92a6a13bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "25dbaac2-1401-4667-bf00-a64eaa433fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([95, 99, 87, 97, 99])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3b4c7d59-57e3-432d-96b3-fbd2edf314d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[90.72172],\n",
       "       [90.75025],\n",
       "       [90.72575],\n",
       "       [90.72146],\n",
       "       [90.72817]], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.predict(x[:5]))\n",
    "#tf.argmax(model.predict(x[:5]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "33ad1097-7e06-4965-a034-64c924d381bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "926815e4-e254-4cad-a0fd-55fd0621db4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Seeds:0', 'xl_model_2/transformer_xl_1/compress__memory_1/pooling_by_multi_head_attention_2/multi_head_attention_block_2/multi_head_attention_6/query/kernel:0', 'xl_model_2/transformer_xl_1/compress__memory_1/pooling_by_multi_head_attention_2/multi_head_attention_block_2/multi_head_attention_6/query/bias:0', 'xl_model_2/transformer_xl_1/compress__memory_1/pooling_by_multi_head_attention_2/multi_head_attention_block_2/multi_head_attention_6/key/kernel:0', 'xl_model_2/transformer_xl_1/compress__memory_1/pooling_by_multi_head_attention_2/multi_head_attention_block_2/multi_head_attention_6/key/bias:0', 'xl_model_2/transformer_xl_1/compress__memory_1/pooling_by_multi_head_attention_2/multi_head_attention_block_2/multi_head_attention_6/value/kernel:0', 'xl_model_2/transformer_xl_1/compress__memory_1/pooling_by_multi_head_attention_2/multi_head_attention_block_2/multi_head_attention_6/value/bias:0', 'xl_model_2/transformer_xl_1/compress__memory_1/pooling_by_multi_head_attention_2/multi_head_attention_block_2/multi_head_attention_6/attention_output/kernel:0', 'xl_model_2/transformer_xl_1/compress__memory_1/pooling_by_multi_head_attention_2/multi_head_attention_block_2/multi_head_attention_6/attention_output/bias:0', 'dense_15/kernel:0', 'dense_15/bias:0', 'dense_16/kernel:0', 'dense_16/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Seeds:0', 'xl_model_2/transformer_xl_1/compress__memory_1/pooling_by_multi_head_attention_2/multi_head_attention_block_2/multi_head_attention_6/query/kernel:0', 'xl_model_2/transformer_xl_1/compress__memory_1/pooling_by_multi_head_attention_2/multi_head_attention_block_2/multi_head_attention_6/query/bias:0', 'xl_model_2/transformer_xl_1/compress__memory_1/pooling_by_multi_head_attention_2/multi_head_attention_block_2/multi_head_attention_6/key/kernel:0', 'xl_model_2/transformer_xl_1/compress__memory_1/pooling_by_multi_head_attention_2/multi_head_attention_block_2/multi_head_attention_6/key/bias:0', 'xl_model_2/transformer_xl_1/compress__memory_1/pooling_by_multi_head_attention_2/multi_head_attention_block_2/multi_head_attention_6/value/kernel:0', 'xl_model_2/transformer_xl_1/compress__memory_1/pooling_by_multi_head_attention_2/multi_head_attention_block_2/multi_head_attention_6/value/bias:0', 'xl_model_2/transformer_xl_1/compress__memory_1/pooling_by_multi_head_attention_2/multi_head_attention_block_2/multi_head_attention_6/attention_output/kernel:0', 'xl_model_2/transformer_xl_1/compress__memory_1/pooling_by_multi_head_attention_2/multi_head_attention_block_2/multi_head_attention_6/attention_output/bias:0', 'dense_15/kernel:0', 'dense_15/bias:0', 'dense_16/kernel:0', 'dense_16/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "1/1 [==============================] - 4s 4s/step - loss: 91.9274 - val_loss: 88.9508\n",
      "Epoch 2/1500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 89.5033 - val_loss: 88.3837\n",
      "Epoch 3/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 87.7763 - val_loss: 87.9464\n",
      "Epoch 4/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 88.1541 - val_loss: 87.4547\n",
      "Epoch 5/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 86.4091 - val_loss: 86.9895\n",
      "Epoch 6/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 86.4938 - val_loss: 86.6464\n",
      "Epoch 7/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 85.9909 - val_loss: 86.3666\n",
      "Epoch 8/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 86.9639 - val_loss: 86.1877\n",
      "Epoch 9/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 86.0394 - val_loss: 86.0121\n",
      "Epoch 10/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 86.3899 - val_loss: 85.8392\n",
      "Epoch 11/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 86.1557 - val_loss: 85.6753\n",
      "Epoch 12/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 86.4981 - val_loss: 85.5194\n",
      "Epoch 13/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 85.8419 - val_loss: 85.3920\n",
      "Epoch 14/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 84.0871 - val_loss: 85.2767\n",
      "Epoch 15/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 85.3857 - val_loss: 85.1740\n",
      "Epoch 16/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 84.8406 - val_loss: 85.0779\n",
      "Epoch 17/1500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 85.1780 - val_loss: 84.9860\n",
      "Epoch 18/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 85.3876 - val_loss: 84.8904\n",
      "Epoch 19/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 83.7884 - val_loss: 84.8022\n",
      "Epoch 20/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 84.1332 - val_loss: 84.7167\n",
      "Epoch 21/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 84.7046 - val_loss: 84.6389\n",
      "Epoch 22/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 85.2811 - val_loss: 84.5680\n",
      "Epoch 23/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 84.6947 - val_loss: 84.4969\n",
      "Epoch 24/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 84.6369 - val_loss: 84.4353\n",
      "Epoch 25/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 84.4535 - val_loss: 84.3853\n",
      "Epoch 26/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 84.3868 - val_loss: 84.3430\n",
      "Epoch 27/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 84.4926 - val_loss: 84.2983\n",
      "Epoch 28/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 84.4395 - val_loss: 84.2406\n",
      "Epoch 29/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 83.2957 - val_loss: 84.1832\n",
      "Epoch 30/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 84.8527 - val_loss: 84.1375\n",
      "Epoch 31/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 83.8117 - val_loss: 84.0826\n",
      "Epoch 32/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 83.6616 - val_loss: 84.0116\n",
      "Epoch 33/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 83.9044 - val_loss: 83.9387\n",
      "Epoch 34/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 83.6878 - val_loss: 83.8677\n",
      "Epoch 35/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 83.9928 - val_loss: 83.8038\n",
      "Epoch 36/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 84.7886 - val_loss: 83.7458\n",
      "Epoch 37/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 83.4404 - val_loss: 83.6896\n",
      "Epoch 38/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 83.9817 - val_loss: 83.6319\n",
      "Epoch 39/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 83.3713 - val_loss: 83.5801\n",
      "Epoch 40/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 84.2600 - val_loss: 83.5294\n",
      "Epoch 41/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 84.7325 - val_loss: 83.4768\n",
      "Epoch 42/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 83.1188 - val_loss: 83.4225\n",
      "Epoch 43/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 82.8649 - val_loss: 83.3705\n",
      "Epoch 44/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 82.8300 - val_loss: 83.3181\n",
      "Epoch 45/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 82.2854 - val_loss: 83.2663\n",
      "Epoch 46/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 83.9150 - val_loss: 83.2175\n",
      "Epoch 47/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 83.3175 - val_loss: 83.1687\n",
      "Epoch 48/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 83.8490 - val_loss: 83.1084\n",
      "Epoch 49/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 82.4454 - val_loss: 83.0363\n",
      "Epoch 50/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 82.0205 - val_loss: 82.9676\n",
      "Epoch 51/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 83.5171 - val_loss: 82.9082\n",
      "Epoch 52/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 82.8272 - val_loss: 82.8569\n",
      "Epoch 53/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 83.4449 - val_loss: 82.8111\n",
      "Epoch 54/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 82.4458 - val_loss: 82.7645\n",
      "Epoch 55/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 82.3617 - val_loss: 82.7112\n",
      "Epoch 56/1500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 84.0044 - val_loss: 82.6579\n",
      "Epoch 57/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 82.5613 - val_loss: 82.6009\n",
      "Epoch 58/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 82.7187 - val_loss: 82.5449\n",
      "Epoch 59/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 81.1145 - val_loss: 82.4872\n",
      "Epoch 60/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 82.6082 - val_loss: 82.4358\n",
      "Epoch 61/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 82.9052 - val_loss: 82.3887\n",
      "Epoch 62/1500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 81.8335 - val_loss: 82.3381\n",
      "Epoch 63/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 82.1927 - val_loss: 82.2846\n",
      "Epoch 64/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 82.0391 - val_loss: 82.2307\n",
      "Epoch 65/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 82.2757 - val_loss: 82.1856\n",
      "Epoch 66/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 81.4384 - val_loss: 82.1260\n",
      "Epoch 67/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 81.6994 - val_loss: 82.0623\n",
      "Epoch 68/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 80.9225 - val_loss: 81.9891\n",
      "Epoch 69/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 81.4265 - val_loss: 81.9229\n",
      "Epoch 70/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 81.6100 - val_loss: 81.8584\n",
      "Epoch 71/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 82.6099 - val_loss: 81.7984\n",
      "Epoch 72/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 81.8169 - val_loss: 81.7433\n",
      "Epoch 73/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 81.0063 - val_loss: 81.6927\n",
      "Epoch 74/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 82.1597 - val_loss: 81.6448\n",
      "Epoch 75/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 81.6796 - val_loss: 81.5972\n",
      "Epoch 76/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 80.5865 - val_loss: 81.5490\n",
      "Epoch 77/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 82.3739 - val_loss: 81.4949\n",
      "Epoch 78/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 82.4038 - val_loss: 81.4360\n",
      "Epoch 79/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 81.1783 - val_loss: 81.3749\n",
      "Epoch 80/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 80.8082 - val_loss: 81.3207\n",
      "Epoch 81/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 80.7854 - val_loss: 81.2700\n",
      "Epoch 82/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 79.8651 - val_loss: 81.2206\n",
      "Epoch 83/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 80.5137 - val_loss: 81.1689\n",
      "Epoch 84/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 80.2565 - val_loss: 81.1100\n",
      "Epoch 85/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 81.7929 - val_loss: 81.0527\n",
      "Epoch 86/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 81.9648 - val_loss: 80.9873\n",
      "Epoch 87/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 80.8023 - val_loss: 80.9183\n",
      "Epoch 88/1500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 80.8838 - val_loss: 80.8519\n",
      "Epoch 89/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 80.5999 - val_loss: 80.7954\n",
      "Epoch 90/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 80.6031 - val_loss: 80.7454\n",
      "Epoch 91/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 81.4030 - val_loss: 80.6952\n",
      "Epoch 92/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 79.7692 - val_loss: 80.6439\n",
      "Epoch 93/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 79.9881 - val_loss: 80.5866\n",
      "Epoch 94/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 81.0303 - val_loss: 80.5242\n",
      "Epoch 95/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 80.4192 - val_loss: 80.4653\n",
      "Epoch 96/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 80.0269 - val_loss: 80.4039\n",
      "Epoch 97/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 78.0139 - val_loss: 80.3425\n",
      "Epoch 98/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 79.1647 - val_loss: 80.2782\n",
      "Epoch 99/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 79.9691 - val_loss: 80.2194\n",
      "Epoch 100/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 80.7101 - val_loss: 80.1636\n",
      "Epoch 101/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 80.7228 - val_loss: 80.1099\n",
      "Epoch 102/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 79.5951 - val_loss: 80.0531\n",
      "Epoch 103/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 79.7269 - val_loss: 79.9942\n",
      "Epoch 104/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 79.2420 - val_loss: 79.9333\n",
      "Epoch 105/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 80.1665 - val_loss: 79.8733\n",
      "Epoch 106/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 79.6513 - val_loss: 79.8146\n",
      "Epoch 107/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 80.6938 - val_loss: 79.7661\n",
      "Epoch 108/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 80.1457 - val_loss: 79.7262\n",
      "Epoch 109/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 78.6423 - val_loss: 79.6836\n",
      "Epoch 110/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 79.1110 - val_loss: 79.6321\n",
      "Epoch 111/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 78.9614 - val_loss: 79.5790\n",
      "Epoch 112/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 79.3545 - val_loss: 79.5145\n",
      "Epoch 113/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 80.8518 - val_loss: 79.4410\n",
      "Epoch 114/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 78.9611 - val_loss: 79.3698\n",
      "Epoch 115/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 78.1294 - val_loss: 79.2965\n",
      "Epoch 116/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 78.4706 - val_loss: 79.2305\n",
      "Epoch 117/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 79.6729 - val_loss: 79.1702\n",
      "Epoch 118/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 78.9691 - val_loss: 79.1145\n",
      "Epoch 119/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 78.9501 - val_loss: 79.0618\n",
      "Epoch 120/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 79.2460 - val_loss: 79.0086\n",
      "Epoch 121/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 79.2257 - val_loss: 78.9500\n",
      "Epoch 122/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 78.8067 - val_loss: 78.8868\n",
      "Epoch 123/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 78.3768 - val_loss: 78.8252\n",
      "Epoch 124/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 78.3819 - val_loss: 78.7644\n",
      "Epoch 125/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 77.9789 - val_loss: 78.7057\n",
      "Epoch 126/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 78.4008 - val_loss: 78.6437\n",
      "Epoch 127/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 78.7859 - val_loss: 78.5819\n",
      "Epoch 128/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 78.1506 - val_loss: 78.5210\n",
      "Epoch 129/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 78.0147 - val_loss: 78.4589\n",
      "Epoch 130/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 78.7993 - val_loss: 78.3968\n",
      "Epoch 131/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 76.7597 - val_loss: 78.3366\n",
      "Epoch 132/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 78.0946 - val_loss: 78.2792\n",
      "Epoch 133/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 77.3507 - val_loss: 78.2200\n",
      "Epoch 134/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 78.4508 - val_loss: 78.1642\n",
      "Epoch 135/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 77.5253 - val_loss: 78.1062\n",
      "Epoch 136/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 78.1171 - val_loss: 78.0488\n",
      "Epoch 137/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 77.7766 - val_loss: 77.9885\n",
      "Epoch 138/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 77.7125 - val_loss: 77.9303\n",
      "Epoch 139/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 78.7493 - val_loss: 77.8724\n",
      "Epoch 140/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 78.2257 - val_loss: 77.8139\n",
      "Epoch 141/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 79.3863 - val_loss: 77.7587\n",
      "Epoch 142/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 77.9455 - val_loss: 77.7022\n",
      "Epoch 143/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 78.9955 - val_loss: 77.6427\n",
      "Epoch 144/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 77.6239 - val_loss: 77.5777\n",
      "Epoch 145/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 76.7037 - val_loss: 77.5103\n",
      "Epoch 146/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 77.2055 - val_loss: 77.4400\n",
      "Epoch 147/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 77.3917 - val_loss: 77.3679\n",
      "Epoch 148/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 77.2013 - val_loss: 77.3009\n",
      "Epoch 149/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 76.5027 - val_loss: 77.2400\n",
      "Epoch 150/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 77.2346 - val_loss: 77.1801\n",
      "Epoch 151/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 76.4202 - val_loss: 77.1221\n",
      "Epoch 152/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 75.9619 - val_loss: 77.0653\n",
      "Epoch 153/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 77.5567 - val_loss: 77.0151\n",
      "Epoch 154/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 75.5462 - val_loss: 76.9641\n",
      "Epoch 155/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 76.3773 - val_loss: 76.9125\n",
      "Epoch 156/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 76.5495 - val_loss: 76.8545\n",
      "Epoch 157/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 78.0135 - val_loss: 76.7886\n",
      "Epoch 158/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 77.0138 - val_loss: 76.7285\n",
      "Epoch 159/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 76.5531 - val_loss: 76.6736\n",
      "Epoch 160/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 76.3770 - val_loss: 76.6208\n",
      "Epoch 161/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 75.5566 - val_loss: 76.5639\n",
      "Epoch 162/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 76.2594 - val_loss: 76.4908\n",
      "Epoch 163/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 75.8304 - val_loss: 76.4212\n",
      "Epoch 164/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 76.6356 - val_loss: 76.3550\n",
      "Epoch 165/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 76.5358 - val_loss: 76.2881\n",
      "Epoch 166/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 77.1214 - val_loss: 76.2240\n",
      "Epoch 167/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 77.3784 - val_loss: 76.1647\n",
      "Epoch 168/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 75.5420 - val_loss: 76.1024\n",
      "Epoch 169/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 75.1008 - val_loss: 76.0361\n",
      "Epoch 170/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 76.1798 - val_loss: 75.9724\n",
      "Epoch 171/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 76.4664 - val_loss: 75.9089\n",
      "Epoch 172/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 75.3903 - val_loss: 75.8445\n",
      "Epoch 173/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 75.3094 - val_loss: 75.7846\n",
      "Epoch 174/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 75.1042 - val_loss: 75.7330\n",
      "Epoch 175/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 76.0134 - val_loss: 75.6876\n",
      "Epoch 176/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 77.0249 - val_loss: 75.6383\n",
      "Epoch 177/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 74.2137 - val_loss: 75.5842\n",
      "Epoch 178/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 75.2270 - val_loss: 75.5318\n",
      "Epoch 179/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 75.3999 - val_loss: 75.4747\n",
      "Epoch 180/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 74.1738 - val_loss: 75.4105\n",
      "Epoch 181/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 73.6046 - val_loss: 75.3445\n",
      "Epoch 182/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 75.3025 - val_loss: 75.2751\n",
      "Epoch 183/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 75.3711 - val_loss: 75.2115\n",
      "Epoch 184/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 76.2646 - val_loss: 75.1536\n",
      "Epoch 185/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 74.8089 - val_loss: 75.1023\n",
      "Epoch 186/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 75.3642 - val_loss: 75.0541\n",
      "Epoch 187/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 74.3512 - val_loss: 75.0064\n",
      "Epoch 188/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 76.0829 - val_loss: 74.9576\n",
      "Epoch 189/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 74.5011 - val_loss: 74.9038\n",
      "Epoch 190/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 74.8028 - val_loss: 74.8320\n",
      "Epoch 191/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 75.3053 - val_loss: 74.7522\n",
      "Epoch 192/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 75.7907 - val_loss: 74.6692\n",
      "Epoch 193/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 74.3944 - val_loss: 74.5841\n",
      "Epoch 194/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 75.8268 - val_loss: 74.5054\n",
      "Epoch 195/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 75.2293 - val_loss: 74.4298\n",
      "Epoch 196/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 72.6183 - val_loss: 74.3593\n",
      "Epoch 197/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 72.1207 - val_loss: 74.2901\n",
      "Epoch 198/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 74.4189 - val_loss: 74.2227\n",
      "Epoch 199/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 72.9100 - val_loss: 74.1541\n",
      "Epoch 200/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 77.9080 - val_loss: 74.0863\n",
      "Epoch 201/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 72.7910 - val_loss: 74.0184\n",
      "Epoch 202/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 73.0419 - val_loss: 73.9588\n",
      "Epoch 203/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 72.9466 - val_loss: 73.9027\n",
      "Epoch 204/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 73.3639 - val_loss: 73.8447\n",
      "Epoch 205/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 72.2650 - val_loss: 73.7896\n",
      "Epoch 206/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 73.6289 - val_loss: 73.7393\n",
      "Epoch 207/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 73.4189 - val_loss: 73.6834\n",
      "Epoch 208/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 73.2794 - val_loss: 73.6238\n",
      "Epoch 209/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 74.3062 - val_loss: 73.5641\n",
      "Epoch 210/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 73.0660 - val_loss: 73.4964\n",
      "Epoch 211/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 73.7069 - val_loss: 73.4319\n",
      "Epoch 212/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 72.4883 - val_loss: 73.3692\n",
      "Epoch 213/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 74.2796 - val_loss: 73.3038\n",
      "Epoch 214/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 71.4569 - val_loss: 73.2349\n",
      "Epoch 215/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 72.0367 - val_loss: 73.1665\n",
      "Epoch 216/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 72.5223 - val_loss: 73.0980\n",
      "Epoch 217/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 74.3500 - val_loss: 73.0342\n",
      "Epoch 218/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 72.9185 - val_loss: 72.9727\n",
      "Epoch 219/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 73.5297 - val_loss: 72.9107\n",
      "Epoch 220/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 71.8479 - val_loss: 72.8527\n",
      "Epoch 221/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 71.8082 - val_loss: 72.7912\n",
      "Epoch 222/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 72.7150 - val_loss: 72.7297\n",
      "Epoch 223/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 71.9553 - val_loss: 72.6697\n",
      "Epoch 224/1500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 72.3210 - val_loss: 72.6099\n",
      "Epoch 225/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 71.8712 - val_loss: 72.5435\n",
      "Epoch 226/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 71.2786 - val_loss: 72.4729\n",
      "Epoch 227/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 73.1919 - val_loss: 72.4048\n",
      "Epoch 228/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 73.0659 - val_loss: 72.3335\n",
      "Epoch 229/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 72.7189 - val_loss: 72.2643\n",
      "Epoch 230/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 73.0952 - val_loss: 72.2010\n",
      "Epoch 231/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 71.1252 - val_loss: 72.1404\n",
      "Epoch 232/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 73.5163 - val_loss: 72.0819\n",
      "Epoch 233/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 71.9039 - val_loss: 72.0217\n",
      "Epoch 234/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 72.5068 - val_loss: 71.9633\n",
      "Epoch 235/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 70.3978 - val_loss: 71.9025\n",
      "Epoch 236/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 71.3241 - val_loss: 71.8365\n",
      "Epoch 237/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 72.5155 - val_loss: 71.7658\n",
      "Epoch 238/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 71.2009 - val_loss: 71.6939\n",
      "Epoch 239/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 70.7073 - val_loss: 71.6216\n",
      "Epoch 240/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 71.2816 - val_loss: 71.5566\n",
      "Epoch 241/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 72.6617 - val_loss: 71.4907\n",
      "Epoch 242/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 73.1583 - val_loss: 71.4275\n",
      "Epoch 243/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 68.9760 - val_loss: 71.3657\n",
      "Epoch 244/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 71.8083 - val_loss: 71.3052\n",
      "Epoch 245/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 70.6677 - val_loss: 71.2466\n",
      "Epoch 246/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 73.0845 - val_loss: 71.1834\n",
      "Epoch 247/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 72.0634 - val_loss: 71.1260\n",
      "Epoch 248/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 69.9021 - val_loss: 71.0660\n",
      "Epoch 249/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 71.2335 - val_loss: 71.0073\n",
      "Epoch 250/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 71.5570 - val_loss: 70.9482\n",
      "Epoch 251/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 70.7145 - val_loss: 70.8858\n",
      "Epoch 252/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 70.7182 - val_loss: 70.8218\n",
      "Epoch 253/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 69.9296 - val_loss: 70.7542\n",
      "Epoch 254/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 70.2126 - val_loss: 70.6809\n",
      "Epoch 255/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 69.7022 - val_loss: 70.6012\n",
      "Epoch 256/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 68.9295 - val_loss: 70.5267\n",
      "Epoch 257/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 69.8066 - val_loss: 70.4560\n",
      "Epoch 258/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 70.3710 - val_loss: 70.3917\n",
      "Epoch 259/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 69.7565 - val_loss: 70.3346\n",
      "Epoch 260/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 69.3139 - val_loss: 70.2832\n",
      "Epoch 261/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 70.2962 - val_loss: 70.2315\n",
      "Epoch 262/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 70.1710 - val_loss: 70.1799\n",
      "Epoch 263/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 69.9282 - val_loss: 70.1270\n",
      "Epoch 264/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 67.3161 - val_loss: 70.0694\n",
      "Epoch 265/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 69.7899 - val_loss: 70.0040\n",
      "Epoch 266/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 69.6656 - val_loss: 69.9270\n",
      "Epoch 267/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 71.8059 - val_loss: 69.8454\n",
      "Epoch 268/1500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 69.8401 - val_loss: 69.7643\n",
      "Epoch 269/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 67.8674 - val_loss: 69.6890\n",
      "Epoch 270/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 69.7590 - val_loss: 69.6189\n",
      "Epoch 271/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 69.4989 - val_loss: 69.5602\n",
      "Epoch 272/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 70.3399 - val_loss: 69.5016\n",
      "Epoch 273/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 68.4169 - val_loss: 69.4487\n",
      "Epoch 274/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 70.9265 - val_loss: 69.3923\n",
      "Epoch 275/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 70.5699 - val_loss: 69.3384\n",
      "Epoch 276/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 71.0028 - val_loss: 69.2916\n",
      "Epoch 277/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 68.6244 - val_loss: 69.2419\n",
      "Epoch 278/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 71.2472 - val_loss: 69.1821\n",
      "Epoch 279/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 69.5034 - val_loss: 69.1084\n",
      "Epoch 280/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 68.1249 - val_loss: 69.0307\n",
      "Epoch 281/1500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 67.7122 - val_loss: 68.9577\n",
      "Epoch 282/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 70.0872 - val_loss: 68.8867\n",
      "Epoch 283/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 68.4451 - val_loss: 68.8191\n",
      "Epoch 284/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 69.2525 - val_loss: 68.7511\n",
      "Epoch 285/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 69.3093 - val_loss: 68.6829\n",
      "Epoch 286/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 67.0257 - val_loss: 68.6204\n",
      "Epoch 287/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 68.3305 - val_loss: 68.5618\n",
      "Epoch 288/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 70.0683 - val_loss: 68.4995\n",
      "Epoch 289/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 66.7559 - val_loss: 68.4320\n",
      "Epoch 290/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 69.2789 - val_loss: 68.3577\n",
      "Epoch 291/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 67.9594 - val_loss: 68.2839\n",
      "Epoch 292/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 69.4737 - val_loss: 68.2094\n",
      "Epoch 293/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 67.8010 - val_loss: 68.1365\n",
      "Epoch 294/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 67.1096 - val_loss: 68.0682\n",
      "Epoch 295/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 66.4614 - val_loss: 68.0029\n",
      "Epoch 296/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 68.1767 - val_loss: 67.9385\n",
      "Epoch 297/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 68.5070 - val_loss: 67.8744\n",
      "Epoch 298/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 66.4394 - val_loss: 67.8072\n",
      "Epoch 299/1500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 67.8712 - val_loss: 67.7377\n",
      "Epoch 300/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 67.7648 - val_loss: 67.6679\n",
      "Epoch 301/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 65.2558 - val_loss: 67.6015\n",
      "Epoch 302/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 68.4013 - val_loss: 67.5370\n",
      "Epoch 303/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 68.2735 - val_loss: 67.4770\n",
      "Epoch 304/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 68.1465 - val_loss: 67.4150\n",
      "Epoch 305/1500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 66.9232 - val_loss: 67.3568\n",
      "Epoch 306/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 67.3580 - val_loss: 67.3004\n",
      "Epoch 307/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 67.0450 - val_loss: 67.2416\n",
      "Epoch 308/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 67.1409 - val_loss: 67.1857\n",
      "Epoch 309/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 68.7659 - val_loss: 67.1266\n",
      "Epoch 310/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 66.1975 - val_loss: 67.0705\n",
      "Epoch 311/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 69.7147 - val_loss: 67.0099\n",
      "Epoch 312/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 65.4808 - val_loss: 66.9469\n",
      "Epoch 313/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 66.7499 - val_loss: 66.8771\n",
      "Epoch 314/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 66.6618 - val_loss: 66.8075\n",
      "Epoch 315/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 66.2460 - val_loss: 66.7378\n",
      "Epoch 316/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 67.5247 - val_loss: 66.6746\n",
      "Epoch 317/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 66.0583 - val_loss: 66.6115\n",
      "Epoch 318/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 67.1152 - val_loss: 66.5566\n",
      "Epoch 319/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 67.5000 - val_loss: 66.4993\n",
      "Epoch 320/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 66.8454 - val_loss: 66.4366\n",
      "Epoch 321/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 64.9939 - val_loss: 66.3704\n",
      "Epoch 322/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 67.6586 - val_loss: 66.3027\n",
      "Epoch 323/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 65.5474 - val_loss: 66.2324\n",
      "Epoch 324/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 63.4562 - val_loss: 66.1619\n",
      "Epoch 325/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 64.6061 - val_loss: 66.0898\n",
      "Epoch 326/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 69.0428 - val_loss: 66.0191\n",
      "Epoch 327/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 65.5197 - val_loss: 65.9473\n",
      "Epoch 328/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 66.7142 - val_loss: 65.8779\n",
      "Epoch 329/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 64.8569 - val_loss: 65.8087\n",
      "Epoch 330/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 65.4785 - val_loss: 65.7375\n",
      "Epoch 331/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 68.4118 - val_loss: 65.6739\n",
      "Epoch 332/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 65.8186 - val_loss: 65.6139\n",
      "Epoch 333/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 64.4688 - val_loss: 65.5488\n",
      "Epoch 334/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 65.0228 - val_loss: 65.4869\n",
      "Epoch 335/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 66.6635 - val_loss: 65.4207\n",
      "Epoch 336/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 62.4009 - val_loss: 65.3536\n",
      "Epoch 337/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 65.4525 - val_loss: 65.2861\n",
      "Epoch 338/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 67.0222 - val_loss: 65.2171\n",
      "Epoch 339/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 64.0837 - val_loss: 65.1539\n",
      "Epoch 340/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 65.6242 - val_loss: 65.0907\n",
      "Epoch 341/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 63.7396 - val_loss: 65.0276\n",
      "Epoch 342/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 65.2822 - val_loss: 64.9654\n",
      "Epoch 343/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 65.1868 - val_loss: 64.9005\n",
      "Epoch 344/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 64.4612 - val_loss: 64.8381\n",
      "Epoch 345/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 64.7490 - val_loss: 64.7802\n",
      "Epoch 346/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 65.4069 - val_loss: 64.7236\n",
      "Epoch 347/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 62.7503 - val_loss: 64.6680\n",
      "Epoch 348/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 65.9558 - val_loss: 64.6078\n",
      "Epoch 349/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 64.2467 - val_loss: 64.5489\n",
      "Epoch 350/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 63.8990 - val_loss: 64.4936\n",
      "Epoch 351/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 64.3866 - val_loss: 64.4305\n",
      "Epoch 352/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 64.6253 - val_loss: 64.3614\n",
      "Epoch 353/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 64.1898 - val_loss: 64.2916\n",
      "Epoch 354/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 64.0603 - val_loss: 64.2241\n",
      "Epoch 355/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 64.6073 - val_loss: 64.1530\n",
      "Epoch 356/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 64.0911 - val_loss: 64.0782\n",
      "Epoch 357/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 60.9640 - val_loss: 63.9985\n",
      "Epoch 358/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 61.9665 - val_loss: 63.9384\n",
      "Epoch 359/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 61.1663 - val_loss: 63.8832\n",
      "Epoch 360/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 62.3519 - val_loss: 63.8313\n",
      "Epoch 361/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 63.8312 - val_loss: 63.7990\n",
      "Epoch 362/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 63.4836 - val_loss: 63.7954\n",
      "Epoch 363/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 63.4256 - val_loss: 63.7472\n",
      "Epoch 364/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 62.7117 - val_loss: 63.6621\n",
      "Epoch 365/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 64.2489 - val_loss: 63.5669\n",
      "Epoch 366/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 64.3849 - val_loss: 63.4646\n",
      "Epoch 367/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 64.3140 - val_loss: 63.3767\n",
      "Epoch 368/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 63.0770 - val_loss: 63.2949\n",
      "Epoch 369/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 62.0756 - val_loss: 63.2174\n",
      "Epoch 370/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 65.1913 - val_loss: 63.1428\n",
      "Epoch 371/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 61.3372 - val_loss: 63.0642\n",
      "Epoch 372/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 62.7065 - val_loss: 62.9895\n",
      "Epoch 373/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 60.9240 - val_loss: 62.9172\n",
      "Epoch 374/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 64.3830 - val_loss: 62.8480\n",
      "Epoch 375/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 61.4380 - val_loss: 62.7839\n",
      "Epoch 376/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 60.8139 - val_loss: 62.7214\n",
      "Epoch 377/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 62.8933 - val_loss: 62.6572\n",
      "Epoch 378/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 62.4301 - val_loss: 62.5930\n",
      "Epoch 379/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 63.0510 - val_loss: 62.5280\n",
      "Epoch 380/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 60.9732 - val_loss: 62.4598\n",
      "Epoch 381/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 60.9807 - val_loss: 62.3919\n",
      "Epoch 382/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 60.3022 - val_loss: 62.3286\n",
      "Epoch 383/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 60.5585 - val_loss: 62.2631\n",
      "Epoch 384/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 63.2008 - val_loss: 62.1955\n",
      "Epoch 385/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 62.4570 - val_loss: 62.1306\n",
      "Epoch 386/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 61.6276 - val_loss: 62.0650\n",
      "Epoch 387/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 60.2239 - val_loss: 61.9980\n",
      "Epoch 388/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 63.0970 - val_loss: 61.9294\n",
      "Epoch 389/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 61.7795 - val_loss: 61.8624\n",
      "Epoch 390/1500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 58.6667 - val_loss: 61.7976\n",
      "Epoch 391/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 59.6332 - val_loss: 61.7363\n",
      "Epoch 392/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 65.2141 - val_loss: 61.6771\n",
      "Epoch 393/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 59.9982 - val_loss: 61.6145\n",
      "Epoch 394/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 62.6059 - val_loss: 61.5496\n",
      "Epoch 395/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 61.1152 - val_loss: 61.4823\n",
      "Epoch 396/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 60.2945 - val_loss: 61.4182\n",
      "Epoch 397/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 59.5802 - val_loss: 61.3532\n",
      "Epoch 398/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 61.4565 - val_loss: 61.2853\n",
      "Epoch 399/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 65.0301 - val_loss: 61.2166\n",
      "Epoch 400/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 60.6695 - val_loss: 61.1507\n",
      "Epoch 401/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 60.5712 - val_loss: 61.0849\n",
      "Epoch 402/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 62.1924 - val_loss: 61.0198\n",
      "Epoch 403/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 59.6460 - val_loss: 60.9507\n",
      "Epoch 404/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 61.3291 - val_loss: 60.8835\n",
      "Epoch 405/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 60.6979 - val_loss: 60.8193\n",
      "Epoch 406/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 60.5830 - val_loss: 60.7589\n",
      "Epoch 407/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 57.9799 - val_loss: 60.7036\n",
      "Epoch 408/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 62.1115 - val_loss: 60.6507\n",
      "Epoch 409/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 60.6420 - val_loss: 60.5909\n",
      "Epoch 410/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 59.2896 - val_loss: 60.5324\n",
      "Epoch 411/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 61.7299 - val_loss: 60.4716\n",
      "Epoch 412/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 61.8906 - val_loss: 60.4087\n",
      "Epoch 413/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 59.7066 - val_loss: 60.3442\n",
      "Epoch 414/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 62.0583 - val_loss: 60.2740\n",
      "Epoch 415/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 58.8772 - val_loss: 60.2021\n",
      "Epoch 416/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 61.7730 - val_loss: 60.1299\n",
      "Epoch 417/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 61.0502 - val_loss: 60.0597\n",
      "Epoch 418/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 61.0474 - val_loss: 59.9899\n",
      "Epoch 419/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 60.9161 - val_loss: 59.9197\n",
      "Epoch 420/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 60.2847 - val_loss: 59.8561\n",
      "Epoch 421/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 59.6026 - val_loss: 59.7900\n",
      "Epoch 422/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 61.8785 - val_loss: 59.7182\n",
      "Epoch 423/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 58.4755 - val_loss: 59.6539\n",
      "Epoch 424/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 58.4195 - val_loss: 59.5900\n",
      "Epoch 425/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 58.3614 - val_loss: 59.5265\n",
      "Epoch 426/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 60.4558 - val_loss: 59.4653\n",
      "Epoch 427/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 57.2320 - val_loss: 59.4084\n",
      "Epoch 428/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 59.3305 - val_loss: 59.3524\n",
      "Epoch 429/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 59.6350 - val_loss: 59.3034\n",
      "Epoch 430/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 58.1660 - val_loss: 59.2525\n",
      "Epoch 431/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 58.5452 - val_loss: 59.1938\n",
      "Epoch 432/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 59.0867 - val_loss: 59.1285\n",
      "Epoch 433/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 57.2146 - val_loss: 59.0587\n",
      "Epoch 434/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 60.1720 - val_loss: 58.9878\n",
      "Epoch 435/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 58.3829 - val_loss: 58.9162\n",
      "Epoch 436/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 57.4580 - val_loss: 58.8434\n",
      "Epoch 437/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 57.4342 - val_loss: 58.7705\n",
      "Epoch 438/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 58.3654 - val_loss: 58.7030\n",
      "Epoch 439/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 59.0688 - val_loss: 58.6340\n",
      "Epoch 440/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 60.8622 - val_loss: 58.5666\n",
      "Epoch 441/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 57.7713 - val_loss: 58.5008\n",
      "Epoch 442/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 57.8523 - val_loss: 58.4405\n",
      "Epoch 443/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 59.8915 - val_loss: 58.3799\n",
      "Epoch 444/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 58.6622 - val_loss: 58.3193\n",
      "Epoch 445/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 57.8343 - val_loss: 58.2581\n",
      "Epoch 446/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 56.8780 - val_loss: 58.1864\n",
      "Epoch 447/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 57.3802 - val_loss: 58.1150\n",
      "Epoch 448/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 60.8379 - val_loss: 58.0515\n",
      "Epoch 449/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 57.5426 - val_loss: 57.9923\n",
      "Epoch 450/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 58.3860 - val_loss: 57.9340\n",
      "Epoch 451/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 56.1927 - val_loss: 57.8781\n",
      "Epoch 452/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 60.9683 - val_loss: 57.8181\n",
      "Epoch 453/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 55.6887 - val_loss: 57.7520\n",
      "Epoch 454/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 60.1460 - val_loss: 57.6795\n",
      "Epoch 455/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 55.9410 - val_loss: 57.6069\n",
      "Epoch 456/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 56.9363 - val_loss: 57.5406\n",
      "Epoch 457/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 56.6089 - val_loss: 57.4765\n",
      "Epoch 458/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 55.5273 - val_loss: 57.4184\n",
      "Epoch 459/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 55.7687 - val_loss: 57.3630\n",
      "Epoch 460/1500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 58.6025 - val_loss: 57.3079\n",
      "Epoch 461/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 58.7281 - val_loss: 57.2536\n",
      "Epoch 462/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 57.0153 - val_loss: 57.1964\n",
      "Epoch 463/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 58.3689 - val_loss: 57.1385\n",
      "Epoch 464/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 57.4892 - val_loss: 57.0739\n",
      "Epoch 465/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 55.5617 - val_loss: 57.0028\n",
      "Epoch 466/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 57.2290 - val_loss: 56.9257\n",
      "Epoch 467/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 57.1237 - val_loss: 56.8479\n",
      "Epoch 468/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 57.3999 - val_loss: 56.7669\n",
      "Epoch 469/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 58.6054 - val_loss: 56.6890\n",
      "Epoch 470/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 57.9306 - val_loss: 56.6157\n",
      "Epoch 471/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 54.7529 - val_loss: 56.5477\n",
      "Epoch 472/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 56.6829 - val_loss: 56.4808\n",
      "Epoch 473/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 53.1659 - val_loss: 56.4118\n",
      "Epoch 474/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 55.1537 - val_loss: 56.3453\n",
      "Epoch 475/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 55.5746 - val_loss: 56.2803\n",
      "Epoch 476/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 56.3474 - val_loss: 56.2185\n",
      "Epoch 477/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 52.0664 - val_loss: 56.1570\n",
      "Epoch 478/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 53.9527 - val_loss: 56.0963\n",
      "Epoch 479/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 57.9361 - val_loss: 56.0345\n",
      "Epoch 480/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 56.0550 - val_loss: 55.9701\n",
      "Epoch 481/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 53.8849 - val_loss: 55.9065\n",
      "Epoch 482/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 55.9941 - val_loss: 55.8413\n",
      "Epoch 483/1500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 56.7740 - val_loss: 55.7800\n",
      "Epoch 484/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 54.3562 - val_loss: 55.7138\n",
      "Epoch 485/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 56.1479 - val_loss: 55.6459\n",
      "Epoch 486/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 54.7184 - val_loss: 55.5728\n",
      "Epoch 487/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 56.9612 - val_loss: 55.4995\n",
      "Epoch 488/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 54.7835 - val_loss: 55.4316\n",
      "Epoch 489/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 56.2138 - val_loss: 55.3603\n",
      "Epoch 490/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 58.0866 - val_loss: 55.2916\n",
      "Epoch 491/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 52.3887 - val_loss: 55.2246\n",
      "Epoch 492/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 55.1709 - val_loss: 55.1576\n",
      "Epoch 493/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 52.9161 - val_loss: 55.0924\n",
      "Epoch 494/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 53.1889 - val_loss: 55.0275\n",
      "Epoch 495/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 58.0001 - val_loss: 54.9611\n",
      "Epoch 496/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 52.6277 - val_loss: 54.8978\n",
      "Epoch 497/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 52.9591 - val_loss: 54.8393\n",
      "Epoch 498/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 56.9830 - val_loss: 54.7814\n",
      "Epoch 499/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 52.4049 - val_loss: 54.7249\n",
      "Epoch 500/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 56.5011 - val_loss: 54.6688\n",
      "Epoch 501/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 55.9988 - val_loss: 54.6113\n",
      "Epoch 502/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 55.2890 - val_loss: 54.5511\n",
      "Epoch 503/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 55.2285 - val_loss: 54.4880\n",
      "Epoch 504/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 55.3051 - val_loss: 54.4215\n",
      "Epoch 505/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 56.3435 - val_loss: 54.3559\n",
      "Epoch 506/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 56.1919 - val_loss: 54.2901\n",
      "Epoch 507/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 56.3175 - val_loss: 54.2263\n",
      "Epoch 508/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 55.2916 - val_loss: 54.1668\n",
      "Epoch 509/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 53.1616 - val_loss: 54.1087\n",
      "Epoch 510/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 54.3039 - val_loss: 54.0409\n",
      "Epoch 511/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 53.9765 - val_loss: 53.9705\n",
      "Epoch 512/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 53.8341 - val_loss: 53.8953\n",
      "Epoch 513/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 53.4806 - val_loss: 53.8193\n",
      "Epoch 514/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 52.3967 - val_loss: 53.7475\n",
      "Epoch 515/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 52.8290 - val_loss: 53.6818\n",
      "Epoch 516/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 53.0906 - val_loss: 53.6139\n",
      "Epoch 517/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 53.7177 - val_loss: 53.5481\n",
      "Epoch 518/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 54.0306 - val_loss: 53.4842\n",
      "Epoch 519/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 52.5556 - val_loss: 53.4227\n",
      "Epoch 520/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 53.1981 - val_loss: 53.3615\n",
      "Epoch 521/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 53.8350 - val_loss: 53.2992\n",
      "Epoch 522/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 53.9430 - val_loss: 53.2383\n",
      "Epoch 523/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 52.3544 - val_loss: 53.1787\n",
      "Epoch 524/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 52.3015 - val_loss: 53.1190\n",
      "Epoch 525/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 53.8553 - val_loss: 53.0605\n",
      "Epoch 526/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 54.0315 - val_loss: 53.0017\n",
      "Epoch 527/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 50.3975 - val_loss: 52.9433\n",
      "Epoch 528/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 56.4390 - val_loss: 52.8824\n",
      "Epoch 529/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 56.5331 - val_loss: 52.8206\n",
      "Epoch 530/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 55.4266 - val_loss: 52.7593\n",
      "Epoch 531/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 49.7381 - val_loss: 52.6981\n",
      "Epoch 532/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 50.7146 - val_loss: 52.6378\n",
      "Epoch 533/1500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 50.1374 - val_loss: 52.5758\n",
      "Epoch 534/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 52.4998 - val_loss: 52.5138\n",
      "Epoch 535/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 51.6820 - val_loss: 52.4572\n",
      "Epoch 536/1500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 52.7504 - val_loss: 52.4025\n",
      "Epoch 537/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 52.4597 - val_loss: 52.3464\n",
      "Epoch 538/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 49.7978 - val_loss: 52.2883\n",
      "Epoch 539/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 50.2683 - val_loss: 52.2236\n",
      "Epoch 540/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 52.0449 - val_loss: 52.1541\n",
      "Epoch 541/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 52.3227 - val_loss: 52.0747\n",
      "Epoch 542/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 50.3561 - val_loss: 51.9946\n",
      "Epoch 543/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 53.6894 - val_loss: 51.9177\n",
      "Epoch 544/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 53.7926 - val_loss: 51.8432\n",
      "Epoch 545/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 49.9709 - val_loss: 51.7718\n",
      "Epoch 546/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 51.9016 - val_loss: 51.7017\n",
      "Epoch 547/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 54.6711 - val_loss: 51.6352\n",
      "Epoch 548/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 51.8291 - val_loss: 51.5758\n",
      "Epoch 549/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 52.7130 - val_loss: 51.5177\n",
      "Epoch 550/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 53.2889 - val_loss: 51.4629\n",
      "Epoch 551/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 51.4164 - val_loss: 51.4056\n",
      "Epoch 552/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 49.5116 - val_loss: 51.3458\n",
      "Epoch 553/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 52.3917 - val_loss: 51.2856\n",
      "Epoch 554/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 48.9230 - val_loss: 51.2236\n",
      "Epoch 555/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 53.0318 - val_loss: 51.1638\n",
      "Epoch 556/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 49.6277 - val_loss: 51.1017\n",
      "Epoch 557/1500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 52.4680 - val_loss: 51.0364\n",
      "Epoch 558/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 51.7016 - val_loss: 50.9649\n",
      "Epoch 559/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 51.9301 - val_loss: 50.8914\n",
      "Epoch 560/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 51.0719 - val_loss: 50.8218\n",
      "Epoch 561/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 51.1565 - val_loss: 50.7526\n",
      "Epoch 562/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 51.3122 - val_loss: 50.6851\n",
      "Epoch 563/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 49.0416 - val_loss: 50.6183\n",
      "Epoch 564/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 48.4973 - val_loss: 50.5558\n",
      "Epoch 565/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 49.4668 - val_loss: 50.4983\n",
      "Epoch 566/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 49.0721 - val_loss: 50.4377\n",
      "Epoch 567/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 47.7664 - val_loss: 50.3794\n",
      "Epoch 568/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 49.5173 - val_loss: 50.3187\n",
      "Epoch 569/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 49.4928 - val_loss: 50.2618\n",
      "Epoch 570/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 50.2167 - val_loss: 50.2080\n",
      "Epoch 571/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 49.4873 - val_loss: 50.1508\n",
      "Epoch 572/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 48.5384 - val_loss: 50.0880\n",
      "Epoch 573/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 47.3644 - val_loss: 50.0279\n",
      "Epoch 574/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 50.7196 - val_loss: 49.9644\n",
      "Epoch 575/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 47.6723 - val_loss: 49.9021\n",
      "Epoch 576/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 49.2455 - val_loss: 49.8416\n",
      "Epoch 577/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 52.4459 - val_loss: 49.7803\n",
      "Epoch 578/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 47.2494 - val_loss: 49.7154\n",
      "Epoch 579/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 48.9406 - val_loss: 49.6475\n",
      "Epoch 580/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 50.5492 - val_loss: 49.5747\n",
      "Epoch 581/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 49.2197 - val_loss: 49.5027\n",
      "Epoch 582/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 48.0295 - val_loss: 49.4323\n",
      "Epoch 583/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 50.7044 - val_loss: 49.3619\n",
      "Epoch 584/1500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 46.8381 - val_loss: 49.2956\n",
      "Epoch 585/1500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 50.4926 - val_loss: 49.2289\n",
      "Epoch 586/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 47.9014 - val_loss: 49.1610\n",
      "Epoch 587/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 50.9758 - val_loss: 49.0956\n",
      "Epoch 588/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 47.4269 - val_loss: 49.0292\n",
      "Epoch 589/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 48.9907 - val_loss: 48.9613\n",
      "Epoch 590/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 48.5866 - val_loss: 48.8905\n",
      "Epoch 591/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 46.4222 - val_loss: 48.8197\n",
      "Epoch 592/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 50.9132 - val_loss: 48.7470\n",
      "Epoch 593/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 48.5284 - val_loss: 48.6752\n",
      "Epoch 594/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 47.8928 - val_loss: 48.6068\n",
      "Epoch 595/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 45.9248 - val_loss: 48.5418\n",
      "Epoch 596/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 50.9745 - val_loss: 48.4790\n",
      "Epoch 597/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 45.0451 - val_loss: 48.4216\n",
      "Epoch 598/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 46.8678 - val_loss: 48.3620\n",
      "Epoch 599/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 46.7142 - val_loss: 48.3037\n",
      "Epoch 600/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 46.9379 - val_loss: 48.2429\n",
      "Epoch 601/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 46.2136 - val_loss: 48.1837\n",
      "Epoch 602/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 50.4077 - val_loss: 48.1277\n",
      "Epoch 603/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 46.5567 - val_loss: 48.0722\n",
      "Epoch 604/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 48.9850 - val_loss: 48.0183\n",
      "Epoch 605/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 49.9111 - val_loss: 47.9623\n",
      "Epoch 606/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 46.3723 - val_loss: 47.9000\n",
      "Epoch 607/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 43.6896 - val_loss: 47.8401\n",
      "Epoch 608/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 43.9115 - val_loss: 47.7783\n",
      "Epoch 609/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 50.0009 - val_loss: 47.7134\n",
      "Epoch 610/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 44.6771 - val_loss: 47.6438\n",
      "Epoch 611/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 45.8477 - val_loss: 47.5667\n",
      "Epoch 612/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 47.1995 - val_loss: 47.4953\n",
      "Epoch 613/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 46.7608 - val_loss: 47.4246\n",
      "Epoch 614/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 49.0263 - val_loss: 47.3574\n",
      "Epoch 615/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 49.6138 - val_loss: 47.2875\n",
      "Epoch 616/1500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 46.9032 - val_loss: 47.2102\n",
      "Epoch 617/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 49.4466 - val_loss: 47.1341\n",
      "Epoch 618/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 43.0729 - val_loss: 47.0606\n",
      "Epoch 619/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 47.2440 - val_loss: 46.9920\n",
      "Epoch 620/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 46.6447 - val_loss: 46.9212\n",
      "Epoch 621/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 46.0554 - val_loss: 46.8523\n",
      "Epoch 622/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 45.9226 - val_loss: 46.7844\n",
      "Epoch 623/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 46.9742 - val_loss: 46.7206\n",
      "Epoch 624/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 45.5581 - val_loss: 46.6640\n",
      "Epoch 625/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 45.0028 - val_loss: 46.6073\n",
      "Epoch 626/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 47.0966 - val_loss: 46.5479\n",
      "Epoch 627/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 46.6918 - val_loss: 46.4857\n",
      "Epoch 628/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 43.9166 - val_loss: 46.4218\n",
      "Epoch 629/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 43.8720 - val_loss: 46.3606\n",
      "Epoch 630/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 44.5755 - val_loss: 46.3026\n",
      "Epoch 631/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 46.0229 - val_loss: 46.2443\n",
      "Epoch 632/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 44.4379 - val_loss: 46.1822\n",
      "Epoch 633/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 47.2988 - val_loss: 46.1177\n",
      "Epoch 634/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 46.6321 - val_loss: 46.0571\n",
      "Epoch 635/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 46.9220 - val_loss: 45.9930\n",
      "Epoch 636/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 45.0960 - val_loss: 45.9276\n",
      "Epoch 637/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 44.3149 - val_loss: 45.8625\n",
      "Epoch 638/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 45.4875 - val_loss: 45.7946\n",
      "Epoch 639/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 43.7047 - val_loss: 45.7318\n",
      "Epoch 640/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 46.7214 - val_loss: 45.6710\n",
      "Epoch 641/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 46.1965 - val_loss: 45.6111\n",
      "Epoch 642/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 47.5828 - val_loss: 45.5511\n",
      "Epoch 643/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 45.6047 - val_loss: 45.4854\n",
      "Epoch 644/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 45.7940 - val_loss: 45.4215\n",
      "Epoch 645/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 44.0388 - val_loss: 45.3523\n",
      "Epoch 646/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 42.4994 - val_loss: 45.2813\n",
      "Epoch 647/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 43.2439 - val_loss: 45.2136\n",
      "Epoch 648/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 45.5872 - val_loss: 45.1482\n",
      "Epoch 649/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 43.1248 - val_loss: 45.0817\n",
      "Epoch 650/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 46.4717 - val_loss: 45.0186\n",
      "Epoch 651/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 47.0931 - val_loss: 44.9543\n",
      "Epoch 652/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 44.0682 - val_loss: 44.8917\n",
      "Epoch 653/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 43.9409 - val_loss: 44.8325\n",
      "Epoch 654/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 42.8940 - val_loss: 44.7778\n",
      "Epoch 655/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 47.5503 - val_loss: 44.7216\n",
      "Epoch 656/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 44.2292 - val_loss: 44.6619\n",
      "Epoch 657/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 44.2511 - val_loss: 44.6032\n",
      "Epoch 658/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 44.9109 - val_loss: 44.5435\n",
      "Epoch 659/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 47.5835 - val_loss: 44.4840\n",
      "Epoch 660/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 45.2805 - val_loss: 44.4203\n",
      "Epoch 661/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 42.4735 - val_loss: 44.3557\n",
      "Epoch 662/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 43.2729 - val_loss: 44.2892\n",
      "Epoch 663/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 44.9495 - val_loss: 44.2227\n",
      "Epoch 664/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 43.7743 - val_loss: 44.1584\n",
      "Epoch 665/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 46.5188 - val_loss: 44.0921\n",
      "Epoch 666/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 47.5882 - val_loss: 44.0278\n",
      "Epoch 667/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 46.4707 - val_loss: 43.9608\n",
      "Epoch 668/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 44.2060 - val_loss: 43.8953\n",
      "Epoch 669/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 47.0843 - val_loss: 43.8309\n",
      "Epoch 670/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 45.0658 - val_loss: 43.7654\n",
      "Epoch 671/1500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 42.0952 - val_loss: 43.7044\n",
      "Epoch 672/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 46.9667 - val_loss: 43.6452\n",
      "Epoch 673/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 46.5740 - val_loss: 43.5867\n",
      "Epoch 674/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 46.8902 - val_loss: 43.5291\n",
      "Epoch 675/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 43.0689 - val_loss: 43.4726\n",
      "Epoch 676/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 43.5118 - val_loss: 43.4153\n",
      "Epoch 677/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 42.7873 - val_loss: 43.3572\n",
      "Epoch 678/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 42.0396 - val_loss: 43.3000\n",
      "Epoch 679/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 43.2367 - val_loss: 43.2432\n",
      "Epoch 680/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 46.4109 - val_loss: 43.1894\n",
      "Epoch 681/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 43.8632 - val_loss: 43.1368\n",
      "Epoch 682/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 43.5741 - val_loss: 43.0827\n",
      "Epoch 683/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 44.9135 - val_loss: 43.0238\n",
      "Epoch 684/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 42.6178 - val_loss: 42.9626\n",
      "Epoch 685/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 42.1953 - val_loss: 42.8960\n",
      "Epoch 686/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 40.6017 - val_loss: 42.8302\n",
      "Epoch 687/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 41.1289 - val_loss: 42.7682\n",
      "Epoch 688/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 41.5577 - val_loss: 42.7057\n",
      "Epoch 689/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 41.0208 - val_loss: 42.6436\n",
      "Epoch 690/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 41.0753 - val_loss: 42.5736\n",
      "Epoch 691/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 41.3111 - val_loss: 42.5054\n",
      "Epoch 692/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 38.0750 - val_loss: 42.4314\n",
      "Epoch 693/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 38.2705 - val_loss: 42.3572\n",
      "Epoch 694/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 41.4175 - val_loss: 42.2817\n",
      "Epoch 695/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 40.6995 - val_loss: 42.2119\n",
      "Epoch 696/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 47.7972 - val_loss: 42.1431\n",
      "Epoch 697/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 45.6231 - val_loss: 42.0800\n",
      "Epoch 698/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 43.6501 - val_loss: 42.0206\n",
      "Epoch 699/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 46.1263 - val_loss: 41.9618\n",
      "Epoch 700/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 42.5221 - val_loss: 41.8999\n",
      "Epoch 701/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 46.2435 - val_loss: 41.8375\n",
      "Epoch 702/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 39.7202 - val_loss: 41.7777\n",
      "Epoch 703/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 42.3126 - val_loss: 41.7208\n",
      "Epoch 704/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 42.4915 - val_loss: 41.6596\n",
      "Epoch 705/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 42.1789 - val_loss: 41.5965\n",
      "Epoch 706/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 39.6266 - val_loss: 41.5289\n",
      "Epoch 707/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 38.5687 - val_loss: 41.4602\n",
      "Epoch 708/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 38.4103 - val_loss: 41.3891\n",
      "Epoch 709/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 36.4948 - val_loss: 41.3150\n",
      "Epoch 710/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 41.4860 - val_loss: 41.2440\n",
      "Epoch 711/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 44.5327 - val_loss: 41.1738\n",
      "Epoch 712/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 40.6630 - val_loss: 41.1065\n",
      "Epoch 713/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 41.9523 - val_loss: 41.0430\n",
      "Epoch 714/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 36.9686 - val_loss: 40.9837\n",
      "Epoch 715/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 39.5894 - val_loss: 40.9239\n",
      "Epoch 716/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 37.8129 - val_loss: 40.8666\n",
      "Epoch 717/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 41.3114 - val_loss: 40.8061\n",
      "Epoch 718/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 38.6665 - val_loss: 40.7398\n",
      "Epoch 719/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 39.1794 - val_loss: 40.6755\n",
      "Epoch 720/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 42.3813 - val_loss: 40.6086\n",
      "Epoch 721/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 43.5611 - val_loss: 40.5411\n",
      "Epoch 722/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 41.6497 - val_loss: 40.4713\n",
      "Epoch 723/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 40.3545 - val_loss: 40.4036\n",
      "Epoch 724/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 39.3412 - val_loss: 40.3339\n",
      "Epoch 725/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 36.6972 - val_loss: 40.2660\n",
      "Epoch 726/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 40.8445 - val_loss: 40.1985\n",
      "Epoch 727/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 39.8764 - val_loss: 40.1344\n",
      "Epoch 728/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 40.6909 - val_loss: 40.0709\n",
      "Epoch 729/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 43.0117 - val_loss: 40.0124\n",
      "Epoch 730/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 40.7672 - val_loss: 39.9568\n",
      "Epoch 731/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 39.7909 - val_loss: 39.9029\n",
      "Epoch 732/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 43.1143 - val_loss: 39.8513\n",
      "Epoch 733/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 38.4632 - val_loss: 39.7958\n",
      "Epoch 734/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 41.5051 - val_loss: 39.7409\n",
      "Epoch 735/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 40.7594 - val_loss: 39.6824\n",
      "Epoch 736/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 39.6008 - val_loss: 39.6225\n",
      "Epoch 737/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 43.6583 - val_loss: 39.5629\n",
      "Epoch 738/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 42.4376 - val_loss: 39.5021\n",
      "Epoch 739/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 41.3060 - val_loss: 39.4401\n",
      "Epoch 740/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 37.5910 - val_loss: 39.3790\n",
      "Epoch 741/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 39.6456 - val_loss: 39.3149\n",
      "Epoch 742/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 40.2805 - val_loss: 39.2531\n",
      "Epoch 743/1500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 35.1545 - val_loss: 39.1861\n",
      "Epoch 744/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 35.6322 - val_loss: 39.1201\n",
      "Epoch 745/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 35.3303 - val_loss: 39.0584\n",
      "Epoch 746/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 40.9647 - val_loss: 38.9981\n",
      "Epoch 747/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 41.7248 - val_loss: 38.9391\n",
      "Epoch 748/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 39.3357 - val_loss: 38.8819\n",
      "Epoch 749/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 39.7619 - val_loss: 38.8243\n",
      "Epoch 750/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 36.3710 - val_loss: 38.7662\n",
      "Epoch 751/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 37.4656 - val_loss: 38.7048\n",
      "Epoch 752/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 40.5071 - val_loss: 38.6380\n",
      "Epoch 753/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 40.0052 - val_loss: 38.5706\n",
      "Epoch 754/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 40.0357 - val_loss: 38.5061\n",
      "Epoch 755/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 32.8355 - val_loss: 38.4424\n",
      "Epoch 756/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 37.9931 - val_loss: 38.3803\n",
      "Epoch 757/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 36.3869 - val_loss: 38.3141\n",
      "Epoch 758/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 40.1010 - val_loss: 38.2462\n",
      "Epoch 759/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 39.9524 - val_loss: 38.1742\n",
      "Epoch 760/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 36.0484 - val_loss: 38.0990\n",
      "Epoch 761/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 39.3741 - val_loss: 38.0290\n",
      "Epoch 762/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 34.2032 - val_loss: 37.9604\n",
      "Epoch 763/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 36.4720 - val_loss: 37.8876\n",
      "Epoch 764/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 36.8862 - val_loss: 37.8209\n",
      "Epoch 765/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 38.3777 - val_loss: 37.7607\n",
      "Epoch 766/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 38.0437 - val_loss: 37.7033\n",
      "Epoch 767/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 38.5659 - val_loss: 37.6433\n",
      "Epoch 768/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 32.8525 - val_loss: 37.5842\n",
      "Epoch 769/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 33.8670 - val_loss: 37.5229\n",
      "Epoch 770/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 39.3171 - val_loss: 37.4634\n",
      "Epoch 771/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 32.9789 - val_loss: 37.4056\n",
      "Epoch 772/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 35.5103 - val_loss: 37.3460\n",
      "Epoch 773/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 37.2442 - val_loss: 37.2864\n",
      "Epoch 774/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 33.7748 - val_loss: 37.2241\n",
      "Epoch 775/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 34.9147 - val_loss: 37.1596\n",
      "Epoch 776/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 36.6376 - val_loss: 37.0909\n",
      "Epoch 777/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 37.4699 - val_loss: 37.0186\n",
      "Epoch 778/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 36.9737 - val_loss: 36.9422\n",
      "Epoch 779/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 40.1137 - val_loss: 36.8681\n",
      "Epoch 780/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 37.1060 - val_loss: 36.7957\n",
      "Epoch 781/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 38.5357 - val_loss: 36.7245\n",
      "Epoch 782/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 39.0916 - val_loss: 36.6573\n",
      "Epoch 783/1500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 37.5137 - val_loss: 36.5916\n",
      "Epoch 784/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 39.0305 - val_loss: 36.5243\n",
      "Epoch 785/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 34.0262 - val_loss: 36.4592\n",
      "Epoch 786/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 33.5205 - val_loss: 36.3935\n",
      "Epoch 787/1500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 36.1843 - val_loss: 36.3329\n",
      "Epoch 788/1500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 35.8340 - val_loss: 36.2667\n",
      "Epoch 789/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 38.4208 - val_loss: 36.2039\n",
      "Epoch 790/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 39.3995 - val_loss: 36.1416\n",
      "Epoch 791/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 35.4694 - val_loss: 36.0840\n",
      "Epoch 792/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 33.4515 - val_loss: 36.0269\n",
      "Epoch 793/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 30.1641 - val_loss: 35.9740\n",
      "Epoch 794/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 36.4039 - val_loss: 35.9232\n",
      "Epoch 795/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 35.9471 - val_loss: 35.8745\n",
      "Epoch 796/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 35.2669 - val_loss: 35.8278\n",
      "Epoch 797/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 36.6825 - val_loss: 35.7757\n",
      "Epoch 798/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 38.1973 - val_loss: 35.7192\n",
      "Epoch 799/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 36.0132 - val_loss: 35.6628\n",
      "Epoch 800/1500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 33.7052 - val_loss: 35.6010\n",
      "Epoch 801/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 34.8258 - val_loss: 35.5360\n",
      "Epoch 802/1500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 38.7129 - val_loss: 35.4637\n",
      "Epoch 803/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 37.5790 - val_loss: 35.3895\n",
      "Epoch 804/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 34.2579 - val_loss: 35.3146\n",
      "Epoch 805/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 38.6960 - val_loss: 35.2404\n",
      "Epoch 806/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 34.1045 - val_loss: 35.1643\n",
      "Epoch 807/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 33.4868 - val_loss: 35.0879\n",
      "Epoch 808/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 41.8146 - val_loss: 35.0115\n",
      "Epoch 809/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 37.9172 - val_loss: 34.9385\n",
      "Epoch 810/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 35.6566 - val_loss: 34.8748\n",
      "Epoch 811/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 34.8269 - val_loss: 34.8144\n",
      "Epoch 812/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 32.4295 - val_loss: 34.7562\n",
      "Epoch 813/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 37.7635 - val_loss: 34.6941\n",
      "Epoch 814/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 34.7430 - val_loss: 34.6324\n",
      "Epoch 815/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 35.1932 - val_loss: 34.5719\n",
      "Epoch 816/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 33.0692 - val_loss: 34.5080\n",
      "Epoch 817/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 32.0557 - val_loss: 34.4407\n",
      "Epoch 818/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 36.7964 - val_loss: 34.3751\n",
      "Epoch 819/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 36.7302 - val_loss: 34.3148\n",
      "Epoch 820/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 31.6791 - val_loss: 34.2566\n",
      "Epoch 821/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 36.5104 - val_loss: 34.1964\n",
      "Epoch 822/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 34.2106 - val_loss: 34.1375\n",
      "Epoch 823/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 34.5528 - val_loss: 34.0781\n",
      "Epoch 824/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 30.9657 - val_loss: 34.0146\n",
      "Epoch 825/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 28.6426 - val_loss: 33.9532\n",
      "Epoch 826/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 36.2036 - val_loss: 33.8913\n",
      "Epoch 827/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 33.9618 - val_loss: 33.8301\n",
      "Epoch 828/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 33.1643 - val_loss: 33.7688\n",
      "Epoch 829/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 36.9908 - val_loss: 33.7108\n",
      "Epoch 830/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 27.4058 - val_loss: 33.6542\n",
      "Epoch 831/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 34.8453 - val_loss: 33.5937\n",
      "Epoch 832/1500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 34.7610 - val_loss: 33.5313\n",
      "Epoch 833/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 31.9573 - val_loss: 33.4708\n",
      "Epoch 834/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 35.6713 - val_loss: 33.4096\n",
      "Epoch 835/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 31.1881 - val_loss: 33.3463\n",
      "Epoch 836/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 29.8976 - val_loss: 33.2859\n",
      "Epoch 837/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 29.7232 - val_loss: 33.2251\n",
      "Epoch 838/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 32.3596 - val_loss: 33.1678\n",
      "Epoch 839/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 34.2241 - val_loss: 33.1105\n",
      "Epoch 840/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 31.9825 - val_loss: 33.0500\n",
      "Epoch 841/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 34.3200 - val_loss: 32.9855\n",
      "Epoch 842/1500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 35.8856 - val_loss: 32.9174\n",
      "Epoch 843/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 32.0656 - val_loss: 32.8531\n",
      "Epoch 844/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 34.9557 - val_loss: 32.7889\n",
      "Epoch 845/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 27.6827 - val_loss: 32.7215\n",
      "Epoch 846/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 29.7963 - val_loss: 32.6561\n",
      "Epoch 847/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 34.2714 - val_loss: 32.5933\n",
      "Epoch 848/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 30.2224 - val_loss: 32.5292\n",
      "Epoch 849/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 33.4373 - val_loss: 32.4682\n",
      "Epoch 850/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 27.5715 - val_loss: 32.4075\n",
      "Epoch 851/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 34.2567 - val_loss: 32.3474\n",
      "Epoch 852/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 30.4591 - val_loss: 32.2868\n",
      "Epoch 853/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 26.2119 - val_loss: 32.2289\n",
      "Epoch 854/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 28.8492 - val_loss: 32.1725\n",
      "Epoch 855/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 32.6244 - val_loss: 32.1147\n",
      "Epoch 856/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 35.3571 - val_loss: 32.0564\n",
      "Epoch 857/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 31.6895 - val_loss: 31.9927\n",
      "Epoch 858/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 30.4586 - val_loss: 31.9302\n",
      "Epoch 859/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 33.0047 - val_loss: 31.8673\n",
      "Epoch 860/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 31.0188 - val_loss: 31.8066\n",
      "Epoch 861/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 28.3979 - val_loss: 31.7493\n",
      "Epoch 862/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 29.3769 - val_loss: 31.6935\n",
      "Epoch 863/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 31.7661 - val_loss: 31.6406\n",
      "Epoch 864/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 29.5732 - val_loss: 31.5894\n",
      "Epoch 865/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 35.7542 - val_loss: 31.5379\n",
      "Epoch 866/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 31.0366 - val_loss: 31.4853\n",
      "Epoch 867/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 28.3131 - val_loss: 31.4233\n",
      "Epoch 868/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 28.7576 - val_loss: 31.3551\n",
      "Epoch 869/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 32.7429 - val_loss: 31.2861\n",
      "Epoch 870/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 33.1839 - val_loss: 31.2140\n",
      "Epoch 871/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 28.7931 - val_loss: 31.1446\n",
      "Epoch 872/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 32.3682 - val_loss: 31.0779\n",
      "Epoch 873/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 30.3854 - val_loss: 31.0175\n",
      "Epoch 874/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 35.4464 - val_loss: 30.9595\n",
      "Epoch 875/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 27.4480 - val_loss: 30.8976\n",
      "Epoch 876/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 30.5778 - val_loss: 30.8347\n",
      "Epoch 877/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 30.8004 - val_loss: 30.7651\n",
      "Epoch 878/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 28.3514 - val_loss: 30.6947\n",
      "Epoch 879/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 37.3853 - val_loss: 30.6270\n",
      "Epoch 880/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 30.3593 - val_loss: 30.5582\n",
      "Epoch 881/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 30.6435 - val_loss: 30.4866\n",
      "Epoch 882/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 28.3845 - val_loss: 30.4181\n",
      "Epoch 883/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 32.0228 - val_loss: 30.3521\n",
      "Epoch 884/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 30.5048 - val_loss: 30.2850\n",
      "Epoch 885/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 30.0749 - val_loss: 30.2196\n",
      "Epoch 886/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 31.9292 - val_loss: 30.1555\n",
      "Epoch 887/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 29.0231 - val_loss: 30.0916\n",
      "Epoch 888/1500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 32.9894 - val_loss: 30.0305\n",
      "Epoch 889/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 34.4705 - val_loss: 29.9677\n",
      "Epoch 890/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 29.0242 - val_loss: 29.9040\n",
      "Epoch 891/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 29.8060 - val_loss: 29.8421\n",
      "Epoch 892/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 29.3896 - val_loss: 29.7791\n",
      "Epoch 893/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 30.2065 - val_loss: 29.7169\n",
      "Epoch 894/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 31.0299 - val_loss: 29.6541\n",
      "Epoch 895/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 31.5474 - val_loss: 29.5977\n",
      "Epoch 896/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 34.1287 - val_loss: 29.5407\n",
      "Epoch 897/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 27.2783 - val_loss: 29.4813\n",
      "Epoch 898/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 29.0241 - val_loss: 29.4205\n",
      "Epoch 899/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 29.7258 - val_loss: 29.3537\n",
      "Epoch 900/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 33.5931 - val_loss: 29.2884\n",
      "Epoch 901/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 24.6271 - val_loss: 29.2252\n",
      "Epoch 902/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 33.1853 - val_loss: 29.1614\n",
      "Epoch 903/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 27.6795 - val_loss: 29.0975\n",
      "Epoch 904/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 30.4910 - val_loss: 29.0350\n",
      "Epoch 905/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 30.1128 - val_loss: 28.9762\n",
      "Epoch 906/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 23.1462 - val_loss: 28.9155\n",
      "Epoch 907/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 30.7589 - val_loss: 28.8554\n",
      "Epoch 908/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 22.9263 - val_loss: 28.7927\n",
      "Epoch 909/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 23.2481 - val_loss: 28.7284\n",
      "Epoch 910/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 30.6831 - val_loss: 28.6657\n",
      "Epoch 911/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 29.4815 - val_loss: 28.6022\n",
      "Epoch 912/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 34.4281 - val_loss: 28.5391\n",
      "Epoch 913/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 23.2247 - val_loss: 28.4782\n",
      "Epoch 914/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 27.4073 - val_loss: 28.4206\n",
      "Epoch 915/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 29.9074 - val_loss: 28.3628\n",
      "Epoch 916/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 26.5920 - val_loss: 28.2991\n",
      "Epoch 917/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 27.4849 - val_loss: 28.2345\n",
      "Epoch 918/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 29.0723 - val_loss: 28.1647\n",
      "Epoch 919/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 25.1595 - val_loss: 28.0933\n",
      "Epoch 920/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 26.8453 - val_loss: 28.0231\n",
      "Epoch 921/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 30.0215 - val_loss: 27.9559\n",
      "Epoch 922/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 23.3710 - val_loss: 27.8885\n",
      "Epoch 923/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 28.8925 - val_loss: 27.8249\n",
      "Epoch 924/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 28.1567 - val_loss: 27.7593\n",
      "Epoch 925/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 26.4553 - val_loss: 27.6940\n",
      "Epoch 926/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 26.9191 - val_loss: 27.6322\n",
      "Epoch 927/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 28.1937 - val_loss: 27.5742\n",
      "Epoch 928/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 27.9736 - val_loss: 27.5175\n",
      "Epoch 929/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 31.4402 - val_loss: 27.4602\n",
      "Epoch 930/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 26.9269 - val_loss: 27.4048\n",
      "Epoch 931/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 25.7604 - val_loss: 27.3467\n",
      "Epoch 932/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 27.7531 - val_loss: 27.2914\n",
      "Epoch 933/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 26.2488 - val_loss: 27.2318\n",
      "Epoch 934/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 25.9263 - val_loss: 27.1687\n",
      "Epoch 935/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 27.2206 - val_loss: 27.1028\n",
      "Epoch 936/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 28.0677 - val_loss: 27.0346\n",
      "Epoch 937/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 28.3034 - val_loss: 26.9662\n",
      "Epoch 938/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 27.5675 - val_loss: 26.8983\n",
      "Epoch 939/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 22.2094 - val_loss: 26.8279\n",
      "Epoch 940/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 29.4641 - val_loss: 26.7600\n",
      "Epoch 941/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 25.9480 - val_loss: 26.6960\n",
      "Epoch 942/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 26.4172 - val_loss: 26.6298\n",
      "Epoch 943/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 27.8770 - val_loss: 26.5650\n",
      "Epoch 944/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 27.6578 - val_loss: 26.4995\n",
      "Epoch 945/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 28.2293 - val_loss: 26.4373\n",
      "Epoch 946/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 26.8337 - val_loss: 26.3821\n",
      "Epoch 947/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 27.7449 - val_loss: 26.3270\n",
      "Epoch 948/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 23.0628 - val_loss: 26.2684\n",
      "Epoch 949/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 21.2379 - val_loss: 26.2110\n",
      "Epoch 950/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 29.1950 - val_loss: 26.1522\n",
      "Epoch 951/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 28.3094 - val_loss: 26.0936\n",
      "Epoch 952/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 27.8173 - val_loss: 26.0377\n",
      "Epoch 953/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 24.5115 - val_loss: 25.9772\n",
      "Epoch 954/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 23.7447 - val_loss: 25.9165\n",
      "Epoch 955/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 28.8034 - val_loss: 25.8601\n",
      "Epoch 956/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 22.3486 - val_loss: 25.8054\n",
      "Epoch 957/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 24.0453 - val_loss: 25.7573\n",
      "Epoch 958/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 27.7155 - val_loss: 25.7042\n",
      "Epoch 959/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 30.4715 - val_loss: 25.6486\n",
      "Epoch 960/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 23.8910 - val_loss: 25.5843\n",
      "Epoch 961/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 24.2427 - val_loss: 25.5187\n",
      "Epoch 962/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 25.2705 - val_loss: 25.4533\n",
      "Epoch 963/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 27.6837 - val_loss: 25.3886\n",
      "Epoch 964/1500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 23.5043 - val_loss: 25.3270\n",
      "Epoch 965/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 21.7065 - val_loss: 25.2634\n",
      "Epoch 966/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 27.9453 - val_loss: 25.2002\n",
      "Epoch 967/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 21.8736 - val_loss: 25.1341\n",
      "Epoch 968/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 27.6610 - val_loss: 25.0694\n",
      "Epoch 969/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 22.0572 - val_loss: 25.0063\n",
      "Epoch 970/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 22.6632 - val_loss: 24.9445\n",
      "Epoch 971/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 21.6694 - val_loss: 24.8850\n",
      "Epoch 972/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 22.3356 - val_loss: 24.8287\n",
      "Epoch 973/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 26.1489 - val_loss: 24.7674\n",
      "Epoch 974/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 28.1615 - val_loss: 24.7055\n",
      "Epoch 975/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 26.8564 - val_loss: 24.6442\n",
      "Epoch 976/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 26.4662 - val_loss: 24.5783\n",
      "Epoch 977/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 22.8090 - val_loss: 24.5100\n",
      "Epoch 978/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 24.4382 - val_loss: 24.4461\n",
      "Epoch 979/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 24.3560 - val_loss: 24.3832\n",
      "Epoch 980/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 28.2936 - val_loss: 24.3200\n",
      "Epoch 981/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 32.5387 - val_loss: 24.2582\n",
      "Epoch 982/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 25.7381 - val_loss: 24.1965\n",
      "Epoch 983/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 20.0328 - val_loss: 24.1338\n",
      "Epoch 984/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 22.5487 - val_loss: 24.0688\n",
      "Epoch 985/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 19.0214 - val_loss: 24.0020\n",
      "Epoch 986/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 22.0450 - val_loss: 23.9449\n",
      "Epoch 987/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 23.7462 - val_loss: 23.8827\n",
      "Epoch 988/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 23.7801 - val_loss: 23.8211\n",
      "Epoch 989/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 23.1758 - val_loss: 23.7586\n",
      "Epoch 990/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 28.3646 - val_loss: 23.6959\n",
      "Epoch 991/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 22.1987 - val_loss: 23.6346\n",
      "Epoch 992/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 24.6482 - val_loss: 23.5734\n",
      "Epoch 993/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 24.2878 - val_loss: 23.5116\n",
      "Epoch 994/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 19.8718 - val_loss: 23.4502\n",
      "Epoch 995/1500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 23.3818 - val_loss: 23.3876\n",
      "Epoch 996/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 18.5569 - val_loss: 23.3235\n",
      "Epoch 997/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 21.9939 - val_loss: 23.2619\n",
      "Epoch 998/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 19.6641 - val_loss: 23.1995\n",
      "Epoch 999/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 21.8713 - val_loss: 23.1363\n",
      "Epoch 1000/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 20.8601 - val_loss: 23.0738\n",
      "Epoch 1001/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 24.1994 - val_loss: 23.0127\n",
      "Epoch 1002/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 27.5123 - val_loss: 22.9516\n",
      "Epoch 1003/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 25.5827 - val_loss: 22.8888\n",
      "Epoch 1004/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 19.5057 - val_loss: 22.8246\n",
      "Epoch 1005/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 24.3848 - val_loss: 22.7566\n",
      "Epoch 1006/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 23.3148 - val_loss: 22.6900\n",
      "Epoch 1007/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 25.1973 - val_loss: 22.6278\n",
      "Epoch 1008/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 21.0316 - val_loss: 22.5670\n",
      "Epoch 1009/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 18.3828 - val_loss: 22.5055\n",
      "Epoch 1010/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 18.7732 - val_loss: 22.4426\n",
      "Epoch 1011/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 22.8095 - val_loss: 22.3836\n",
      "Epoch 1012/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 25.1965 - val_loss: 22.3271\n",
      "Epoch 1013/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 21.9600 - val_loss: 22.2736\n",
      "Epoch 1014/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 27.0768 - val_loss: 22.2181\n",
      "Epoch 1015/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 18.9722 - val_loss: 22.1617\n",
      "Epoch 1016/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 22.8232 - val_loss: 22.1030\n",
      "Epoch 1017/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 21.8935 - val_loss: 22.0478\n",
      "Epoch 1018/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 22.7839 - val_loss: 21.9950\n",
      "Epoch 1019/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 22.9303 - val_loss: 21.9438\n",
      "Epoch 1020/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 19.1866 - val_loss: 21.8928\n",
      "Epoch 1021/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 28.1854 - val_loss: 21.8412\n",
      "Epoch 1022/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 23.0886 - val_loss: 21.7892\n",
      "Epoch 1023/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 19.5806 - val_loss: 21.7417\n",
      "Epoch 1024/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 16.6629 - val_loss: 21.6931\n",
      "Epoch 1025/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 22.9394 - val_loss: 21.6424\n",
      "Epoch 1026/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 18.1079 - val_loss: 21.5918\n",
      "Epoch 1027/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 20.7206 - val_loss: 21.5394\n",
      "Epoch 1028/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 25.0557 - val_loss: 21.4855\n",
      "Epoch 1029/1500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 21.0759 - val_loss: 21.4313\n",
      "Epoch 1030/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 16.4959 - val_loss: 21.3731\n",
      "Epoch 1031/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 20.5150 - val_loss: 21.3117\n",
      "Epoch 1032/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 25.1438 - val_loss: 21.2475\n",
      "Epoch 1033/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 20.5212 - val_loss: 21.1801\n",
      "Epoch 1034/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 19.7391 - val_loss: 21.1161\n",
      "Epoch 1035/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 14.1514 - val_loss: 21.0494\n",
      "Epoch 1036/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 24.7261 - val_loss: 20.9859\n",
      "Epoch 1037/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 19.6070 - val_loss: 20.9170\n",
      "Epoch 1038/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 24.9032 - val_loss: 20.8497\n",
      "Epoch 1039/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 23.4311 - val_loss: 20.7869\n",
      "Epoch 1040/1500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 17.0313 - val_loss: 20.7249\n",
      "Epoch 1041/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 22.7052 - val_loss: 20.6670\n",
      "Epoch 1042/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 23.3186 - val_loss: 20.6134\n",
      "Epoch 1043/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 18.3499 - val_loss: 20.5658\n",
      "Epoch 1044/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 16.4509 - val_loss: 20.5194\n",
      "Epoch 1045/1500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 22.0914 - val_loss: 20.4730\n",
      "Epoch 1046/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 20.4140 - val_loss: 20.4200\n",
      "Epoch 1047/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 16.7670 - val_loss: 20.3683\n",
      "Epoch 1048/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 17.7746 - val_loss: 20.3164\n",
      "Epoch 1049/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 24.8422 - val_loss: 20.2647\n",
      "Epoch 1050/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 17.4287 - val_loss: 20.2160\n",
      "Epoch 1051/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 25.6976 - val_loss: 20.1579\n",
      "Epoch 1052/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 18.5925 - val_loss: 20.0941\n",
      "Epoch 1053/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 21.5419 - val_loss: 20.0301\n",
      "Epoch 1054/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 21.1426 - val_loss: 19.9596\n",
      "Epoch 1055/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 18.1885 - val_loss: 19.8912\n",
      "Epoch 1056/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 25.2154 - val_loss: 19.8235\n",
      "Epoch 1057/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 18.9842 - val_loss: 19.7580\n",
      "Epoch 1058/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 17.2134 - val_loss: 19.6901\n",
      "Epoch 1059/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 18.9571 - val_loss: 19.6249\n",
      "Epoch 1060/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 14.9445 - val_loss: 19.5591\n",
      "Epoch 1061/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 20.4185 - val_loss: 19.4923\n",
      "Epoch 1062/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 22.8029 - val_loss: 19.4274\n",
      "Epoch 1063/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 14.9978 - val_loss: 19.3621\n",
      "Epoch 1064/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 16.6853 - val_loss: 19.2971\n",
      "Epoch 1065/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 14.3581 - val_loss: 19.2338\n",
      "Epoch 1066/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 24.7638 - val_loss: 19.1695\n",
      "Epoch 1067/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 20.1301 - val_loss: 19.1060\n",
      "Epoch 1068/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 18.1695 - val_loss: 19.0438\n",
      "Epoch 1069/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 16.6740 - val_loss: 18.9809\n",
      "Epoch 1070/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 19.0912 - val_loss: 18.9216\n",
      "Epoch 1071/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 22.1993 - val_loss: 18.8627\n",
      "Epoch 1072/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 19.4842 - val_loss: 18.8074\n",
      "Epoch 1073/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 20.5349 - val_loss: 18.7521\n",
      "Epoch 1074/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 14.2499 - val_loss: 18.6962\n",
      "Epoch 1075/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 21.7700 - val_loss: 18.6398\n",
      "Epoch 1076/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 22.6246 - val_loss: 18.5809\n",
      "Epoch 1077/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 25.1761 - val_loss: 18.5237\n",
      "Epoch 1078/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 17.5989 - val_loss: 18.4658\n",
      "Epoch 1079/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 18.5560 - val_loss: 18.4078\n",
      "Epoch 1080/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 21.3980 - val_loss: 18.3505\n",
      "Epoch 1081/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 14.7099 - val_loss: 18.2945\n",
      "Epoch 1082/1500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 20.6935 - val_loss: 18.2384\n",
      "Epoch 1083/1500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 17.9239 - val_loss: 18.1840\n",
      "Epoch 1084/1500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 19.4072 - val_loss: 18.1273\n",
      "Epoch 1085/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 17.8983 - val_loss: 18.0671\n",
      "Epoch 1086/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 19.8898 - val_loss: 18.0119\n",
      "Epoch 1087/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 19.5985 - val_loss: 17.9561\n",
      "Epoch 1088/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 19.7837 - val_loss: 17.9005\n",
      "Epoch 1089/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 16.4170 - val_loss: 17.8456\n",
      "Epoch 1090/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 16.2915 - val_loss: 17.7917\n",
      "Epoch 1091/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 19.4563 - val_loss: 17.7400\n",
      "Epoch 1092/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 21.0066 - val_loss: 17.6938\n",
      "Epoch 1093/1500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 18.0997 - val_loss: 17.6489\n",
      "Epoch 1094/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 18.0958 - val_loss: 17.6008\n",
      "Epoch 1095/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 18.6880 - val_loss: 17.5458\n",
      "Epoch 1096/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 19.3102 - val_loss: 17.4937\n",
      "Epoch 1097/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 15.5923 - val_loss: 17.4494\n",
      "Epoch 1098/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 17.3464 - val_loss: 17.4058\n",
      "Epoch 1099/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 21.6434 - val_loss: 17.3646\n",
      "Epoch 1100/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 16.5235 - val_loss: 17.3233\n",
      "Epoch 1101/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 15.5629 - val_loss: 17.2779\n",
      "Epoch 1102/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 19.4471 - val_loss: 17.2359\n",
      "Epoch 1103/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 18.5348 - val_loss: 17.1822\n",
      "Epoch 1104/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 21.5805 - val_loss: 17.1268\n",
      "Epoch 1105/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 17.6734 - val_loss: 17.0738\n",
      "Epoch 1106/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 20.6182 - val_loss: 17.0180\n",
      "Epoch 1107/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 18.0946 - val_loss: 16.9560\n",
      "Epoch 1108/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 15.1501 - val_loss: 16.8965\n",
      "Epoch 1109/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 24.5693 - val_loss: 16.8379\n",
      "Epoch 1110/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 15.2048 - val_loss: 16.7729\n",
      "Epoch 1111/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 16.4972 - val_loss: 16.7055\n",
      "Epoch 1112/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 26.0775 - val_loss: 16.6384\n",
      "Epoch 1113/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 16.5514 - val_loss: 16.5743\n",
      "Epoch 1114/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 18.4350 - val_loss: 16.5163\n",
      "Epoch 1115/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 15.7033 - val_loss: 16.4603\n",
      "Epoch 1116/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 11.4136 - val_loss: 16.4083\n",
      "Epoch 1117/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 16.9472 - val_loss: 16.3540\n",
      "Epoch 1118/1500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 17.1250 - val_loss: 16.3004\n",
      "Epoch 1119/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 17.6353 - val_loss: 16.2467\n",
      "Epoch 1120/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 10.9133 - val_loss: 16.2006\n",
      "Epoch 1121/1500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 14.6861 - val_loss: 16.1529\n",
      "Epoch 1122/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 19.2053 - val_loss: 16.1058\n",
      "Epoch 1123/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 14.6888 - val_loss: 16.0548\n",
      "Epoch 1124/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 18.2884 - val_loss: 16.0077\n",
      "Epoch 1125/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 18.6825 - val_loss: 15.9642\n",
      "Epoch 1126/1500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 10.1980 - val_loss: 15.9226\n",
      "Epoch 1127/1500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 19.6629 - val_loss: 15.8808\n",
      "Epoch 1128/1500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 16.5926 - val_loss: 15.8381\n",
      "Epoch 1129/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 17.3194 - val_loss: 15.7981\n",
      "Epoch 1130/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 15.5151 - val_loss: 15.7558\n",
      "Epoch 1131/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 19.4021 - val_loss: 15.7120\n",
      "Epoch 1132/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 15.8426 - val_loss: 15.6639\n",
      "Epoch 1133/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 14.9440 - val_loss: 15.6151\n",
      "Epoch 1134/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 13.7421 - val_loss: 15.5616\n",
      "Epoch 1135/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 14.6669 - val_loss: 15.5015\n",
      "Epoch 1136/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 22.2092 - val_loss: 15.4357\n",
      "Epoch 1137/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 16.6992 - val_loss: 15.3770\n",
      "Epoch 1138/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 16.1503 - val_loss: 15.3209\n",
      "Epoch 1139/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 18.1494 - val_loss: 15.2680\n",
      "Epoch 1140/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 17.6162 - val_loss: 15.2242\n",
      "Epoch 1141/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 18.9598 - val_loss: 15.1783\n",
      "Epoch 1142/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 17.1466 - val_loss: 15.1253\n",
      "Epoch 1143/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 18.2191 - val_loss: 15.0684\n",
      "Epoch 1144/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 16.2387 - val_loss: 15.0133\n",
      "Epoch 1145/1500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 15.2110 - val_loss: 14.9584\n",
      "Epoch 1146/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 13.5235 - val_loss: 14.9081\n",
      "Epoch 1147/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 17.3036 - val_loss: 14.8650\n",
      "Epoch 1148/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 15.6194 - val_loss: 14.8229\n",
      "Epoch 1149/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 12.7950 - val_loss: 14.7821\n",
      "Epoch 1150/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 14.2091 - val_loss: 14.7374\n",
      "Epoch 1151/1500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 9.0596 - val_loss: 14.6869\n",
      "Epoch 1152/1500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 16.5857 - val_loss: 14.6372\n",
      "Epoch 1153/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 23.0592 - val_loss: 14.5803\n",
      "Epoch 1154/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 10.4044 - val_loss: 14.5237\n",
      "Epoch 1155/1500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 11.1690 - val_loss: 14.4628\n",
      "Epoch 1156/1500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 19.1136 - val_loss: 14.4017\n",
      "Epoch 1157/1500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 11.9406 - val_loss: 14.3432\n",
      "Epoch 1158/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 9.9059 - val_loss: 14.2846\n",
      "Epoch 1159/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 19.4311 - val_loss: 14.2244\n",
      "Epoch 1160/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 19.8066 - val_loss: 14.1683\n",
      "Epoch 1161/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 9.6875 - val_loss: 14.1079\n",
      "Epoch 1162/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 15.6496 - val_loss: 14.0513\n",
      "Epoch 1163/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 20.6267 - val_loss: 13.9980\n",
      "Epoch 1164/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 18.0633 - val_loss: 13.9446\n",
      "Epoch 1165/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 17.1258 - val_loss: 13.8922\n",
      "Epoch 1166/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 13.6049 - val_loss: 13.8415\n",
      "Epoch 1167/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 15.5987 - val_loss: 13.7951\n",
      "Epoch 1168/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 13.6609 - val_loss: 13.7483\n",
      "Epoch 1169/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 17.5582 - val_loss: 13.7051\n",
      "Epoch 1170/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 13.3170 - val_loss: 13.6607\n",
      "Epoch 1171/1500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 16.1972 - val_loss: 13.6156\n",
      "Epoch 1172/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 22.0130 - val_loss: 13.5707\n",
      "Epoch 1173/1500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 15.8083 - val_loss: 13.5247\n",
      "Epoch 1174/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 10.0802 - val_loss: 13.4777\n",
      "Epoch 1175/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 20.2734 - val_loss: 13.4353\n",
      "Epoch 1176/1500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 13.6095 - val_loss: 13.3937\n",
      "Epoch 1177/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 11.3944 - val_loss: 13.3452\n",
      "Epoch 1178/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 20.3542 - val_loss: 13.2950\n",
      "Epoch 1179/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 12.6211 - val_loss: 13.2490\n",
      "Epoch 1180/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 18.9719 - val_loss: 13.2020\n",
      "Epoch 1181/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 14.5582 - val_loss: 13.1491\n",
      "Epoch 1182/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 14.1646 - val_loss: 13.0974\n",
      "Epoch 1183/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 14.5671 - val_loss: 13.0464\n",
      "Epoch 1184/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 14.7744 - val_loss: 12.9907\n",
      "Epoch 1185/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 14.1346 - val_loss: 12.9333\n",
      "Epoch 1186/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 17.2935 - val_loss: 12.8775\n",
      "Epoch 1187/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 15.2282 - val_loss: 12.8196\n",
      "Epoch 1188/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 19.2949 - val_loss: 12.7692\n",
      "Epoch 1189/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 14.5682 - val_loss: 12.7284\n",
      "Epoch 1190/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 17.1563 - val_loss: 12.6868\n",
      "Epoch 1191/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 11.1959 - val_loss: 12.6432\n",
      "Epoch 1192/1500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 17.9957 - val_loss: 12.6045\n",
      "Epoch 1193/1500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 13.0136 - val_loss: 12.5625\n",
      "Epoch 1194/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 14.5685 - val_loss: 12.5161\n",
      "Epoch 1195/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 11.1566 - val_loss: 12.4659\n",
      "Epoch 1196/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 13.2736 - val_loss: 12.4125\n",
      "Epoch 1197/1500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 9.2505 - val_loss: 12.3740\n",
      "Epoch 1198/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 10.5576 - val_loss: 12.3423\n",
      "Epoch 1199/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 17.4745 - val_loss: 12.3109\n",
      "Epoch 1200/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 14.7552 - val_loss: 12.2832\n",
      "Epoch 1201/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 12.7777 - val_loss: 12.2545\n",
      "Epoch 1202/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 14.1599 - val_loss: 12.2242\n",
      "Epoch 1203/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 18.6718 - val_loss: 12.1884\n",
      "Epoch 1204/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 14.6184 - val_loss: 12.1501\n",
      "Epoch 1205/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 15.3358 - val_loss: 12.1052\n",
      "Epoch 1206/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 13.6842 - val_loss: 12.0656\n",
      "Epoch 1207/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 16.1049 - val_loss: 12.0196\n",
      "Epoch 1208/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 13.7992 - val_loss: 11.9689\n",
      "Epoch 1209/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 13.9661 - val_loss: 11.9173\n",
      "Epoch 1210/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 11.9792 - val_loss: 11.8697\n",
      "Epoch 1211/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 8.2398 - val_loss: 11.8308\n",
      "Epoch 1212/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 10.7497 - val_loss: 11.7936\n",
      "Epoch 1213/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 19.8758 - val_loss: 11.7547\n",
      "Epoch 1214/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 13.2136 - val_loss: 11.7059\n",
      "Epoch 1215/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 11.1885 - val_loss: 11.6610\n",
      "Epoch 1216/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 12.6100 - val_loss: 11.6262\n",
      "Epoch 1217/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 13.3843 - val_loss: 11.5856\n",
      "Epoch 1218/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 12.9913 - val_loss: 11.5426\n",
      "Epoch 1219/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 12.5499 - val_loss: 11.4965\n",
      "Epoch 1220/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 12.3301 - val_loss: 11.4412\n",
      "Epoch 1221/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 13.0095 - val_loss: 11.3866\n",
      "Epoch 1222/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 15.2232 - val_loss: 11.3276\n",
      "Epoch 1223/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 10.1626 - val_loss: 11.2695\n",
      "Epoch 1224/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 14.8839 - val_loss: 11.2151\n",
      "Epoch 1225/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 11.8960 - val_loss: 11.1656\n",
      "Epoch 1226/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 12.6082 - val_loss: 11.1230\n",
      "Epoch 1227/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 8.3272 - val_loss: 11.0783\n",
      "Epoch 1228/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 9.7606 - val_loss: 11.0308\n",
      "Epoch 1229/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 12.7453 - val_loss: 10.9881\n",
      "Epoch 1230/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 16.6927 - val_loss: 10.9474\n",
      "Epoch 1231/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 13.5371 - val_loss: 10.9104\n",
      "Epoch 1232/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 13.6224 - val_loss: 10.8765\n",
      "Epoch 1233/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 14.4453 - val_loss: 10.8389\n",
      "Epoch 1234/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 15.6093 - val_loss: 10.7991\n",
      "Epoch 1235/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 16.9174 - val_loss: 10.7648\n",
      "Epoch 1236/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 13.4321 - val_loss: 10.7283\n",
      "Epoch 1237/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 10.9290 - val_loss: 10.6890\n",
      "Epoch 1238/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 9.6985 - val_loss: 10.6459\n",
      "Epoch 1239/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 12.1291 - val_loss: 10.5997\n",
      "Epoch 1240/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 12.1496 - val_loss: 10.5543\n",
      "Epoch 1241/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 15.2468 - val_loss: 10.5065\n",
      "Epoch 1242/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 13.5720 - val_loss: 10.4666\n",
      "Epoch 1243/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 9.4918 - val_loss: 10.4304\n",
      "Epoch 1244/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 16.2776 - val_loss: 10.3936\n",
      "Epoch 1245/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 11.9485 - val_loss: 10.3572\n",
      "Epoch 1246/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 11.7713 - val_loss: 10.3156\n",
      "Epoch 1247/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 11.8951 - val_loss: 10.2788\n",
      "Epoch 1248/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.9247 - val_loss: 10.2413\n",
      "Epoch 1249/1500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 14.5981 - val_loss: 10.2087\n",
      "Epoch 1250/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 8.1606 - val_loss: 10.1698\n",
      "Epoch 1251/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 10.3231 - val_loss: 10.1315\n",
      "Epoch 1252/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 11.5411 - val_loss: 10.0901\n",
      "Epoch 1253/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 12.1602 - val_loss: 10.0570\n",
      "Epoch 1254/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 14.6540 - val_loss: 10.0234\n",
      "Epoch 1255/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 14.4330 - val_loss: 9.9899\n",
      "Epoch 1256/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 10.6039 - val_loss: 9.9612\n",
      "Epoch 1257/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 12.6787 - val_loss: 9.9376\n",
      "Epoch 1258/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 10.5486 - val_loss: 9.9081\n",
      "Epoch 1259/1500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 9.0409 - val_loss: 9.8724\n",
      "Epoch 1260/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 7.8986 - val_loss: 9.8444\n",
      "Epoch 1261/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 17.6758 - val_loss: 9.8171\n",
      "Epoch 1262/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 12.0208 - val_loss: 9.7983\n",
      "Epoch 1263/1500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 6.7594 - val_loss: 9.7807\n",
      "Epoch 1264/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 9.3764 - val_loss: 9.7604\n",
      "Epoch 1265/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 14.8930 - val_loss: 9.7361\n",
      "Epoch 1266/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 12.8911 - val_loss: 9.7160\n",
      "Epoch 1267/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 9.9625 - val_loss: 9.6902\n",
      "Epoch 1268/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 10.1356 - val_loss: 9.6551\n",
      "Epoch 1269/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 13.4098 - val_loss: 9.6179\n",
      "Epoch 1270/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 5.8298 - val_loss: 9.5756\n",
      "Epoch 1271/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 10.7072 - val_loss: 9.5285\n",
      "Epoch 1272/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 11.3331 - val_loss: 9.4837\n",
      "Epoch 1273/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 13.5348 - val_loss: 9.4466\n",
      "Epoch 1274/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 10.4523 - val_loss: 9.4194\n",
      "Epoch 1275/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 7.4023 - val_loss: 9.3970\n",
      "Epoch 1276/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 12.8527 - val_loss: 9.3756\n",
      "Epoch 1277/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 14.1348 - val_loss: 9.3506\n",
      "Epoch 1278/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 11.0155 - val_loss: 9.3240\n",
      "Epoch 1279/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 12.4259 - val_loss: 9.2981\n",
      "Epoch 1280/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 10.7926 - val_loss: 9.2707\n",
      "Epoch 1281/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 10.9693 - val_loss: 9.2421\n",
      "Epoch 1282/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 12.3406 - val_loss: 9.2194\n",
      "Epoch 1283/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 12.7168 - val_loss: 9.1903\n",
      "Epoch 1284/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 9.7778 - val_loss: 9.1560\n",
      "Epoch 1285/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 13.3921 - val_loss: 9.1258\n",
      "Epoch 1286/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 7.3133 - val_loss: 9.1037\n",
      "Epoch 1287/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 11.3257 - val_loss: 9.0761\n",
      "Epoch 1288/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 10.8790 - val_loss: 9.0543\n",
      "Epoch 1289/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 13.4956 - val_loss: 9.0399\n",
      "Epoch 1290/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 7.6567 - val_loss: 9.0291\n",
      "Epoch 1291/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 11.0512 - val_loss: 9.0105\n",
      "Epoch 1292/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 11.7185 - val_loss: 8.9854\n",
      "Epoch 1293/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 9.0073 - val_loss: 8.9580\n",
      "Epoch 1294/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.7247 - val_loss: 8.9245\n",
      "Epoch 1295/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 9.0365 - val_loss: 8.8918\n",
      "Epoch 1296/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 9.9010 - val_loss: 8.8589\n",
      "Epoch 1297/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 11.9520 - val_loss: 8.8220\n",
      "Epoch 1298/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 13.1347 - val_loss: 8.7855\n",
      "Epoch 1299/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 11.3185 - val_loss: 8.7436\n",
      "Epoch 1300/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 14.4315 - val_loss: 8.7021\n",
      "Epoch 1301/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 12.9856 - val_loss: 8.6663\n",
      "Epoch 1302/1500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 15.5815 - val_loss: 8.6282\n",
      "Epoch 1303/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 11.7517 - val_loss: 8.5940\n",
      "Epoch 1304/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 14.1655 - val_loss: 8.5599\n",
      "Epoch 1305/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 14.5110 - val_loss: 8.5265\n",
      "Epoch 1306/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 9.0954 - val_loss: 8.4970\n",
      "Epoch 1307/1500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 9.7487 - val_loss: 8.4705\n",
      "Epoch 1308/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 6.8596 - val_loss: 8.4447\n",
      "Epoch 1309/1500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 14.2572 - val_loss: 8.4235\n",
      "Epoch 1310/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 10.6673 - val_loss: 8.4021\n",
      "Epoch 1311/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 14.1757 - val_loss: 8.3742\n",
      "Epoch 1312/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 12.8778 - val_loss: 8.3484\n",
      "Epoch 1313/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 13.7007 - val_loss: 8.3203\n",
      "Epoch 1314/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 11.3693 - val_loss: 8.2873\n",
      "Epoch 1315/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 6.6290 - val_loss: 8.2544\n",
      "Epoch 1316/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 10.2431 - val_loss: 8.2289\n",
      "Epoch 1317/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 6.3159 - val_loss: 8.2045\n",
      "Epoch 1318/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 10.0346 - val_loss: 8.1764\n",
      "Epoch 1319/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 13.6582 - val_loss: 8.1518\n",
      "Epoch 1320/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 13.4533 - val_loss: 8.1295\n",
      "Epoch 1321/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 11.5445 - val_loss: 8.1126\n",
      "Epoch 1322/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 12.9580 - val_loss: 8.0936\n",
      "Epoch 1323/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 9.8590 - val_loss: 8.0752\n",
      "Epoch 1324/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 13.0875 - val_loss: 8.0569\n",
      "Epoch 1325/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.5013 - val_loss: 8.0383\n",
      "Epoch 1326/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 11.2756 - val_loss: 8.0179\n",
      "Epoch 1327/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 13.7120 - val_loss: 7.9979\n",
      "Epoch 1328/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 12.3914 - val_loss: 7.9782\n",
      "Epoch 1329/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 8.8283 - val_loss: 7.9566\n",
      "Epoch 1330/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 12.8605 - val_loss: 7.9318\n",
      "Epoch 1331/1500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 9.9549 - val_loss: 7.9132\n",
      "Epoch 1332/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 10.1019 - val_loss: 7.8976\n",
      "Epoch 1333/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 12.0357 - val_loss: 7.8821\n",
      "Epoch 1334/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 10.5106 - val_loss: 7.8655\n",
      "Epoch 1335/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 10.1648 - val_loss: 7.8499\n",
      "Epoch 1336/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 7.5149 - val_loss: 7.8271\n",
      "Epoch 1337/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 10.3910 - val_loss: 7.8055\n",
      "Epoch 1338/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 7.7540 - val_loss: 7.7822\n",
      "Epoch 1339/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 14.3579 - val_loss: 7.7525\n",
      "Epoch 1340/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 12.0484 - val_loss: 7.7276\n",
      "Epoch 1341/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 8.9154 - val_loss: 7.7056\n",
      "Epoch 1342/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 9.3651 - val_loss: 7.6852\n",
      "Epoch 1343/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 7.6577 - val_loss: 7.6698\n",
      "Epoch 1344/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 12.1140 - val_loss: 7.6588\n",
      "Epoch 1345/1500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 11.3812 - val_loss: 7.6480\n",
      "Epoch 1346/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 9.7978 - val_loss: 7.6359\n",
      "Epoch 1347/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 10.0956 - val_loss: 7.6248\n",
      "Epoch 1348/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 7.2268 - val_loss: 7.6118\n",
      "Epoch 1349/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 13.2345 - val_loss: 7.5977\n",
      "Epoch 1350/1500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 9.0010 - val_loss: 7.5740\n",
      "Epoch 1351/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 10.0378 - val_loss: 7.5540\n",
      "Epoch 1352/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 12.7967 - val_loss: 7.5341\n",
      "Epoch 1353/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 15.8673 - val_loss: 7.5117\n",
      "Epoch 1354/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 9.0490 - val_loss: 7.4896\n",
      "Epoch 1355/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 10.7931 - val_loss: 7.4693\n",
      "Epoch 1356/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 13.8069 - val_loss: 7.4451\n",
      "Epoch 1357/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 9.1260 - val_loss: 7.4269\n",
      "Epoch 1358/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 9.0791 - val_loss: 7.4156\n",
      "Epoch 1359/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 13.8222 - val_loss: 7.4051\n",
      "Epoch 1360/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 9.8340 - val_loss: 7.3975\n",
      "Epoch 1361/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 17.0931 - val_loss: 7.3865\n",
      "Epoch 1362/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 9.7472 - val_loss: 7.3788\n",
      "Epoch 1363/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 10.1160 - val_loss: 7.3718\n",
      "Epoch 1364/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 9.4919 - val_loss: 7.3711\n",
      "Epoch 1365/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 7.6025 - val_loss: 7.3666\n",
      "Epoch 1366/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 16.0885 - val_loss: 7.3562\n",
      "Epoch 1367/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 10.7355 - val_loss: 7.3421\n",
      "Epoch 1368/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 6.9836 - val_loss: 7.3338\n",
      "Epoch 1369/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 15.6151 - val_loss: 7.3299\n",
      "Epoch 1370/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 11.4792 - val_loss: 7.3288\n",
      "Epoch 1371/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 14.2234 - val_loss: 7.3291\n",
      "Epoch 1372/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 11.0679 - val_loss: 7.3218\n",
      "Epoch 1373/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 6.5569 - val_loss: 7.3112\n",
      "Epoch 1374/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 13.2654 - val_loss: 7.3028\n",
      "Epoch 1375/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 9.7194 - val_loss: 7.2833\n",
      "Epoch 1376/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 12.6914 - val_loss: 7.2574\n",
      "Epoch 1377/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 7.4381 - val_loss: 7.2341\n",
      "Epoch 1378/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 9.7512 - val_loss: 7.2081\n",
      "Epoch 1379/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 13.3014 - val_loss: 7.1814\n",
      "Epoch 1380/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 10.3489 - val_loss: 7.1600\n",
      "Epoch 1381/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 7.5193 - val_loss: 7.1348\n",
      "Epoch 1382/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 8.0069 - val_loss: 7.1127\n",
      "Epoch 1383/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 10.2031 - val_loss: 7.0899\n",
      "Epoch 1384/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 12.7316 - val_loss: 7.0661\n",
      "Epoch 1385/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 8.1516 - val_loss: 7.0399\n",
      "Epoch 1386/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 9.3272 - val_loss: 7.0116\n",
      "Epoch 1387/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 8.3903 - val_loss: 6.9917\n",
      "Epoch 1388/1500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 13.4105 - val_loss: 6.9587\n",
      "Epoch 1389/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 12.6919 - val_loss: 6.9275\n",
      "Epoch 1390/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 8.0767 - val_loss: 6.8959\n",
      "Epoch 1391/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 13.0442 - val_loss: 6.8583\n",
      "Epoch 1392/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 11.8554 - val_loss: 6.8229\n",
      "Epoch 1393/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 15.7030 - val_loss: 6.7893\n",
      "Epoch 1394/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 10.2938 - val_loss: 6.7540\n",
      "Epoch 1395/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 11.9103 - val_loss: 6.7213\n",
      "Epoch 1396/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 13.0005 - val_loss: 6.6922\n",
      "Epoch 1397/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 13.3659 - val_loss: 6.6690\n",
      "Epoch 1398/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 8.4053 - val_loss: 6.6489\n",
      "Epoch 1399/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 9.2858 - val_loss: 6.6299\n",
      "Epoch 1400/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 10.1435 - val_loss: 6.6165\n",
      "Epoch 1401/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 11.5379 - val_loss: 6.5995\n",
      "Epoch 1402/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 11.6635 - val_loss: 6.5827\n",
      "Epoch 1403/1500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 9.7731 - val_loss: 6.5631\n",
      "Epoch 1404/1500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 7.2987 - val_loss: 6.5418\n",
      "Epoch 1405/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 10.3581 - val_loss: 6.5185\n",
      "Epoch 1406/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 9.5115 - val_loss: 6.4967\n",
      "Epoch 1407/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 8.5900 - val_loss: 6.4738\n",
      "Epoch 1408/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 9.4424 - val_loss: 6.4535\n",
      "Epoch 1409/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 15.1249 - val_loss: 6.4384\n",
      "Epoch 1410/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 9.8373 - val_loss: 6.4296\n",
      "Epoch 1411/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 12.1647 - val_loss: 6.4342\n",
      "Epoch 1412/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 7.1560 - val_loss: 6.4382\n",
      "Epoch 1413/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 12.7863 - val_loss: 6.4422\n",
      "Epoch 1414/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 15.6343 - val_loss: 6.4517\n",
      "Epoch 1415/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 9.8858 - val_loss: 6.4569\n",
      "Epoch 1416/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 7.4634 - val_loss: 6.4512\n",
      "Epoch 1417/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 6.0410 - val_loss: 6.4528\n",
      "Epoch 1418/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 8.3802 - val_loss: 6.4455\n",
      "Epoch 1419/1500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 12.0343 - val_loss: 6.4463\n",
      "Epoch 1420/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 12.9982 - val_loss: 6.4382\n",
      "Epoch 1421/1500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 12.0040 - val_loss: 6.4276\n",
      "Epoch 1422/1500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 16.0721 - val_loss: 6.4135\n",
      "Epoch 1423/1500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 9.9468 - val_loss: 6.4239\n",
      "Epoch 1424/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 12.0033 - val_loss: 6.4362\n",
      "Epoch 1425/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 10.8322 - val_loss: 6.4538\n",
      "Epoch 1426/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 12.2705 - val_loss: 6.4741\n",
      "Epoch 1427/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 7.7920 - val_loss: 6.4942\n",
      "Epoch 1428/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 14.5585 - val_loss: 6.4949\n",
      "Epoch 1429/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 10.6135 - val_loss: 6.4895\n",
      "Epoch 1430/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 7.7276 - val_loss: 6.4802\n",
      "Epoch 1431/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 9.2483 - val_loss: 6.4641\n",
      "Epoch 1432/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 13.9539 - val_loss: 6.4426\n",
      "Epoch 1433/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 9.0544 - val_loss: 6.4125\n",
      "Epoch 1434/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 9.7828 - val_loss: 6.3935\n",
      "Epoch 1435/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 9.9799 - val_loss: 6.3827\n",
      "Epoch 1436/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 10.4658 - val_loss: 6.3677\n",
      "Epoch 1437/1500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 6.2443 - val_loss: 6.3432\n",
      "Epoch 1438/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 14.7440 - val_loss: 6.3072\n",
      "Epoch 1439/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 13.3363 - val_loss: 6.2823\n",
      "Epoch 1440/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 9.3224 - val_loss: 6.2547\n",
      "Epoch 1441/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 6.0637 - val_loss: 6.2268\n",
      "Epoch 1442/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 16.1024 - val_loss: 6.2003\n",
      "Epoch 1443/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 14.7921 - val_loss: 6.1733\n",
      "Epoch 1444/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 11.1516 - val_loss: 6.1397\n",
      "Epoch 1445/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 9.6546 - val_loss: 6.1176\n",
      "Epoch 1446/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 13.4009 - val_loss: 6.0917\n",
      "Epoch 1447/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 7.8033 - val_loss: 6.0653\n",
      "Epoch 1448/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 11.1221 - val_loss: 6.0364\n",
      "Epoch 1449/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 12.5890 - val_loss: 6.0114\n",
      "Epoch 1450/1500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 5.9408 - val_loss: 5.9871\n",
      "Epoch 1451/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 10.5335 - val_loss: 5.9611\n",
      "Epoch 1452/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 10.7155 - val_loss: 5.9344\n",
      "Epoch 1453/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 11.1132 - val_loss: 5.8971\n",
      "Epoch 1454/1500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 6.9432 - val_loss: 5.8692\n",
      "Epoch 1455/1500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 7.8222 - val_loss: 5.8379\n",
      "Epoch 1456/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 11.6291 - val_loss: 5.8128\n",
      "Epoch 1457/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 12.2532 - val_loss: 5.7869\n",
      "Epoch 1458/1500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 9.4269 - val_loss: 5.7686\n",
      "Epoch 1459/1500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 8.9685 - val_loss: 5.7545\n",
      "Epoch 1460/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 9.7466 - val_loss: 5.7426\n",
      "Epoch 1461/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 12.7389 - val_loss: 5.7277\n",
      "Epoch 1462/1500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 8.3589 - val_loss: 5.7130\n",
      "Epoch 1463/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 9.7928 - val_loss: 5.7032\n",
      "Epoch 1464/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 8.8382 - val_loss: 5.6922\n",
      "Epoch 1465/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 11.5562 - val_loss: 5.6769\n",
      "Epoch 1466/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 12.6876 - val_loss: 5.6583\n",
      "Epoch 1467/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 5.9341 - val_loss: 5.6425\n",
      "Epoch 1468/1500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 10.6522 - val_loss: 5.6288\n",
      "Epoch 1469/1500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 7.6198 - val_loss: 5.6077\n",
      "Epoch 1470/1500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 8.0336 - val_loss: 5.5934\n",
      "Epoch 1471/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 7.6478 - val_loss: 5.5839\n",
      "Epoch 1472/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 8.8202 - val_loss: 5.5816\n",
      "Epoch 1473/1500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 8.5109 - val_loss: 5.5789\n",
      "Epoch 1474/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 8.4831 - val_loss: 5.5758\n",
      "Epoch 1475/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 11.6075 - val_loss: 5.5705\n",
      "Epoch 1476/1500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 7.8762 - val_loss: 5.5641\n",
      "Epoch 1477/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 10.9110 - val_loss: 5.5631\n",
      "Epoch 1478/1500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 13.4590 - val_loss: 5.5607\n",
      "Epoch 1479/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 14.5585 - val_loss: 5.5574\n",
      "Epoch 1480/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 8.4910 - val_loss: 5.5585\n",
      "Epoch 1481/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 7.6225 - val_loss: 5.5587\n",
      "Epoch 1482/1500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 12.8262 - val_loss: 5.5597\n",
      "Epoch 1483/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 10.0293 - val_loss: 5.5643\n",
      "Epoch 1484/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 11.0635 - val_loss: 5.5716\n",
      "Epoch 1485/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 11.3599 - val_loss: 5.5737\n",
      "Epoch 1486/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 9.9060 - val_loss: 5.5765\n",
      "Epoch 1487/1500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 7.1195 - val_loss: 5.5707\n",
      "Epoch 1488/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 9.9770 - val_loss: 5.5639\n",
      "Epoch 1489/1500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 9.5745 - val_loss: 5.5514\n",
      "Epoch 1490/1500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 12.4431 - val_loss: 5.5383\n",
      "Epoch 1491/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 6.2051 - val_loss: 5.5211\n",
      "Epoch 1492/1500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 14.7156 - val_loss: 5.5032\n",
      "Epoch 1493/1500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 8.6885 - val_loss: 5.4932\n",
      "Epoch 1494/1500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 14.5608 - val_loss: 5.4871\n",
      "Epoch 1495/1500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 11.2244 - val_loss: 5.4789\n",
      "Epoch 1496/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 12.9082 - val_loss: 5.4701\n",
      "Epoch 1497/1500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 10.9138 - val_loss: 5.4669\n",
      "Epoch 1498/1500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 8.5528 - val_loss: 5.4645\n",
      "Epoch 1499/1500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 10.9350 - val_loss: 5.4635\n",
      "Epoch 1500/1500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 11.5160 - val_loss: 5.4643\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x, y, validation_data=(vx, vy), epochs=epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "801bd3dc-0059-4293-8df8-9d484a3674b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "36e4f1bf-33ed-4b16-ab55-578509abe649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20, 66, 42, 29, 48, 48, 32, 56, 95, 16, 36, 26, 41, 90, 87,  1,\n",
       "        12, 57, 37, 88],\n",
       "       [63, 23, 22, 24, 89, 67, 76, 99, 61, 66, 55, 93, 77, 88, 99, 74,\n",
       "        35, 40, 72, 81],\n",
       "       [45,  4, 46, 33, 74, 66, 87, 85, 42, 60, 77, 70, 63, 63, 51,  9,\n",
       "        63, 15, 41, 41],\n",
       "       [68, 55, 11,  8, 77, 73, 30, 62, 80, 51,  8,  7, 29, 88, 54,  7,\n",
       "        97, 46, 37, 66],\n",
       "       [13, 16, 27, 92, 56, 65, 42, 99, 81, 84, 83, 41, 17, 52, 33, 43,\n",
       "        83, 68, 94, 23]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4e14f148-25cd-4a3b-b305-422abb8bad95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([95, 99, 87, 97, 99])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a6f27fc4-f665-4220-8d04-f6cf2be256d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[90.72583],\n",
       "       [90.73942],\n",
       "       [90.72425],\n",
       "       [90.72194],\n",
       "       [90.73026]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.predict(x[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0a541298-b88c-43d1-851c-fa58d2234a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eb4da48a-1c51-4824-b6e6-8005f1caf21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = gen_data(batch_size=30, length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "18ba42bf-0ac7-4829-b4d8-3e36c4c4dfe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([82, 97, 88, 88, 96])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7e8d41df-9e59-42e1-a600-8bc4679f176f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[90.72172],\n",
       "       [90.75025],\n",
       "       [90.72575],\n",
       "       [90.72146],\n",
       "       [90.72817]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.predict(x[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b7506-93df-4162-8438-e728c2f30443",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
