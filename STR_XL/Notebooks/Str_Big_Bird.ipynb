{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "477b0698-1d5c-4d85-b5a5-0c4cc9766421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "665026fc-9622-4dc3-972e-4e8c3de33e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5a20b11-1a27-4f59-8054-f4686a12e6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../../../deep-learning-dna\")\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "487d6841-517e-4726-aefd-ba8e7c6d33fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4198c41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "import math\n",
    "import string\n",
    "\n",
    "from Attention import BigBird, Set_Transformer\n",
    "from common.models import dnabert\n",
    "from common import dna\n",
    "from lmdbm import Lmdb\n",
    "from common.data import DnaSequenceGenerator, DnaLabelType, DnaSampleGenerator, find_dbs\n",
    "import wandb\n",
    "\n",
    "import tf_utils as tfu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dae2ca0-69c8-442c-8bee-da802a6b26bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tfu.devices.select_gpu(0, use_dynamic_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4300b2-d429-48f4-85ed-68772465def6",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18d6bf42-0914-487c-922f-b54f2712adee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<common.models.dnabert.DnaBertPretrainModel at 0x7f022e407d00>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import pretrained model\n",
    "api = wandb.Api()\n",
    "model_path = api.artifact(\"sirdavidludwig/dnabert-pretrain/dnabert-pretrain-8dim:latest\").download()\n",
    "pretrained_model = dnabert.DnaBertModel.load(model_path)\n",
    "pretrained_model.load_weights(model_path + \"/model.h5\")\n",
    "pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc98b4a9-90f8-442f-8f7e-7633b6f117cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact dnasamples-complete:latest, 4079.09MB. 420 files... Done. 0:0:0.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/data/dna_samples:v1/train/WS-CCW-Jul2015_S82_L001_R1_001.db'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load datafiles\n",
    "dataset_path = api.artifact(\"sirdavidludwig/nachusa-dna/dnasamples-complete:latest\").download('/data/dna_samples:v1')\n",
    "samples = find_dbs(dataset_path + '/train')\n",
    "samples[13]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ae0979-85aa-46e8-9cc4-61380710728e",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a14bf26f-f2d8-4907-991e-7dcbbc68f206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Generate batches\n",
    "split_ratios = [0.8, 0.2]\n",
    "subsample_length = 1000\n",
    "sequence_length = 150\n",
    "kmer = 3\n",
    "batch_size = [20,5]\n",
    "batches_per_epoch = 20\n",
    "augument = True\n",
    "labels = DnaLabelType.SampleIds\n",
    "seed = 0\n",
    "rng = np.random.default_rng(seed)\n",
    "random_samples = samples.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "613ec15a-94ea-40f6-83c4-593956d79975",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng.shuffle(random_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3bfd656b-2de3-4a36-a0d6-9be8c1492d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_samples, (train_dataset, val_dataset) = DnaSampleGenerator.split(samples=random_samples[0:10], split_ratios=split_ratios, subsample_length=subsample_length, sequence_length=sequence_length,kmer=kmer,batch_size=batch_size,batches_per_epoch=batches_per_epoch,augment=augument,labels=labels, rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81aba5f7-0fd3-4017-8eeb-cd955c99b7d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/data/dna_samples:v1/train/WS-CCE-Apr2016_S6_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/Wes52-10-TC_S53_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/WS-WH-Jul2016_S46_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/Wes41-10-HN_S42_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/Wesley026-Ag-072820_S165_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/WS-MU-Apr2016_S84_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/Wes5-5-CCE_S6_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/WS-MR-Apr2016_S13_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/WS-MU-Sep2015_S43_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/Wesley012-HN-051120_S151_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/WS-HPN-Sep2015_S91_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/WS-TCR-Sep2015_S52_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/Wes26-8-AG_S27_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/WS-SB-Jul2016_S22_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/Wes25-8-MU_S26_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/WS-SB-Oct2016_S63_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/Wes3-5-TCR_S4_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/WS-WH-May2015_S160_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/WS-TCR-Oct2016_S80_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/WS-TC-Jul2015_S74_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/WS-SB-Sep2015_S35_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/WS-CCW-Sep2015_S28_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/WS-TC-Oct2016_S48_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/WS-HF-Jul2015_S42_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/WS-TCR-May2015_S73_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/WS-HPN-Apr2016_S37_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/WS-AG-Apr2016_S85_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/Wes7-PCRblank1_S8_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/Wes20-8-HF_S21_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/Wes38-10-WH_S39_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/Wesley047-SF-100420_S186_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/Wesley055-HAP-051120_S194_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/Wesley006-HLP-051220_S145_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/Wes39-10-HF_S40_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/WS-SF-Jul2016_S38_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/Wesley035-HLP2-072820_S174_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/Wes13-5-HLP_S14_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/WS-HPN-May2015_S17_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/Wesley005-HF-051220_S144_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/Wes47-10-FC_S48_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/WS-SF-May2015_S152_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/Wesley045-HLP-100420_S184_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/Wesley033-MU-072820_S172_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/Wesley043-L-100420_S182_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/Wes48-10-CCW_S49_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/WS-AG-Sep2015_S44_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/Wesley010-HW-051120_S149_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/Wes36-8-HW_S37_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/Wesley011-Ag-051120_S150_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/Wesley027-HLP-072820_S166_L001_R1_001.db']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_samples[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be7e12c7-d792-4e37-9ab2-3db8658b5814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_files = len(train_dataset.samples)\n",
    "max_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae445919-6300-44ff-b3c2-fd2d9828335b",
   "metadata": {},
   "source": [
    "---\n",
    "# Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a5928c7f-92ab-4952-ab8b-477e0e457959",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create 8 dimensional embeddings\n",
    "pretrained_encoder= dnabert.DnaBertEncoderModel(pretrained_model.base)\n",
    "pretrained_encoder.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e6fc1685-c320-40f5-bb4a-1c217be25963",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Create_Embeddings(keras.layers.Layer):\n",
    "    def __init__(self, encoder):\n",
    "        super(Create_Embeddings, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        \n",
    "    \n",
    "    def subbatch_predict(self, model, batch, subbatch_size, concat=lambda old, new: tf.concat((old, new), axis=0)):\n",
    "        def predict(i, result=None):\n",
    "            n = i + subbatch_size\n",
    "            pred = tf.stop_gradient(model(batch[i:n]))\n",
    "            if result is None:\n",
    "                return [n, pred]\n",
    "            return [n, concat(result, pred)]\n",
    "        i, result = predict(0)\n",
    "        batch_size = tf.shape(batch)[0]\n",
    "        i, result = tf.while_loop(\n",
    "            cond=lambda i, _: i < batch_size,\n",
    "            body=predict,\n",
    "            loop_vars=[i, result],\n",
    "            parallel_iterations=1)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def modify_data_for_input(self, data):\n",
    "        batch_size = tf.shape(data)[0]\n",
    "        subsample_size = tf.shape(data)[1]\n",
    "        flat_data = tf.reshape(data, (batch_size*subsample_size, -1))\n",
    "        encoded = self.subbatch_predict(self.encoder, flat_data, 128)\n",
    "        return tf.reshape(encoded, (batch_size, subsample_size, -1))\n",
    "    \n",
    "    def call(self, data):\n",
    "        return  self.modify_data_for_input(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e02e62d-f7b7-4a0a-abf8-d17d58206b7f",
   "metadata": {},
   "source": [
    "---\n",
    "# Create Big Bird Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a399d0bf-05c4-4c6b-9972-901d598bd631",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Create_BigBird_Masks(keras.layers.Layer):\n",
    "    def __init__(self, attention_block_size):\n",
    "        super(Create_BigBird_Masks, self).__init__()\n",
    "            \n",
    "        self.mask_layer = BigBird.BigBirdMasks(block_size=attention_block_size)\n",
    "        \n",
    "    def call(self, one_batch):\n",
    "\n",
    "        mask = tf.ones(tf.shape(one_batch)[:-1])\n",
    "                       \n",
    "        masks = self.mask_layer(one_batch, mask)      \n",
    "        \n",
    "        return masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8173ae86-2b0a-49f9-8f2a-ab3c83158c4b",
   "metadata": {},
   "source": [
    "---\n",
    "# Big Bird Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5a227454-6052-4ebf-ac17-bc44dcb6263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Big_Bird_Attention(keras.layers.Layer):\n",
    "    def __init__(self, dropout, inner_size, num_heads, key_dim, num_rand_blocks,from_block_size,to_block_size,max_rand_mask_length):\n",
    "        super(Big_Bird_Attention, self).__init__()\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.inner_size = inner_size\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.num_rand_blocks = num_rand_blocks\n",
    "        self.from_block_size = from_block_size\n",
    "        self.to_block_size = to_block_size\n",
    "        self.max_rand_mask_length = max_rand_mask_length\n",
    "        \n",
    "        self.attention_layer = BigBird.BigBirdAttention(num_heads=self.num_heads, key_dim=self.key_dim, num_rand_blocks=self.num_rand_blocks,from_block_size=self.from_block_size,to_block_size=self.to_block_size,max_rand_mask_length=self.max_rand_mask_length)\n",
    "        self.attention_dropout = tf.keras.layers.Dropout(rate=self.dropout)\n",
    "        self.attention_layer_norm = tf.keras.layers.LayerNormalization(axis=-1, epsilon=1e-12, dtype=tf.float32)\n",
    "        self.inner_dense = tf.keras.layers.experimental.EinsumDense(\"abc,cd->abd\", output_shape=(None, self.inner_size), bias_axes=\"d\", kernel_initializer=keras.initializers.RandomNormal(stddev=0.1))\n",
    "        self.inner_activation_layer = tf.keras.layers.Activation(\"relu\")\n",
    "        self.inner_dropout_layer = tf.keras.layers.Dropout(rate=self.dropout)\n",
    "        self.output_dense = tf.keras.layers.experimental.EinsumDense(\"abc,cd->abd\", output_shape=(None, inner_size), bias_axes=\"d\", kernel_initializer=keras.initializers.RandomNormal(stddev=0.1))\n",
    "        self.output_dropout = tf.keras.layers.Dropout(rate=self.dropout)\n",
    "        self.output_layer_norm = tf.keras.layers.LayerNormalization(axis=-1, epsilon=1e-12)\n",
    "        \n",
    "    def call(self, content_stream, mask):\n",
    "\n",
    "        attention_output = self.attention_layer(content_stream, content_stream, content_stream, mask)\n",
    "\n",
    "        attention_stream = attention_output\n",
    "        input_stream = content_stream\n",
    "\n",
    "        attention_stream = self.attention_dropout(attention_stream)\n",
    "        attention_stream = self.attention_layer_norm(attention_stream + input_stream)\n",
    "        inner_output = self.inner_dense(attention_stream)\n",
    "        inner_output = self.inner_activation_layer(inner_output)\n",
    "        inner_output = self.inner_dropout_layer(inner_output)\n",
    "        layer_output = self.output_dense(inner_output)\n",
    "        layer_output = self.output_dropout(layer_output)\n",
    "        layer_output = self.output_layer_norm(layer_output + attention_stream)\n",
    "        attention_output = layer_output\n",
    "        \n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa47f6fd-2a44-46f8-8b46-15fc33786e13",
   "metadata": {},
   "source": [
    "---\n",
    "# Set Transformer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f11e396d-98b4-47ce-903a-c342ec73e234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Set_Big_Bird_Model(keras.Model):\n",
    "    def __init__(self, embed_dim, stack, encoder, max_files, num_seeds, pooling_num_heads, attention_block_size, dropout, inner_size, attention_num_heads, key_dim, num_rand_blocks, from_block_size, to_block_size, max_rand_mask_length, use_layernorm, pre_layernorm, use_keras_mha):\n",
    "        super(Set_Big_Bird_Model, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.stack = stack\n",
    "        self.encoder = encoder\n",
    "        self.max_files = max_files\n",
    "        self.num_seeds = num_seeds \n",
    "        self.pooling_num_heads = pooling_num_heads   \n",
    "        \n",
    "        self.attention_block_size = attention_block_size\n",
    "        self.dropout = dropout\n",
    "        self.inner_size = inner_size\n",
    "        self.attention_num_heads = attention_num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.num_rand_blocks = num_rand_blocks\n",
    "        self.from_block_size = from_block_size\n",
    "        self.to_block_size = to_block_size\n",
    "        self.max_rand_mask_length = max_rand_mask_length\n",
    "        \n",
    "        self.use_layernorm = use_layernorm\n",
    "        self.pre_layernorm = pre_layernorm\n",
    "        self.use_keras_mha = use_keras_mha\n",
    "        \n",
    "        self.embedding_layer = Create_Embeddings(self.encoder)\n",
    "        self.linear_layer = keras.layers.Dense(self.embed_dim)\n",
    "        self.attention_blocks = []\n",
    "        \n",
    "        self.mask_layer = Create_BigBird_Masks(attention_block_size)\n",
    "        \n",
    "        self.attention_layer = Big_Bird_Attention(self.dropout, self.inner_size, self.attention_num_heads, self.key_dim, self.num_rand_blocks, self.from_block_size, self.to_block_size, self.max_rand_mask_length)\n",
    "        \n",
    "        for i in range(self.stack):\n",
    "            self.attention_blocks.append(self.attention_layer)\n",
    "                \n",
    "        self.pooling_layer = Set_Transformer.PoolingByMultiHeadAttention(num_seeds=self.num_seeds,embed_dim=self.embed_dim,num_heads=self.pooling_num_heads,use_layernorm=self.use_layernorm,pre_layernorm=self.pre_layernorm,use_keras_mha=self.use_keras_mha,is_final_block=True)\n",
    "    \n",
    "        self.reshape_layer = keras.layers.Reshape((self.embed_dim,))\n",
    "        \n",
    "        self.output_layer = keras.layers.Dense(self.max_files)\n",
    "    \n",
    "    def call(self, data):\n",
    "        \n",
    "            embeddings = self.embedding_layer(data)\n",
    "            \n",
    "            linear_transform = self.linear_layer(embeddings)\n",
    "            \n",
    "            mask = self.mask_layer(linear_transform)\n",
    "            \n",
    "            attention = linear_transform\n",
    "            \n",
    "            for attention_block in self.attention_blocks:\n",
    "                attention = attention_block(attention, mask)\n",
    "                \n",
    "            pooling = self.pooling_layer(attention)\n",
    "        \n",
    "            reshape = self.reshape_layer(pooling)\n",
    "            \n",
    "            output = self.output_layer(reshape)    \n",
    "            \n",
    "            return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1c8632-97aa-46b0-9af1-afee2c87613f",
   "metadata": {},
   "source": [
    "---\n",
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "15b79d89-3c83-41f0-954f-86419d612032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "embed_dim = 64\n",
    "stack = 4\n",
    "encoder = pretrained_encoder\n",
    "num_seeds = 1\n",
    "pooling_num_heads = 1\n",
    "attention_block_size = 2\n",
    "dropout = 0.01\n",
    "inner_size = 32\n",
    "attention_num_heads = 2\n",
    "key_dim = 32\n",
    "num_rand_blocks = 2\n",
    "from_block_size = 2\n",
    "to_block_size = 2\n",
    "max_rand_mask_length = subsample_length\n",
    "use_layernorm = True\n",
    "pre_layernorm = True\n",
    "use_keras_mha = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d174e7c1-fd3c-466b-a211-79555706313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Parameters = dict(\n",
    "    embed_dim = 32,\n",
    "    stack = 4,\n",
    "    num_seeds = 1,\n",
    "    pooling_num_heads = 1,\n",
    "    attention_block_size = 10,\n",
    "    dropout = 0.01,\n",
    "    inner_size = 32,\n",
    "    attention_num_heads = 10,\n",
    "    key_dim = 32,\n",
    "    num_rand_blocks = 5,\n",
    "    from_block_size = 10,\n",
    "    to_block_size = 10,\n",
    "    max_rand_mask_length = max_files,\n",
    "    use_layernorm = True,\n",
    "    pre_layernorm = True,\n",
    "    use_keras_mha = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ef28007c-fa3c-4608-befb-9e6b0a4f0d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:x41gglae) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">peach-pine-18</strong>: <a href=\"https://wandb.ai/kendragivens/Set_Big_Bird/runs/x41gglae\" target=\"_blank\">https://wandb.ai/kendragivens/Set_Big_Bird/runs/x41gglae</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220807_122052-x41gglae/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:x41gglae). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/work/TransformerXL/STR_XL/Notebooks/wandb/run-20220807_122107-2der85kw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kendragivens/Set_Big_Bird/runs/2der85kw\" target=\"_blank\">drawn-puddle-19</a></strong> to <a href=\"https://wandb.ai/kendragivens/Set_Big_Bird\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"Set_Big_Bird\", config=Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "11a8fa23-80d6-47e7-9f5d-39e6d55c6816",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Set_Big_Bird_Model(embed_dim, stack, encoder, max_files, num_seeds, pooling_num_heads, attention_block_size, dropout, inner_size, attention_num_heads, key_dim, num_rand_blocks, from_block_size, to_block_size, max_rand_mask_length, use_layernorm, pre_layernorm, use_keras_mha)\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-3),loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics = [keras.metrics.sparse_categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e1b78c15-5643-4ce8-88df-fb53bf302dba",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer \"big__bird__attention_7\" (type Big_Bird_Attention).\n\nrequired broadcastable shapes [Op:AddV2]\n\nCall arguments received:\n  • content_stream=tf.Tensor(shape=(20, 1000, 64), dtype=float32)\n  • mask=['tf.Tensor(shape=(20, 1, 496, 2, 6), dtype=float32)', 'tf.Tensor(shape=(20, 1, 1000, 1), dtype=float32)', 'tf.Tensor(shape=(20, 1, 1, 1000), dtype=float32)', 'tf.Tensor(shape=(20, 500, 2), dtype=float32)']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[0;32mIn [84]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Input \u001b[0;32mIn [75]\u001b[0m, in \u001b[0;36mSet_Big_Bird_Model.call\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     51\u001b[0m attention \u001b[38;5;241m=\u001b[39m linear_transform\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attention_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_blocks:\n\u001b[0;32m---> 54\u001b[0m     attention \u001b[38;5;241m=\u001b[39m \u001b[43mattention_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m pooling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling_layer(attention)\n\u001b[1;32m     58\u001b[0m reshape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreshape_layer(pooling)\n",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36mBig_Bird_Attention.call\u001b[0;34m(self, content_stream, mask)\u001b[0m\n\u001b[1;32m     36\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dense(inner_output)\n\u001b[1;32m     37\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dropout(layer_output)\n\u001b[0;32m---> 38\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer_norm(\u001b[43mlayer_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_stream\u001b[49m)\n\u001b[1;32m     39\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m layer_output\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attention_output\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer \"big__bird__attention_7\" (type Big_Bird_Attention).\n\nrequired broadcastable shapes [Op:AddV2]\n\nCall arguments received:\n  • content_stream=tf.Tensor(shape=(20, 1000, 64), dtype=float32)\n  • mask=['tf.Tensor(shape=(20, 1, 496, 2, 6), dtype=float32)', 'tf.Tensor(shape=(20, 1, 1000, 1), dtype=float32)', 'tf.Tensor(shape=(20, 1, 1, 1000), dtype=float32)', 'tf.Tensor(shape=(20, 500, 2), dtype=float32)']"
     ]
    }
   ],
   "source": [
    "model(train_dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "51fbd3f6-ad09-4e86-a5e4-27703b9e21eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c7dbf382",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer \"big__bird__attention_7\" (type Big_Bird_Attention).\n\nrequired broadcastable shapes [Op:AddV2]\n\nCall arguments received:\n  • content_stream=tf.Tensor(shape=(20, 1000, 64), dtype=float32)\n  • mask=['tf.Tensor(shape=(20, 1, 496, 2, 6), dtype=float32)', 'tf.Tensor(shape=(20, 1, 1000, 1), dtype=float32)', 'tf.Tensor(shape=(20, 1, 1, 1000), dtype=float32)', 'tf.Tensor(shape=(20, 500, 2), dtype=float32)']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[0;32mIn [81]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWandbCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_weights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/wandb/integration/keras/keras.py:173\u001b[0m, in \u001b[0;36mpatch_tf_keras.<locals>.new_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cbk \u001b[38;5;129;01min\u001b[39;00m cbks:\n\u001b[1;32m    172\u001b[0m         set_wandb_attrs(cbk, val_data)\n\u001b[0;32m--> 173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mold_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Input \u001b[0;32mIn [75]\u001b[0m, in \u001b[0;36mSet_Big_Bird_Model.call\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     51\u001b[0m attention \u001b[38;5;241m=\u001b[39m linear_transform\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attention_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_blocks:\n\u001b[0;32m---> 54\u001b[0m     attention \u001b[38;5;241m=\u001b[39m \u001b[43mattention_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m pooling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling_layer(attention)\n\u001b[1;32m     58\u001b[0m reshape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreshape_layer(pooling)\n",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36mBig_Bird_Attention.call\u001b[0;34m(self, content_stream, mask)\u001b[0m\n\u001b[1;32m     36\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dense(inner_output)\n\u001b[1;32m     37\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dropout(layer_output)\n\u001b[0;32m---> 38\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer_norm(\u001b[43mlayer_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_stream\u001b[49m)\n\u001b[1;32m     39\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m layer_output\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attention_output\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer \"big__bird__attention_7\" (type Big_Bird_Attention).\n\nrequired broadcastable shapes [Op:AddV2]\n\nCall arguments received:\n  • content_stream=tf.Tensor(shape=(20, 1000, 64), dtype=float32)\n  • mask=['tf.Tensor(shape=(20, 1, 496, 2, 6), dtype=float32)', 'tf.Tensor(shape=(20, 1, 1000, 1), dtype=float32)', 'tf.Tensor(shape=(20, 1, 1, 1000), dtype=float32)', 'tf.Tensor(shape=(20, 500, 2), dtype=float32)']"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=train_dataset, validation_data=val_dataset, epochs=epochs, verbose=1, callbacks=[wandb.keras.WandbCallback(save_weights_only=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547d0269-2b98-46af-8084-742c2e254389",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
