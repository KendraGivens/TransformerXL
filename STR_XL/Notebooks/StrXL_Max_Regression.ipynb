{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad2c14f8-8dd8-459f-81fe-a6708e6a8355",
   "metadata": {},
   "source": [
    "---\n",
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dcad43d-67f7-49b9-84ce-6d91a0f468a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e7cb9df-5746-44a3-998a-b671b284957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c507a85-fe6f-4d3c-ae1f-16914fdec693",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../../../deep-learning-dna\")\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../../deep-learning-dna/common\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31b9c98a-b4ee-4667-89a3-5f04549f7d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fda64db1-8058-4e15-b2d4-1a3e64f23fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import math\n",
    "import string\n",
    "\n",
    "from Attention import Set_Transformer\n",
    "from common.models import dnabert\n",
    "from common import dna\n",
    "from lmdbm import Lmdb\n",
    "from common.data import DnaSequenceGenerator, DnaLabelType, DnaSampleGenerator, find_dbs\n",
    "import wandb\n",
    "\n",
    "import tf_utils as tfu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dfb3b1a-2db7-4463-a208-b5d29e20f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tfu.devices.select_gpu(0, use_dynamic_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54593a0f-b5b4-401e-91e2-8c1a1598432e",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65e83dc7-9349-4aab-b240-1e0d39988513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(batch_size, length=5):\n",
    "    x = np.random.randint(1, 100, (batch_size, length))\n",
    "    y = np.max(x, axis=1)\n",
    "    return x, y # (batch_size, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f47ff082-2d44-4467-872b-c925150812b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = gen_data(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df9ded01-96fb-4df2-b52f-429f623ab30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5) (3,)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0adef207-d041-409c-9f7b-f57799812e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[91, 88, 22, 47, 97],\n",
       "       [32, 58, 72, 21, 15],\n",
       "       [44, 58,  1, 20, 86]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df941e9f-16d1-4d9d-a6ec-bec7469d2fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([97, 72, 86])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835c39dd-9fb5-49bf-8763-14ca694a1a35",
   "metadata": {},
   "source": [
    "---\n",
    "# Cache Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da4cb5e4-2d26-4e40-a00f-3e71f470b273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cache_Memory(current_state, previous_state, memory_length):\n",
    "    \n",
    "    if memory_length is None or memory_length == 0:\n",
    "        return None\n",
    "    else:\n",
    "        if previous_state is None:\n",
    "            new_mem = current_state[:, -memory_length:, :]\n",
    "        else:\n",
    "            new_mem = tf.concat(\n",
    "                    [previous_state, current_state], 1)[:, -memory_length:, :]\n",
    "\n",
    "    return tf.stop_gradient(new_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb50453f-628c-412e-942b-d486d01c6b8a",
   "metadata": {},
   "source": [
    "---\n",
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bf9ef79-9ede-4271-8542-223cdd1e11ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(keras.Model):\n",
    "    def __init__(self, num_induce, embed_dim, num_heads, use_layernorm, pre_layernorm, use_keras_mha):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.num_induce = num_induce\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.use_layernorm = use_layernorm\n",
    "        self.pre_layernorm = pre_layernorm\n",
    "        self.use_keras_mha = use_keras_mha\n",
    "        \n",
    "        if self.num_induce == 0:       \n",
    "            self.attention = (Set_Transformer.SetAttentionBlock(embed_dim=self.embed_dim, num_heads=self.num_heads, use_layernorm=self.use_layernorm,pre_layernorm=self.pre_layernorm,use_keras_mha=self.use_keras_mha))\n",
    "        else:\n",
    "            self.attention = Set_Transformer.InducedSetAttentionBlock(embed_dim=self.embed_dim, num_heads=self.num_heads, num_induce=self.num_induce, use_layernorm=self.use_layernorm, pre_layernorm=self.pre_layernorm, use_keras_mha=self.use_keras_mha)\n",
    "    \n",
    "    \n",
    "    def call(self, data, mems):\n",
    "                \n",
    "            attention = self.attention([data, mems])\n",
    "                \n",
    "            return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c32bb5-4520-4802-a8f0-5f7849905a83",
   "metadata": {},
   "source": [
    "---\n",
    "# XL Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7245b7dd-a3b2-464a-9f5b-7efd4a9d0ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerXLBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 num_induce, \n",
    "                 embed_dim,\n",
    "                 num_heads,\n",
    "                 use_layernorm,\n",
    "                 pre_layernorm,\n",
    "                 use_keras_mha,):\n",
    "\n",
    "        super(TransformerXLBlock, self).__init__()\n",
    "        \n",
    "        self.num_induce = num_induce\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.use_layernorm = use_layernorm\n",
    "        self.pre_layernorm = pre_layernorm\n",
    "        self.use_keras_mha = use_keras_mha\n",
    "        \n",
    "        self.attention = Attention\n",
    "        \n",
    "        self.attention_layer = self.attention(self.num_induce, self.embed_dim, self.num_heads, self.use_layernorm, self.pre_layernorm, self.use_keras_mha)\n",
    "\n",
    "   \n",
    "    def call(self,\n",
    "             content_stream,\n",
    "             state=None):\n",
    "        \n",
    "        attention_output = self.attention_layer(content_stream, state)\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8749ec30-1281-4f9d-8baa-54b890fb7949",
   "metadata": {},
   "source": [
    "---\n",
    "# Transformer XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "019ca25d-e899-4e17-8f41-dfdc3d9bdee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerXL(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 mem_switched, \n",
    "                 num_layers,\n",
    "                 num_induce,\n",
    "                 embed_dim,\n",
    "                 num_heads,\n",
    "                 dropout_rate,\n",
    "                 mem_len=None,\n",
    "                 use_layernorm=True,\n",
    "                 pre_layernorm=True, \n",
    "                 use_keras_mha=True):\n",
    "        \n",
    "        super(TransformerXL, self).__init__()\n",
    "\n",
    "        self.mem_switched = mem_switched\n",
    "        self.num_layers = num_layers\n",
    "        self.num_induce = num_induce\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mem_len = mem_len\n",
    "        self.use_layernorm = use_layernorm\n",
    "        self.pre_layernorm = pre_layernorm\n",
    "        self.use_keras_mha = use_keras_mha\n",
    "\n",
    "        self.transformer_xl_layers = []\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            self.transformer_xl_layers.append(\n",
    "                    TransformerXLBlock(self.num_induce,\n",
    "                                        self.embed_dim,\n",
    "                                        self.num_heads,\n",
    "                                        self.use_layernorm,\n",
    "                                        self.pre_layernorm, \n",
    "                                        self.use_keras_mha))\n",
    "\n",
    "        self.output_dropout = tf.keras.layers.Dropout(rate=self.dropout_rate)\n",
    "\n",
    "    def call(self,\n",
    "             content_stream,\n",
    "             state=None):\n",
    "        \n",
    "        new_mems = []\n",
    "\n",
    "        if state is None:\n",
    "            state = [None] * self.num_layers\n",
    "            \n",
    "        for i in range(self.num_layers):\n",
    "            if self.mem_switched == False:\n",
    "                new_mems.append(Cache_Memory(content_stream, state[i], self.mem_len))\n",
    "            \n",
    "            transformer_xl_layer = self.transformer_xl_layers[i]\n",
    "            \n",
    "            transformer_xl_output = transformer_xl_layer(content_stream=content_stream,\n",
    "                                                        state=state[i])\n",
    "            \n",
    "            content_stream = self.output_dropout(transformer_xl_output)\n",
    "            \n",
    "            if self.mem_switched == True:\n",
    "                new_mems.append(Cache_Memory(content_stream, state[i], self.mem_len))\n",
    "\n",
    "        output_stream = content_stream\n",
    "        return output_stream, new_mems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a127a4b5-15e3-46e3-838b-a5e18e4d8dc8",
   "metadata": {},
   "source": [
    "---\n",
    "# Xl Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "855d43dd-e182-4c2a-961a-ff9781141f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XlModel(keras.Model):\n",
    "    def __init__(self, mem_switched, max_files, encoder, block_size, max_set_len, num_induce, embed_dim, num_layers, num_heads, mem_len, dropout_rate, num_seeds, use_layernorm, pre_layernorm, use_keras_mha):\n",
    "        super(XlModel, self).__init__()\n",
    "        \n",
    "        self.mem_switched = mem_switched\n",
    "        self.max_files = max_files\n",
    "        self.encoder = encoder\n",
    "        self.block_size = block_size\n",
    "        self.max_set_len = max_set_len\n",
    "        self.num_induce = num_induce\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.mem_len = mem_len\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_seeds = num_seeds\n",
    "        self.use_layernorm = use_layernorm\n",
    "        self.pre_layernorm = pre_layernorm\n",
    "        self.use_keras_mha = use_keras_mha\n",
    "        \n",
    "        self.linear_layer = keras.layers.Dense(self.embed_dim)\n",
    "        \n",
    "        self.transformer_xl = TransformerXL(self.mem_switched,\n",
    "                                            self.num_layers,\n",
    "                                             self.num_induce,\n",
    "                                             self.embed_dim,\n",
    "                                             self.num_heads,\n",
    "                                             self.dropout_rate,\n",
    "                                             self.mem_len,\n",
    "                                             self.use_layernorm,\n",
    "                                             self.pre_layernorm,\n",
    "                                             self.use_keras_mha)\n",
    "        \n",
    "\n",
    "        self.segment_pooling_layer = Set_Transformer.PoolingByMultiHeadAttention(num_seeds=self.num_seeds,embed_dim=self.embed_dim,num_heads=self.num_heads,use_layernorm=self.use_layernorm,pre_layernorm=self.pre_layernorm, use_keras_mha=self.use_keras_mha, is_final_block=True)\n",
    "       \n",
    "        self.output_pooling_layer = Set_Transformer.PoolingByMultiHeadAttention(num_seeds=self.num_seeds,embed_dim=self.embed_dim,num_heads=self.num_heads,use_layernorm=self.use_layernorm,pre_layernorm=self.pre_layernorm, use_keras_mha=self.use_keras_mha, is_final_block=True)\n",
    "    \n",
    "        self.dropout_layer = keras.layers.Dropout(.5)\n",
    "    \n",
    "        self.dense_layer = keras.layers.Dense(1) \n",
    "        \n",
    "        #Classification\n",
    "        #self.dense_layer = keras.layers.Dense(100) \n",
    "\n",
    "    def call(self, x, training=None):    \n",
    " \n",
    "        mems = tf.zeros((self.num_layers, tf.shape(x)[0], self.mem_len, self.embed_dim))\n",
    "    \n",
    "        x = tf.expand_dims(x, axis=2)\n",
    "        \n",
    "        linear_transform = self.linear_layer(x)\n",
    "\n",
    "        for i in range(0, self.max_set_len, self.block_size):\n",
    "\n",
    "            block = linear_transform[:,i:i+self.block_size]\n",
    "            \n",
    "            output, mems = self.transformer_xl(content_stream=block, state=mems)\n",
    "            \n",
    "            segment_pooling = self.segment_pooling_layer(output)\n",
    "\n",
    "        output_pooling = self.output_pooling_layer(segment_pooling)\n",
    "        \n",
    "        dropout = self.dropout_layer(output_pooling)\n",
    "\n",
    "        dense = self.dense_layer(dropout)\n",
    "\n",
    "        #output = tf.reshape(dense, tf.shape(dense)[:2])\n",
    "        \n",
    "        #Classification\n",
    "        output = tf.reshape(dense, tf.shape(dense)[::2])         \n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3469de1-250d-4c14-bb79-a4e74f0fe93e",
   "metadata": {},
   "source": [
    "---\n",
    "# Xl Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bc3ea7d-d4a1-4a1e-bf2a-7cd1c91430cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xl Parameters\n",
    "max_files = 0\n",
    "encoder = 0\n",
    "mem_switched = True\n",
    "num_induce = 0\n",
    "embed_dim = 64\n",
    "num_layers = 4\n",
    "num_heads = 8\n",
    "mem_len = 0\n",
    "dropout_rate = 0.01\n",
    "num_seeds = 1\n",
    "use_layernorm = True\n",
    "pre_layernorm = True\n",
    "use_keras_mha = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89649663-7d18-48bc-8678-ce5f093908cc",
   "metadata": {},
   "source": [
    "---\n",
    "# Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6709d90-e797-4f99-b863-119a9ee20cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 20\n",
    "length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75799d2a-1d30-4758-9927-61514be7b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = gen_data(batch_size=10, length=length)\n",
    "vx, vy = gen_data(batch_size=10, length=length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63636972-d74a-4840-b731-2b9bb46823d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_set_len = length\n",
    "set_len = length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "821b0c4a-6d05-4867-a713-474bfdbff881",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XlModel(mem_switched, max_files, encoder, block_size, max_set_len, num_induce, embed_dim, num_layers, num_heads, mem_len, dropout_rate, num_seeds, use_layernorm, pre_layernorm, use_keras_mha)\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-3),loss=keras.losses.MeanAbsoluteError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52ceb122-67f0-4634-8b82-2ea318621958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([97, 97, 97, 91, 99])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccbcafc4-e399-4b30-986e-9a04fc656545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.39295352],\n",
       "       [-0.47098017],\n",
       "       [-0.37351608],\n",
       "       [-0.40641773],\n",
       "       [-0.4280383 ]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ddcc096-cb9a-4c19-a838-0aa92a6a13bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a4e79fc-5cbf-4807-a782-ed7cbe4e4e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 97.3646 - val_loss: 89.4729\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 92.2015 - val_loss: 87.8514\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 89.9914 - val_loss: 87.1742\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 90.3208 - val_loss: 86.6127\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 88.8447 - val_loss: 86.1644\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 89.8347 - val_loss: 85.7099\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 89.1812 - val_loss: 85.2104\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 87.8810 - val_loss: 84.7157\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 87.7617 - val_loss: 84.3576\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 87.1385 - val_loss: 84.1147\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 86.8419 - val_loss: 83.8835\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 86.7171 - val_loss: 83.6219\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 87.8584 - val_loss: 83.3781\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 86.8756 - val_loss: 83.1362\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 85.1251 - val_loss: 82.9001\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 85.0645 - val_loss: 82.6688\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 86.0410 - val_loss: 82.5035\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 84.5067 - val_loss: 82.3830\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 86.8208 - val_loss: 82.2641\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 85.5196 - val_loss: 82.1583\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 84.7856 - val_loss: 82.0628\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 84.9788 - val_loss: 81.9845\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 85.2218 - val_loss: 81.9164\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 85.1909 - val_loss: 81.8403\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 84.0894 - val_loss: 81.7617\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 85.5576 - val_loss: 81.6929\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 84.6521 - val_loss: 81.7035\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 83.0463 - val_loss: 81.7214\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 84.9937 - val_loss: 81.6885\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 85.6957 - val_loss: 81.5762\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 84.6341 - val_loss: 81.4974\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 84.7429 - val_loss: 81.4351\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 83.8239 - val_loss: 81.3679\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 84.9108 - val_loss: 81.2797\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 84.6349 - val_loss: 81.1747\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 84.8472 - val_loss: 81.0774\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 83.1018 - val_loss: 80.9903\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 84.6254 - val_loss: 80.9252\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 83.9773 - val_loss: 80.8801\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 83.3731 - val_loss: 80.8355\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 83.7982 - val_loss: 80.7953\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 84.4209 - val_loss: 80.7458\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 83.3854 - val_loss: 80.6959\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 83.7286 - val_loss: 80.6373\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 83.3277 - val_loss: 80.5640\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 84.3803 - val_loss: 80.4974\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 84.0748 - val_loss: 80.4398\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 83.5340 - val_loss: 80.3899\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 82.8002 - val_loss: 80.3484\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 83.6697 - val_loss: 80.3148\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 83.0711 - val_loss: 80.2737\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 82.7373 - val_loss: 80.2160\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 83.5835 - val_loss: 80.1508\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 83.2160 - val_loss: 80.0827\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 82.7148 - val_loss: 80.0093\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 83.1365 - val_loss: 79.9367\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 82.7714 - val_loss: 79.8656\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 81.8491 - val_loss: 79.8029\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 83.6754 - val_loss: 79.7511\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 81.8142 - val_loss: 79.7085\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 82.8013 - val_loss: 79.6630\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 83.8143 - val_loss: 79.6126\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 81.8190 - val_loss: 79.5499\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 82.8521 - val_loss: 79.4855\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 81.9252 - val_loss: 79.4227\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 83.3831 - val_loss: 79.3581\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 83.2068 - val_loss: 79.2990\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 82.3782 - val_loss: 79.2476\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 82.4520 - val_loss: 79.1891\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 81.4171 - val_loss: 79.1283\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 81.5933 - val_loss: 79.0676\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 81.9891 - val_loss: 79.0151\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 81.6172 - val_loss: 78.9639\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 80.4328 - val_loss: 78.9108\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 83.0305 - val_loss: 78.8590\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 82.4192 - val_loss: 78.8033\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 81.8529 - val_loss: 78.7467\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 82.5855 - val_loss: 78.6875\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 81.8993 - val_loss: 78.6182\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 82.2300 - val_loss: 78.5556\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 81.7253 - val_loss: 78.4942\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 81.4823 - val_loss: 78.4419\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 82.2363 - val_loss: 78.3903\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 82.3287 - val_loss: 78.3390\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 80.4722 - val_loss: 78.2864\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 81.6088 - val_loss: 78.2334\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 81.2155 - val_loss: 78.1844\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 80.8382 - val_loss: 78.1354\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 81.2299 - val_loss: 78.0864\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 80.9129 - val_loss: 78.0316\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 81.3568 - val_loss: 77.9735\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 81.6719 - val_loss: 77.9075\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 81.0787 - val_loss: 77.8393\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 80.6001 - val_loss: 77.7759\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 81.8882 - val_loss: 77.7141\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 80.2666 - val_loss: 77.6519\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 81.3743 - val_loss: 77.5961\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 80.1681 - val_loss: 77.5388\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 79.5636 - val_loss: 77.4862\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 81.2977 - val_loss: 77.4348\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 80.6327 - val_loss: 77.3870\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 79.4060 - val_loss: 77.3375\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 79.5766 - val_loss: 77.2842\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 81.4451 - val_loss: 77.2268\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 80.8177 - val_loss: 77.1707\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 80.2173 - val_loss: 77.1111\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 80.5336 - val_loss: 77.0476\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 79.7865 - val_loss: 76.9885\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 81.3368 - val_loss: 76.9355\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 79.0249 - val_loss: 76.8879\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 79.0223 - val_loss: 76.8439\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 79.8540 - val_loss: 76.7946\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 80.2477 - val_loss: 76.7425\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 79.0711 - val_loss: 76.6732\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 78.3117 - val_loss: 76.6043\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 80.6677 - val_loss: 76.5326\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 79.1895 - val_loss: 76.4624\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 80.3999 - val_loss: 76.3971\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 78.7724 - val_loss: 76.3389\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 80.5256 - val_loss: 76.2839\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 80.3359 - val_loss: 76.2255\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 79.3971 - val_loss: 76.1722\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 78.8754 - val_loss: 76.1204\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 78.9352 - val_loss: 76.0692\n",
      "Epoch 125/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 78.0553 - val_loss: 76.0201\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 79.6468 - val_loss: 75.9741\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 77.2931 - val_loss: 75.9239\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 79.3767 - val_loss: 75.8669\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 78.5587 - val_loss: 75.8066\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 79.6194 - val_loss: 75.7522\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 78.4449 - val_loss: 75.7020\n",
      "Epoch 132/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 79.2199 - val_loss: 75.6485\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 78.8750 - val_loss: 75.6012\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 79.3103 - val_loss: 75.5361\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 79.5371 - val_loss: 75.4705\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 78.1519 - val_loss: 75.3998\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 79.2099 - val_loss: 75.3340\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 78.6403 - val_loss: 75.2638\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 76.9349 - val_loss: 75.2009\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 78.6949 - val_loss: 75.1380\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 77.8587 - val_loss: 75.0776\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 77.8189 - val_loss: 75.0231\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 76.4259 - val_loss: 74.9694\n",
      "Epoch 144/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 79.4537 - val_loss: 74.9049\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 78.0954 - val_loss: 74.8395\n",
      "Epoch 146/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 78.4387 - val_loss: 74.7799\n",
      "Epoch 147/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 77.5961 - val_loss: 74.7278\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 76.4323 - val_loss: 74.6797\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 77.3268 - val_loss: 74.6326\n",
      "Epoch 150/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 77.3236 - val_loss: 74.5901\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 78.9450 - val_loss: 74.5430\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 76.1123 - val_loss: 74.4858\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 77.6369 - val_loss: 74.4271\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 79.8877 - val_loss: 74.3674\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 78.0768 - val_loss: 74.2979\n",
      "Epoch 156/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 76.8865 - val_loss: 74.2300\n",
      "Epoch 157/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 77.2295 - val_loss: 74.1660\n",
      "Epoch 158/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 77.8475 - val_loss: 74.0976\n",
      "Epoch 159/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 76.9469 - val_loss: 74.0338\n",
      "Epoch 160/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 76.9905 - val_loss: 73.9676\n",
      "Epoch 161/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 77.4726 - val_loss: 73.9030\n",
      "Epoch 162/500\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 76.5850 - val_loss: 73.8457\n",
      "Epoch 163/500\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 74.9521 - val_loss: 73.7938\n",
      "Epoch 164/500\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 76.3633 - val_loss: 73.7389\n",
      "Epoch 165/500\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 77.1347 - val_loss: 73.6851\n",
      "Epoch 166/500\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 76.2285 - val_loss: 73.6358\n",
      "Epoch 167/500\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 77.0502 - val_loss: 73.5847\n",
      "Epoch 168/500\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 76.7582 - val_loss: 73.5312\n",
      "Epoch 169/500\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 77.4678 - val_loss: 73.4766\n",
      "Epoch 170/500\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 77.7707 - val_loss: 73.4141\n",
      "Epoch 171/500\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 76.7551 - val_loss: 73.3561\n",
      "Epoch 172/500\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 75.8651 - val_loss: 73.3021\n",
      "Epoch 173/500\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 76.3352 - val_loss: 73.2380\n",
      "Epoch 174/500\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 77.6930 - val_loss: 73.1579\n",
      "Epoch 175/500\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 74.3895 - val_loss: 73.0895\n",
      "Epoch 176/500\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 76.4467 - val_loss: 73.0219\n",
      "Epoch 177/500\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 78.2822 - val_loss: 72.9548\n",
      "Epoch 178/500\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 75.2862 - val_loss: 72.8940\n",
      "Epoch 179/500\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 75.4490 - val_loss: 72.8358\n",
      "Epoch 180/500\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 74.9430 - val_loss: 72.7828\n",
      "Epoch 181/500\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 77.0367 - val_loss: 72.7292\n",
      "Epoch 182/500\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 75.2860 - val_loss: 72.6783\n",
      "Epoch 183/500\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 75.4371 - val_loss: 72.6291\n",
      "Epoch 184/500\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 75.4526 - val_loss: 72.5686\n",
      "Epoch 185/500\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 76.5593 - val_loss: 72.5068\n",
      "Epoch 186/500\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 75.4764 - val_loss: 72.4445\n",
      "Epoch 187/500\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 76.2597 - val_loss: 72.3828\n",
      "Epoch 188/500\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 76.0547 - val_loss: 72.3226\n",
      "Epoch 189/500\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 75.8355 - val_loss: 72.2629\n",
      "Epoch 190/500\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 74.6584 - val_loss: 72.2045\n",
      "Epoch 191/500\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 74.3152 - val_loss: 72.1420\n",
      "Epoch 192/500\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 74.9087 - val_loss: 72.0729\n",
      "Epoch 193/500\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 75.8525 - val_loss: 72.0073\n",
      "Epoch 194/500\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 74.7785 - val_loss: 71.9432\n",
      "Epoch 195/500\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 75.2898 - val_loss: 71.8835\n",
      "Epoch 196/500\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 76.1444 - val_loss: 71.8281\n",
      "Epoch 197/500\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 75.6804 - val_loss: 71.7758\n",
      "Epoch 198/500\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 74.4002 - val_loss: 71.7211\n",
      "Epoch 199/500\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 73.2542 - val_loss: 71.6670\n",
      "Epoch 200/500\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 73.4149 - val_loss: 71.6152\n",
      "Epoch 201/500\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 73.6153 - val_loss: 71.5526\n",
      "Epoch 202/500\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 73.3031 - val_loss: 71.4771\n",
      "Epoch 203/500\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 73.9553 - val_loss: 71.3987\n",
      "Epoch 204/500\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 78.0617 - val_loss: 71.3286\n",
      "Epoch 205/500\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 73.1689 - val_loss: 71.2624\n",
      "Epoch 206/500\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 76.1782 - val_loss: 71.2057\n",
      "Epoch 207/500\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 74.7572 - val_loss: 71.1522\n",
      "Epoch 208/500\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 75.3407 - val_loss: 71.0988\n",
      "Epoch 209/500\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 73.7693 - val_loss: 71.0390\n",
      "Epoch 210/500\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 76.0593 - val_loss: 70.9783\n",
      "Epoch 211/500\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 73.1289 - val_loss: 70.9144\n",
      "Epoch 212/500\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 72.3252 - val_loss: 70.8440\n",
      "Epoch 213/500\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 74.1273 - val_loss: 70.7798\n",
      "Epoch 214/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 72.4824 - val_loss: 70.7122\n",
      "Epoch 215/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 73.5505 - val_loss: 70.6478\n",
      "Epoch 216/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 73.7607 - val_loss: 70.5852\n",
      "Epoch 217/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 73.2414 - val_loss: 70.5224\n",
      "Epoch 218/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 72.2392 - val_loss: 70.4606\n",
      "Epoch 219/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 74.4626 - val_loss: 70.4002\n",
      "Epoch 220/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 73.2646 - val_loss: 70.3400\n",
      "Epoch 221/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 72.1028 - val_loss: 70.2809\n",
      "Epoch 222/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 74.8686 - val_loss: 70.2143\n",
      "Epoch 223/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 73.7620 - val_loss: 70.1535\n",
      "Epoch 224/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 73.1997 - val_loss: 70.0916\n",
      "Epoch 225/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 73.4048 - val_loss: 70.0326\n",
      "Epoch 226/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 73.9314 - val_loss: 69.9797\n",
      "Epoch 227/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 74.1515 - val_loss: 69.9270\n",
      "Epoch 228/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 73.3853 - val_loss: 69.8721\n",
      "Epoch 229/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 73.6511 - val_loss: 69.8146\n",
      "Epoch 230/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 73.2566 - val_loss: 69.7519\n",
      "Epoch 231/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 72.3372 - val_loss: 69.6853\n",
      "Epoch 232/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 72.4756 - val_loss: 69.6200\n",
      "Epoch 233/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 72.6762 - val_loss: 69.5548\n",
      "Epoch 234/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 74.0672 - val_loss: 69.4927\n",
      "Epoch 235/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 71.5679 - val_loss: 69.4327\n",
      "Epoch 236/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 73.5667 - val_loss: 69.3802\n",
      "Epoch 237/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 72.7952 - val_loss: 69.3260\n",
      "Epoch 238/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 71.2738 - val_loss: 69.2693\n",
      "Epoch 239/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 71.3090 - val_loss: 69.2102\n",
      "Epoch 240/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 72.2518 - val_loss: 69.1454\n",
      "Epoch 241/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 71.6584 - val_loss: 69.0825\n",
      "Epoch 242/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 71.8018 - val_loss: 69.0173\n",
      "Epoch 243/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 71.7542 - val_loss: 68.9482\n",
      "Epoch 244/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 73.9208 - val_loss: 68.8778\n",
      "Epoch 245/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 71.0961 - val_loss: 68.8080\n",
      "Epoch 246/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 71.7613 - val_loss: 68.7382\n",
      "Epoch 247/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 70.7727 - val_loss: 68.6642\n",
      "Epoch 248/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 70.4597 - val_loss: 68.5910\n",
      "Epoch 249/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 71.2897 - val_loss: 68.5217\n",
      "Epoch 250/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 72.6266 - val_loss: 68.4599\n",
      "Epoch 251/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 70.7675 - val_loss: 68.3996\n",
      "Epoch 252/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 70.9079 - val_loss: 68.3448\n",
      "Epoch 253/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 71.7927 - val_loss: 68.2906\n",
      "Epoch 254/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 70.9675 - val_loss: 68.2334\n",
      "Epoch 255/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 69.6281 - val_loss: 68.1803\n",
      "Epoch 256/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 69.4295 - val_loss: 68.1261\n",
      "Epoch 257/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 71.3604 - val_loss: 68.0709\n",
      "Epoch 258/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 72.4074 - val_loss: 68.0121\n",
      "Epoch 259/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 71.2198 - val_loss: 67.9454\n",
      "Epoch 260/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 70.8116 - val_loss: 67.8847\n",
      "Epoch 261/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 71.0422 - val_loss: 67.8216\n",
      "Epoch 262/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 71.0359 - val_loss: 67.7620\n",
      "Epoch 263/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 67.9337 - val_loss: 67.6985\n",
      "Epoch 264/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 70.4326 - val_loss: 67.6331\n",
      "Epoch 265/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 69.9451 - val_loss: 67.5688\n",
      "Epoch 266/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 70.6616 - val_loss: 67.5068\n",
      "Epoch 267/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 69.3842 - val_loss: 67.4455\n",
      "Epoch 268/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 70.7459 - val_loss: 67.3840\n",
      "Epoch 269/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 68.4505 - val_loss: 67.3293\n",
      "Epoch 270/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 70.4411 - val_loss: 67.2770\n",
      "Epoch 271/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 68.6656 - val_loss: 67.2204\n",
      "Epoch 272/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 71.5363 - val_loss: 67.1543\n",
      "Epoch 273/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 71.6146 - val_loss: 67.0826\n",
      "Epoch 274/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 69.6035 - val_loss: 67.0092\n",
      "Epoch 275/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 70.3833 - val_loss: 66.9403\n",
      "Epoch 276/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 70.1753 - val_loss: 66.8723\n",
      "Epoch 277/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 70.3288 - val_loss: 66.8015\n",
      "Epoch 278/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 71.1534 - val_loss: 66.7396\n",
      "Epoch 279/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 69.3512 - val_loss: 66.6796\n",
      "Epoch 280/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 69.9082 - val_loss: 66.6222\n",
      "Epoch 281/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 71.3679 - val_loss: 66.5653\n",
      "Epoch 282/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 70.2552 - val_loss: 66.5061\n",
      "Epoch 283/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 68.6513 - val_loss: 66.4461\n",
      "Epoch 284/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 68.0362 - val_loss: 66.3822\n",
      "Epoch 285/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 68.9872 - val_loss: 66.3169\n",
      "Epoch 286/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 69.9256 - val_loss: 66.2485\n",
      "Epoch 287/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 70.6844 - val_loss: 66.1819\n",
      "Epoch 288/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 68.9034 - val_loss: 66.1167\n",
      "Epoch 289/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 70.5368 - val_loss: 66.0523\n",
      "Epoch 290/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 69.8243 - val_loss: 65.9897\n",
      "Epoch 291/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 69.2400 - val_loss: 65.9218\n",
      "Epoch 292/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 68.4052 - val_loss: 65.8589\n",
      "Epoch 293/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 69.2862 - val_loss: 65.7918\n",
      "Epoch 294/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 69.2408 - val_loss: 65.7226\n",
      "Epoch 295/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 69.4979 - val_loss: 65.6503\n",
      "Epoch 296/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 68.7378 - val_loss: 65.5767\n",
      "Epoch 297/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 69.5210 - val_loss: 65.5060\n",
      "Epoch 298/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 66.9839 - val_loss: 65.4380\n",
      "Epoch 299/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 68.7570 - val_loss: 65.3743\n",
      "Epoch 300/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 68.1392 - val_loss: 65.3202\n",
      "Epoch 301/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 67.5023 - val_loss: 65.2719\n",
      "Epoch 302/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 69.8196 - val_loss: 65.2197\n",
      "Epoch 303/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 65.9130 - val_loss: 65.1683\n",
      "Epoch 304/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 68.8790 - val_loss: 65.1166\n",
      "Epoch 305/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 67.7205 - val_loss: 65.0570\n",
      "Epoch 306/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 66.0381 - val_loss: 64.9983\n",
      "Epoch 307/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 68.5770 - val_loss: 64.9307\n",
      "Epoch 308/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 68.2428 - val_loss: 64.8618\n",
      "Epoch 309/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 69.8424 - val_loss: 64.7861\n",
      "Epoch 310/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 69.6305 - val_loss: 64.7147\n",
      "Epoch 311/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 68.4892 - val_loss: 64.6511\n",
      "Epoch 312/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 68.1653 - val_loss: 64.5909\n",
      "Epoch 313/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 67.0839 - val_loss: 64.5300\n",
      "Epoch 314/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 68.5639 - val_loss: 64.4774\n",
      "Epoch 315/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 68.4464 - val_loss: 64.4273\n",
      "Epoch 316/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 68.0498 - val_loss: 64.3727\n",
      "Epoch 317/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 67.2130 - val_loss: 64.3168\n",
      "Epoch 318/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 64.3458 - val_loss: 64.2570\n",
      "Epoch 319/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 69.1064 - val_loss: 64.2014\n",
      "Epoch 320/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 65.9042 - val_loss: 64.1400\n",
      "Epoch 321/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 68.6539 - val_loss: 64.0722\n",
      "Epoch 322/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 66.1912 - val_loss: 64.0019\n",
      "Epoch 323/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 67.8175 - val_loss: 63.9251\n",
      "Epoch 324/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 65.7808 - val_loss: 63.8500\n",
      "Epoch 325/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 64.9809 - val_loss: 63.7773\n",
      "Epoch 326/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 66.6176 - val_loss: 63.7090\n",
      "Epoch 327/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 66.3289 - val_loss: 63.6435\n",
      "Epoch 328/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 66.5772 - val_loss: 63.5804\n",
      "Epoch 329/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 66.1019 - val_loss: 63.5182\n",
      "Epoch 330/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 68.5250 - val_loss: 63.4622\n",
      "Epoch 331/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 66.0974 - val_loss: 63.4032\n",
      "Epoch 332/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 68.7217 - val_loss: 63.3365\n",
      "Epoch 333/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 66.4127 - val_loss: 63.2685\n",
      "Epoch 334/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 68.0318 - val_loss: 63.1992\n",
      "Epoch 335/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 67.0808 - val_loss: 63.1306\n",
      "Epoch 336/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 67.4067 - val_loss: 63.0673\n",
      "Epoch 337/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 65.7656 - val_loss: 63.0074\n",
      "Epoch 338/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 63.8133 - val_loss: 62.9448\n",
      "Epoch 339/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 66.3370 - val_loss: 62.8807\n",
      "Epoch 340/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 64.8432 - val_loss: 62.8155\n",
      "Epoch 341/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 65.1291 - val_loss: 62.7492\n",
      "Epoch 342/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 61.9523 - val_loss: 62.6777\n",
      "Epoch 343/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 65.5891 - val_loss: 62.6071\n",
      "Epoch 344/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 65.5542 - val_loss: 62.5440\n",
      "Epoch 345/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 67.6947 - val_loss: 62.4820\n",
      "Epoch 346/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 65.9117 - val_loss: 62.4225\n",
      "Epoch 347/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 64.9947 - val_loss: 62.3647\n",
      "Epoch 348/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 65.9710 - val_loss: 62.3013\n",
      "Epoch 349/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 64.4862 - val_loss: 62.2369\n",
      "Epoch 350/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 62.7740 - val_loss: 62.1673\n",
      "Epoch 351/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 65.1899 - val_loss: 62.0938\n",
      "Epoch 352/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 64.0243 - val_loss: 62.0177\n",
      "Epoch 353/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 66.3181 - val_loss: 61.9511\n",
      "Epoch 354/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 62.6732 - val_loss: 61.8915\n",
      "Epoch 355/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 63.3459 - val_loss: 61.8363\n",
      "Epoch 356/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 67.3806 - val_loss: 61.7776\n",
      "Epoch 357/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 66.1463 - val_loss: 61.7195\n",
      "Epoch 358/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 65.1176 - val_loss: 61.6611\n",
      "Epoch 359/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 63.3106 - val_loss: 61.5978\n",
      "Epoch 360/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 62.6857 - val_loss: 61.5278\n",
      "Epoch 361/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 64.0561 - val_loss: 61.4518\n",
      "Epoch 362/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 63.6935 - val_loss: 61.3784\n",
      "Epoch 363/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 64.3339 - val_loss: 61.3092\n",
      "Epoch 364/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 64.9462 - val_loss: 61.2421\n",
      "Epoch 365/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 64.2246 - val_loss: 61.1777\n",
      "Epoch 366/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 63.7526 - val_loss: 61.1130\n",
      "Epoch 367/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 66.7390 - val_loss: 61.0446\n",
      "Epoch 368/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 65.1331 - val_loss: 60.9763\n",
      "Epoch 369/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 63.3956 - val_loss: 60.9097\n",
      "Epoch 370/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 63.9677 - val_loss: 60.8409\n",
      "Epoch 371/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 63.1902 - val_loss: 60.7707\n",
      "Epoch 372/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 62.6361 - val_loss: 60.6999\n",
      "Epoch 373/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 64.5020 - val_loss: 60.6274\n",
      "Epoch 374/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 64.3511 - val_loss: 60.5574\n",
      "Epoch 375/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 64.3021 - val_loss: 60.4957\n",
      "Epoch 376/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 64.1846 - val_loss: 60.4374\n",
      "Epoch 377/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 65.5802 - val_loss: 60.3888\n",
      "Epoch 378/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 62.8879 - val_loss: 60.3493\n",
      "Epoch 379/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 65.4370 - val_loss: 60.3089\n",
      "Epoch 380/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 63.6598 - val_loss: 60.2578\n",
      "Epoch 381/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 62.3336 - val_loss: 60.2009\n",
      "Epoch 382/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 64.0386 - val_loss: 60.1443\n",
      "Epoch 383/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 62.0158 - val_loss: 60.0817\n",
      "Epoch 384/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 65.0207 - val_loss: 60.0158\n",
      "Epoch 385/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 65.0276 - val_loss: 59.9454\n",
      "Epoch 386/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 61.4680 - val_loss: 59.8784\n",
      "Epoch 387/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 63.5274 - val_loss: 59.8049\n",
      "Epoch 388/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 62.1211 - val_loss: 59.7340\n",
      "Epoch 389/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 62.8124 - val_loss: 59.6646\n",
      "Epoch 390/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 63.6602 - val_loss: 59.6003\n",
      "Epoch 391/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 62.9200 - val_loss: 59.5362\n",
      "Epoch 392/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 60.9504 - val_loss: 59.4714\n",
      "Epoch 393/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 61.3262 - val_loss: 59.4102\n",
      "Epoch 394/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 60.7009 - val_loss: 59.3468\n",
      "Epoch 395/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 63.8219 - val_loss: 59.2852\n",
      "Epoch 396/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 62.6451 - val_loss: 59.2254\n",
      "Epoch 397/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 60.4566 - val_loss: 59.1572\n",
      "Epoch 398/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 61.1466 - val_loss: 59.0873\n",
      "Epoch 399/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 63.0658 - val_loss: 59.0140\n",
      "Epoch 400/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 62.4026 - val_loss: 58.9420\n",
      "Epoch 401/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 62.1602 - val_loss: 58.8727\n",
      "Epoch 402/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 60.3603 - val_loss: 58.8016\n",
      "Epoch 403/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 59.2728 - val_loss: 58.7313\n",
      "Epoch 404/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 63.0751 - val_loss: 58.6608\n",
      "Epoch 405/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 61.3683 - val_loss: 58.5868\n",
      "Epoch 406/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 62.5544 - val_loss: 58.5150\n",
      "Epoch 407/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 61.0240 - val_loss: 58.4427\n",
      "Epoch 408/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 63.8019 - val_loss: 58.3746\n",
      "Epoch 409/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 61.2233 - val_loss: 58.3084\n",
      "Epoch 410/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 63.9331 - val_loss: 58.2484\n",
      "Epoch 411/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 61.4007 - val_loss: 58.1950\n",
      "Epoch 412/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 60.3187 - val_loss: 58.1464\n",
      "Epoch 413/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 60.8020 - val_loss: 58.0964\n",
      "Epoch 414/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 61.8678 - val_loss: 58.0454\n",
      "Epoch 415/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 63.8487 - val_loss: 57.9930\n",
      "Epoch 416/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 60.8543 - val_loss: 57.9379\n",
      "Epoch 417/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 62.5099 - val_loss: 57.8736\n",
      "Epoch 418/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 63.9559 - val_loss: 57.8072\n",
      "Epoch 419/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 60.9623 - val_loss: 57.7360\n",
      "Epoch 420/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 60.3216 - val_loss: 57.6677\n",
      "Epoch 421/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 62.0468 - val_loss: 57.5954\n",
      "Epoch 422/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 60.5656 - val_loss: 57.5270\n",
      "Epoch 423/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 58.8222 - val_loss: 57.4622\n",
      "Epoch 424/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 58.1521 - val_loss: 57.3976\n",
      "Epoch 425/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 58.5073 - val_loss: 57.3356\n",
      "Epoch 426/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 61.0260 - val_loss: 57.2782\n",
      "Epoch 427/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 59.9999 - val_loss: 57.2184\n",
      "Epoch 428/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 58.3736 - val_loss: 57.1510\n",
      "Epoch 429/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 61.2312 - val_loss: 57.0824\n",
      "Epoch 430/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 60.8021 - val_loss: 57.0111\n",
      "Epoch 431/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 59.3294 - val_loss: 56.9440\n",
      "Epoch 432/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 59.4979 - val_loss: 56.8785\n",
      "Epoch 433/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 58.7724 - val_loss: 56.8188\n",
      "Epoch 434/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 61.2737 - val_loss: 56.7645\n",
      "Epoch 435/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 57.7885 - val_loss: 56.7165\n",
      "Epoch 436/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 58.3540 - val_loss: 56.6749\n",
      "Epoch 437/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 58.6941 - val_loss: 56.6235\n",
      "Epoch 438/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 60.6828 - val_loss: 56.5686\n",
      "Epoch 439/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 62.2303 - val_loss: 56.5041\n",
      "Epoch 440/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 60.6143 - val_loss: 56.4316\n",
      "Epoch 441/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 56.5530 - val_loss: 56.3576\n",
      "Epoch 442/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 59.2539 - val_loss: 56.2801\n",
      "Epoch 443/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 58.1983 - val_loss: 56.1982\n",
      "Epoch 444/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 59.0461 - val_loss: 56.1221\n",
      "Epoch 445/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 60.7461 - val_loss: 56.0483\n",
      "Epoch 446/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 57.5755 - val_loss: 55.9791\n",
      "Epoch 447/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 59.1572 - val_loss: 55.9159\n",
      "Epoch 448/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 58.9046 - val_loss: 55.8613\n",
      "Epoch 449/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 60.9557 - val_loss: 55.8088\n",
      "Epoch 450/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 60.8765 - val_loss: 55.7558\n",
      "Epoch 451/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 58.6454 - val_loss: 55.6964\n",
      "Epoch 452/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 59.7251 - val_loss: 55.6397\n",
      "Epoch 453/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 57.9212 - val_loss: 55.5772\n",
      "Epoch 454/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 59.3273 - val_loss: 55.5073\n",
      "Epoch 455/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 59.6889 - val_loss: 55.4354\n",
      "Epoch 456/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 55.5669 - val_loss: 55.3645\n",
      "Epoch 457/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 57.6797 - val_loss: 55.2894\n",
      "Epoch 458/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 57.6988 - val_loss: 55.2126\n",
      "Epoch 459/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 59.2047 - val_loss: 55.1341\n",
      "Epoch 460/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 60.9126 - val_loss: 55.0639\n",
      "Epoch 461/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 57.0872 - val_loss: 54.9996\n",
      "Epoch 462/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 58.7728 - val_loss: 54.9391\n",
      "Epoch 463/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 58.9037 - val_loss: 54.8820\n",
      "Epoch 464/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 57.4542 - val_loss: 54.8255\n",
      "Epoch 465/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 55.4571 - val_loss: 54.7712\n",
      "Epoch 466/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 59.0064 - val_loss: 54.7160\n",
      "Epoch 467/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 54.6906 - val_loss: 54.6618\n",
      "Epoch 468/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 59.5073 - val_loss: 54.6073\n",
      "Epoch 469/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 59.2417 - val_loss: 54.5531\n",
      "Epoch 470/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 56.4088 - val_loss: 54.4989\n",
      "Epoch 471/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 60.5296 - val_loss: 54.4457\n",
      "Epoch 472/500\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 57.6751 - val_loss: 54.3938\n",
      "Epoch 473/500\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 56.8270 - val_loss: 54.3348\n",
      "Epoch 474/500\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 58.1348 - val_loss: 54.2667\n",
      "Epoch 475/500\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 57.9110 - val_loss: 54.1906\n",
      "Epoch 476/500\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 55.2869 - val_loss: 54.1201\n",
      "Epoch 477/500\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 56.3544 - val_loss: 54.0501\n",
      "Epoch 478/500\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 54.4520 - val_loss: 53.9762\n",
      "Epoch 479/500\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 55.2546 - val_loss: 53.8990\n",
      "Epoch 480/500\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 58.9365 - val_loss: 53.8195\n",
      "Epoch 481/500\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 58.6229 - val_loss: 53.7408\n",
      "Epoch 482/500\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 58.9660 - val_loss: 53.6683\n",
      "Epoch 483/500\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 56.3307 - val_loss: 53.5989\n",
      "Epoch 484/500\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 59.4578 - val_loss: 53.5323\n",
      "Epoch 485/500\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 55.8860 - val_loss: 53.4716\n",
      "Epoch 486/500\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 56.4497 - val_loss: 53.4134\n",
      "Epoch 487/500\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 55.6447 - val_loss: 53.3539\n",
      "Epoch 488/500\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 58.4440 - val_loss: 53.2947\n",
      "Epoch 489/500\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 55.7834 - val_loss: 53.2393\n",
      "Epoch 490/500\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 53.3651 - val_loss: 53.1862\n",
      "Epoch 491/500\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 58.2157 - val_loss: 53.1309\n",
      "Epoch 492/500\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 55.8179 - val_loss: 53.0754\n",
      "Epoch 493/500\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 57.2002 - val_loss: 53.0150\n",
      "Epoch 494/500\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 58.0072 - val_loss: 52.9614\n",
      "Epoch 495/500\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 54.7800 - val_loss: 52.9030\n",
      "Epoch 496/500\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 55.3976 - val_loss: 52.8412\n",
      "Epoch 497/500\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 57.0197 - val_loss: 52.7696\n",
      "Epoch 498/500\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 55.4659 - val_loss: 52.6944\n",
      "Epoch 499/500\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 52.9115 - val_loss: 52.6148\n",
      "Epoch 500/500\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 57.6636 - val_loss: 52.5411\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x, y, validation_data=(vx, vy), epochs=epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "3406b84d-f3d9-41ff-91d9-1ac28687a789",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "a01fa082-b2d1-4328-87c0-92305ddc7f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[47,  4, 46, 35,  6, 66, 25, 20, 20, 59, 17, 61,  3, 81, 92, 43,\n",
       "        98, 84, 16, 49],\n",
       "       [59,  6, 56, 95, 81, 41, 90, 98, 99, 28, 74, 27, 40, 44, 88, 82,\n",
       "        34, 35, 52, 97],\n",
       "       [29, 78, 51, 59, 23, 50, 42, 72, 96, 62, 44, 90, 87, 61, 85, 13,\n",
       "        23, 44, 87, 36],\n",
       "       [94, 56, 42, 58, 13, 69, 52, 21, 20, 84, 33, 66, 88, 91, 75, 11,\n",
       "        76, 36, 87, 94],\n",
       "       [10, 74, 39, 99, 41, 93, 37, 95,  9, 77, 72, 63, 84, 31, 99, 63,\n",
       "        94, 37,  3, 28]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79f3652d-04c7-4fa2-9d5b-2207ec6ab9fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([97, 97, 97, 91, 99])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53cdbfe0-effd-4fc7-92c3-610393c316f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[40.858913],\n",
       "       [40.858963],\n",
       "       [40.85885 ],\n",
       "       [40.858837],\n",
       "       [40.858932]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "92fc19e8-4331-47cc-b055-6d48dd6aa9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([0, 0, 0, 0, 0])>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tf.argmax(model.predict(x[:5]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "e0419e46-04dc-4ee5-ba94-0665e4a61276",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "169bfd38-be4c-4693-ae6e-574da519677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = gen_data(batch_size=30, length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670ae1a1-56ad-4b63-89fe-4e7ba70187c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([90, 92, 76, 95, 81])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "435adf74-c8b0-490d-a52b-171360d0a396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([0, 0, 0, 0, 0])>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(model.predict(x[:5]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeec3f6-76fe-4329-923b-a4d667463a24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87276ce9-8691-4143-85de-6eb04f9ff869",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
