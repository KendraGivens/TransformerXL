{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad2c14f8-8dd8-459f-81fe-a6708e6a8355",
   "metadata": {},
   "source": [
    "---\n",
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3dcad43d-67f7-49b9-84ce-6d91a0f468a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5e7cb9df-5746-44a3-998a-b671b284957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2c507a85-fe6f-4d3c-ae1f-16914fdec693",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../../../deep-learning-dna\")\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../../deep-learning-dna/common\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "31b9c98a-b4ee-4667-89a3-5f04549f7d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fda64db1-8058-4e15-b2d4-1a3e64f23fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import math\n",
    "import string\n",
    "\n",
    "from Attention import Set_Transformer\n",
    "from common.models import dnabert\n",
    "from common import dna\n",
    "from lmdbm import Lmdb\n",
    "from common.data import DnaSequenceGenerator, DnaLabelType, DnaSampleGenerator, find_dbs\n",
    "import wandb\n",
    "\n",
    "import tf_utils as tfu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0dfb3b1a-2db7-4463-a208-b5d29e20f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tfu.devices.select_gpu(0, use_dynamic_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54593a0f-b5b4-401e-91e2-8c1a1598432e",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "65e83dc7-9349-4aab-b240-1e0d39988513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(batch_size, length=5):\n",
    "    x = np.random.randint(1, 100, (batch_size, length))\n",
    "    y = np.max(x, axis=1)\n",
    "    return x, y # (batch_size, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f47ff082-2d44-4467-872b-c925150812b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = gen_data(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "df9ded01-96fb-4df2-b52f-429f623ab30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5) (3,)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0adef207-d041-409c-9f7b-f57799812e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[36, 12, 68, 50, 37],\n",
       "       [43, 29, 98, 57, 65],\n",
       "       [36, 64, 61, 32, 20]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "df941e9f-16d1-4d9d-a6ec-bec7469d2fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([68, 98, 64])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835c39dd-9fb5-49bf-8763-14ca694a1a35",
   "metadata": {},
   "source": [
    "---\n",
    "# Cache Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "da4cb5e4-2d26-4e40-a00f-3e71f470b273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cache_Memory(current_state, previous_state, memory_length):\n",
    "    \n",
    "    if memory_length is None or memory_length == 0:\n",
    "        return None\n",
    "    else:\n",
    "        if previous_state is None:\n",
    "            new_mem = current_state[:, -memory_length:, :]\n",
    "        else:\n",
    "            new_mem = tf.concat(\n",
    "                    [previous_state, current_state], 1)[:, -memory_length:, :]\n",
    "\n",
    "    return tf.stop_gradient(new_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb50453f-628c-412e-942b-d486d01c6b8a",
   "metadata": {},
   "source": [
    "---\n",
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1bf9ef79-9ede-4271-8542-223cdd1e11ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(keras.Model):\n",
    "    def __init__(self, num_induce, embed_dim, num_heads, use_layernorm, pre_layernorm, use_keras_mha):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.num_induce = num_induce\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.use_layernorm = use_layernorm\n",
    "        self.pre_layernorm = pre_layernorm\n",
    "        self.use_keras_mha = use_keras_mha\n",
    "        \n",
    "        if self.num_induce == 0:       \n",
    "            self.attention = (Set_Transformer.SetAttentionBlock(embed_dim=self.embed_dim, num_heads=self.num_heads, use_layernorm=self.use_layernorm,pre_layernorm=self.pre_layernorm,use_keras_mha=self.use_keras_mha))\n",
    "        else:\n",
    "            self.attention = Set_Transformer.InducedSetAttentionBlock(embed_dim=self.embed_dim, num_heads=self.num_heads, num_induce=self.num_induce, use_layernorm=self.use_layernorm, pre_layernorm=self.pre_layernorm, use_keras_mha=self.use_keras_mha)\n",
    "    \n",
    "    \n",
    "    def call(self, data, mems):\n",
    "                \n",
    "            attention = self.attention([data, mems])\n",
    "                \n",
    "            return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c32bb5-4520-4802-a8f0-5f7849905a83",
   "metadata": {},
   "source": [
    "---\n",
    "# XL Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7245b7dd-a3b2-464a-9f5b-7efd4a9d0ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerXLBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 num_induce, \n",
    "                 embed_dim,\n",
    "                 num_heads,\n",
    "                 use_layernorm,\n",
    "                 pre_layernorm,\n",
    "                 use_keras_mha,):\n",
    "\n",
    "        super(TransformerXLBlock, self).__init__()\n",
    "        \n",
    "        self.num_induce = num_induce\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.use_layernorm = use_layernorm\n",
    "        self.pre_layernorm = pre_layernorm\n",
    "        self.use_keras_mha = use_keras_mha\n",
    "        \n",
    "        self.attention = Attention\n",
    "        \n",
    "        self.attention_layer = self.attention(self.num_induce, self.embed_dim, self.num_heads, self.use_layernorm, self.pre_layernorm, self.use_keras_mha)\n",
    "\n",
    "   \n",
    "    def call(self,\n",
    "             content_stream,\n",
    "             state=None):\n",
    "        \n",
    "        attention_output = self.attention_layer(content_stream, state)\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8749ec30-1281-4f9d-8baa-54b890fb7949",
   "metadata": {},
   "source": [
    "---\n",
    "# Transformer XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "019ca25d-e899-4e17-8f41-dfdc3d9bdee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerXL(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 mem_switched, \n",
    "                 num_layers,\n",
    "                 num_induce,\n",
    "                 embed_dim,\n",
    "                 num_heads,\n",
    "                 dropout_rate,\n",
    "                 mem_len=None,\n",
    "                 use_layernorm=True,\n",
    "                 pre_layernorm=True, \n",
    "                 use_keras_mha=True):\n",
    "        \n",
    "        super(TransformerXL, self).__init__()\n",
    "\n",
    "        self.mem_switched = mem_switched\n",
    "        self.num_layers = num_layers\n",
    "        self.num_induce = num_induce\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mem_len = mem_len\n",
    "        self.use_layernorm = use_layernorm\n",
    "        self.pre_layernorm = pre_layernorm\n",
    "        self.use_keras_mha = use_keras_mha\n",
    "\n",
    "        self.transformer_xl_layers = []\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            self.transformer_xl_layers.append(\n",
    "                    TransformerXLBlock(self.num_induce,\n",
    "                                        self.embed_dim,\n",
    "                                        self.num_heads,\n",
    "                                        self.use_layernorm,\n",
    "                                        self.pre_layernorm, \n",
    "                                        self.use_keras_mha))\n",
    "\n",
    "        self.output_dropout = tf.keras.layers.Dropout(rate=self.dropout_rate)\n",
    "\n",
    "    def call(self,\n",
    "             content_stream,\n",
    "             state=None):\n",
    "        \n",
    "        new_mems = []\n",
    "\n",
    "        if state is None:\n",
    "            state = [None] * self.num_layers\n",
    "            \n",
    "        for i in range(self.num_layers):\n",
    "            if self.mem_switched == False:\n",
    "                new_mems.append(Cache_Memory(content_stream, state[i], self.mem_len))\n",
    "            \n",
    "            transformer_xl_layer = self.transformer_xl_layers[i]\n",
    "            \n",
    "            transformer_xl_output = transformer_xl_layer(content_stream=content_stream,\n",
    "                                                        state=state[i])\n",
    "            \n",
    "            content_stream = self.output_dropout(transformer_xl_output)\n",
    "            \n",
    "            if self.mem_switched == True:\n",
    "                new_mems.append(Cache_Memory(content_stream, state[i], self.mem_len))\n",
    "\n",
    "        output_stream = content_stream\n",
    "        return output_stream, new_mems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a127a4b5-15e3-46e3-838b-a5e18e4d8dc8",
   "metadata": {},
   "source": [
    "---\n",
    "# Xl Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "855d43dd-e182-4c2a-961a-ff9781141f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XlModel(keras.Model):\n",
    "    def __init__(self, mem_switched, max_files, encoder, block_size, max_set_len, num_induce, embed_dim, num_layers, num_heads, mem_len, dropout_rate, num_seeds, use_layernorm, pre_layernorm, use_keras_mha):\n",
    "        super(XlModel, self).__init__()\n",
    "        \n",
    "        self.mem_switched = mem_switched\n",
    "        self.max_files = max_files\n",
    "        self.encoder = encoder\n",
    "        self.block_size = block_size\n",
    "        self.max_set_len = max_set_len\n",
    "        self.num_induce = num_induce\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.mem_len = mem_len\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_seeds = num_seeds\n",
    "        self.use_layernorm = use_layernorm\n",
    "        self.pre_layernorm = pre_layernorm\n",
    "        self.use_keras_mha = use_keras_mha\n",
    "        \n",
    "        self.linear_layer = keras.layers.Dense(self.embed_dim)\n",
    "        \n",
    "        self.transformer_xl = TransformerXL(self.mem_switched,\n",
    "                                            self.num_layers,\n",
    "                                             self.num_induce,\n",
    "                                             self.embed_dim,\n",
    "                                             self.num_heads,\n",
    "                                             self.dropout_rate,\n",
    "                                             self.mem_len,\n",
    "                                             self.use_layernorm,\n",
    "                                             self.pre_layernorm,\n",
    "                                             self.use_keras_mha)\n",
    "        \n",
    "\n",
    "        self.segment_pooling_layer = Set_Transformer.PoolingByMultiHeadAttention(num_seeds=self.num_seeds,embed_dim=self.embed_dim,num_heads=self.num_heads,use_layernorm=self.use_layernorm,pre_layernorm=self.pre_layernorm, use_keras_mha=self.use_keras_mha, is_final_block=True)\n",
    "       \n",
    "        self.output_pooling_layer = Set_Transformer.PoolingByMultiHeadAttention(num_seeds=self.num_seeds,embed_dim=self.embed_dim,num_heads=self.num_heads,use_layernorm=self.use_layernorm,pre_layernorm=self.pre_layernorm, use_keras_mha=self.use_keras_mha, is_final_block=True)\n",
    "    \n",
    "        self.dropout_layer = keras.layers.Dropout(.5)\n",
    "    \n",
    "        self.dense_layer = keras.layers.Dense(100) \n",
    "        \n",
    "        #Classification\n",
    "        #self.dense_layer = keras.layers.Dense(100) \n",
    "\n",
    "    def call(self, x, training=None):    \n",
    " \n",
    "        mems = tf.zeros((self.num_layers, tf.shape(x)[0], self.mem_len, self.embed_dim))\n",
    "    \n",
    "        x = tf.expand_dims(x, axis=2)\n",
    "        \n",
    "        linear_transform = self.linear_layer(x)\n",
    "\n",
    "        for i in range(0, self.max_set_len, self.block_size):\n",
    "\n",
    "            block = linear_transform[:,i:i+self.block_size]\n",
    "            \n",
    "            output, mems = self.transformer_xl(content_stream=block, state=mems)\n",
    "            \n",
    "            segment_pooling = self.segment_pooling_layer(output)\n",
    "\n",
    "        output_pooling = self.output_pooling_layer(segment_pooling)\n",
    "        \n",
    "        dropout = self.dropout_layer(output_pooling)\n",
    "\n",
    "        dense = self.dense_layer(dropout)\n",
    "\n",
    "        #output = tf.reshape(dense, tf.shape(dense)[:2])\n",
    "        \n",
    "        #Classification\n",
    "        output = tf.reshape(dense, tf.shape(dense)[::2])         \n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3469de1-250d-4c14-bb79-a4e74f0fe93e",
   "metadata": {},
   "source": [
    "---\n",
    "# Xl Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "2bc3ea7d-d4a1-4a1e-bf2a-7cd1c91430cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xl Parameters\n",
    "max_files = 0\n",
    "encoder = 0\n",
    "mem_switched = True\n",
    "num_induce = 0\n",
    "embed_dim = 64\n",
    "num_layers = 4\n",
    "num_heads = 8\n",
    "mem_len = 0\n",
    "dropout_rate = 0.01\n",
    "num_seeds = 1\n",
    "use_layernorm = True\n",
    "pre_layernorm = True\n",
    "use_keras_mha = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89649663-7d18-48bc-8678-ce5f093908cc",
   "metadata": {},
   "source": [
    "---\n",
    "# Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d6709d90-e797-4f99-b863-119a9ee20cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 20\n",
    "length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "75799d2a-1d30-4758-9927-61514be7b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = gen_data(batch_size=10, length=length)\n",
    "vx, vy = gen_data(batch_size=10, length=length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "63636972-d74a-4840-b731-2b9bb46823d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_set_len = length\n",
    "set_len = length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "821b0c4a-6d05-4867-a713-474bfdbff881",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XlModel(mem_switched, max_files, encoder, block_size, max_set_len, num_induce, embed_dim, num_layers, num_heads, mem_len, dropout_rate, num_seeds, use_layernorm, pre_layernorm, use_keras_mha)\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-3),loss=keras.losses.MeanAbsoluteError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "52ceb122-67f0-4634-8b82-2ea318621958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([97, 93, 93, 91, 99])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "ccbcafc4-e399-4b30-986e-9a04fc656545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 100), dtype=int64, numpy=\n",
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "7ddcc096-cb9a-4c19-a838-0aa92a6a13bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "3a4e79fc-5cbf-4807-a782-ed7cbe4e4e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1/1 [==============================] - 4s 4s/step - loss: 96.6529 - val_loss: 95.5401\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 96.4316 - val_loss: 95.4352\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 96.2543 - val_loss: 95.3571\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 96.1945 - val_loss: 95.2737\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 96.1412 - val_loss: 95.1963\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 96.1163 - val_loss: 95.1385\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 95.9952 - val_loss: 95.0952\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 96.0557 - val_loss: 95.0451\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 95.8186 - val_loss: 94.9887\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 95.9525 - val_loss: 94.9337\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 95.7790 - val_loss: 94.8823\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 95.7626 - val_loss: 94.8351\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 95.8003 - val_loss: 94.7894\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 95.7723 - val_loss: 94.7403\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 95.8213 - val_loss: 94.6902\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 95.5908 - val_loss: 94.6353\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 95.5973 - val_loss: 94.5807\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 95.4262 - val_loss: 94.5297\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 95.4772 - val_loss: 94.4840\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 95.4521 - val_loss: 94.4395\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 95.3270 - val_loss: 94.3888\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 95.2575 - val_loss: 94.3331\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 95.1976 - val_loss: 94.2746\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 95.0883 - val_loss: 94.2179\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 94.9170 - val_loss: 94.1645\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 95.0049 - val_loss: 94.1130\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 94.9072 - val_loss: 94.0607\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 94.9036 - val_loss: 94.0039\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 95.0339 - val_loss: 93.9481\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 94.6763 - val_loss: 93.8991\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 94.9519 - val_loss: 93.8508\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 94.7902 - val_loss: 93.8017\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 94.8012 - val_loss: 93.7484\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 94.6940 - val_loss: 93.6898\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 94.3912 - val_loss: 93.6266\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 94.4475 - val_loss: 93.5661\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 94.4299 - val_loss: 93.5068\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 94.4050 - val_loss: 93.4525\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 94.6020 - val_loss: 93.3965\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 94.2209 - val_loss: 93.3366\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 94.1178 - val_loss: 93.2807\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 93.9524 - val_loss: 93.2240\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 94.3458 - val_loss: 93.1657\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 93.7758 - val_loss: 93.1030\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 93.9668 - val_loss: 93.0423\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 93.9923 - val_loss: 92.9806\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 93.5613 - val_loss: 92.9165\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 93.9068 - val_loss: 92.8560\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 93.4822 - val_loss: 92.8009\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 93.7032 - val_loss: 92.7453\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 93.5385 - val_loss: 92.6865\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 93.5811 - val_loss: 92.6256\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 93.5382 - val_loss: 92.5634\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 93.3239 - val_loss: 92.4988\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 93.3122 - val_loss: 92.4370\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 93.5899 - val_loss: 92.3752\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 93.2677 - val_loss: 92.3162\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 93.3346 - val_loss: 92.2612\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 93.3003 - val_loss: 92.2044\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 93.1456 - val_loss: 92.1411\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 93.1586 - val_loss: 92.0732\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 93.0005 - val_loss: 92.0047\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 92.5207 - val_loss: 91.9406\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 92.6494 - val_loss: 91.8777\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 92.8409 - val_loss: 91.8187\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 92.7703 - val_loss: 91.7596\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 92.5665 - val_loss: 91.6984\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 92.2544 - val_loss: 91.6364\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 92.7102 - val_loss: 91.5765\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 92.4086 - val_loss: 91.5177\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 92.6343 - val_loss: 91.4564\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 92.3522 - val_loss: 91.3952\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 92.2281 - val_loss: 91.3327\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 92.0920 - val_loss: 91.2676\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 92.0263 - val_loss: 91.2024\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 92.1923 - val_loss: 91.1378\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 92.0102 - val_loss: 91.0729\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 91.8468 - val_loss: 91.0099\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 92.3038 - val_loss: 90.9536\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 92.0511 - val_loss: 90.9011\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 91.8360 - val_loss: 90.8417\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 91.5899 - val_loss: 90.7742\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 91.6035 - val_loss: 90.7063\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 91.7537 - val_loss: 90.6395\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 91.3907 - val_loss: 90.5709\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 91.2271 - val_loss: 90.5054\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 91.8433 - val_loss: 90.4446\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 91.4999 - val_loss: 90.3831\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 91.4091 - val_loss: 90.3198\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 91.0615 - val_loss: 90.2533\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 91.1633 - val_loss: 90.1874\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 90.9243 - val_loss: 90.1283\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 91.3638 - val_loss: 90.0682\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 90.4818 - val_loss: 90.0064\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 90.8661 - val_loss: 89.9428\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 91.0791 - val_loss: 89.8769\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 90.6960 - val_loss: 89.8058\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 90.4928 - val_loss: 89.7353\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 90.7835 - val_loss: 89.6706\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 90.4390 - val_loss: 89.6069\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 90.7970 - val_loss: 89.5415\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 90.5572 - val_loss: 89.4768\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 89.7888 - val_loss: 89.4137\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 90.2974 - val_loss: 89.3531\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 90.0362 - val_loss: 89.2909\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 89.8553 - val_loss: 89.2289\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 90.1370 - val_loss: 89.1651\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 90.4859 - val_loss: 89.1031\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 89.7477 - val_loss: 89.0364\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 89.5451 - val_loss: 88.9664\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 90.5053 - val_loss: 88.8963\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 89.6668 - val_loss: 88.8303\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 89.4230 - val_loss: 88.7693\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 89.9825 - val_loss: 88.7097\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 89.6106 - val_loss: 88.6525\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 89.1821 - val_loss: 88.5889\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 90.1888 - val_loss: 88.5242\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 89.5988 - val_loss: 88.4595\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 89.3942 - val_loss: 88.3925\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 89.5784 - val_loss: 88.3206\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 89.3919 - val_loss: 88.2511\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 89.2612 - val_loss: 88.1871\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 89.2815 - val_loss: 88.1236\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 88.6789 - val_loss: 88.0609\n",
      "Epoch 125/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 89.7890 - val_loss: 88.0060\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 89.1318 - val_loss: 87.9557\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 88.8144 - val_loss: 87.8968\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 88.6434 - val_loss: 87.8366\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 88.3445 - val_loss: 87.7753\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 88.1184 - val_loss: 87.7181\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 88.8006 - val_loss: 87.6617\n",
      "Epoch 132/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 88.4425 - val_loss: 87.6023\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 88.9063 - val_loss: 87.5341\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 88.6941 - val_loss: 87.4643\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 88.4646 - val_loss: 87.3878\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 87.9048 - val_loss: 87.3087\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 87.4129 - val_loss: 87.2284\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 88.6601 - val_loss: 87.1550\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 87.8415 - val_loss: 87.0867\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 87.5293 - val_loss: 87.0229\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 87.4943 - val_loss: 86.9632\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 87.9221 - val_loss: 86.9044\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 87.9695 - val_loss: 86.8475\n",
      "Epoch 144/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 87.5272 - val_loss: 86.7879\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 87.9040 - val_loss: 86.7252\n",
      "Epoch 146/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 87.4570 - val_loss: 86.6662\n",
      "Epoch 147/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 87.4324 - val_loss: 86.6017\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 87.1480 - val_loss: 86.5463\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 86.6469 - val_loss: 86.4906\n",
      "Epoch 150/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 87.7906 - val_loss: 86.4284\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 87.7601 - val_loss: 86.3593\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 87.6198 - val_loss: 86.2816\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 87.1388 - val_loss: 86.2078\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 87.1160 - val_loss: 86.1370\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 86.8253 - val_loss: 86.0621\n",
      "Epoch 156/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 87.1884 - val_loss: 85.9924\n",
      "Epoch 157/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 87.3263 - val_loss: 85.9235\n",
      "Epoch 158/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 87.2594 - val_loss: 85.8588\n",
      "Epoch 159/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 86.6940 - val_loss: 85.7984\n",
      "Epoch 160/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 86.6506 - val_loss: 85.7342\n",
      "Epoch 161/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 87.0534 - val_loss: 85.6727\n",
      "Epoch 162/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 86.6216 - val_loss: 85.6093\n",
      "Epoch 163/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 85.6494 - val_loss: 85.5441\n",
      "Epoch 164/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 86.6272 - val_loss: 85.4767\n",
      "Epoch 165/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 86.8689 - val_loss: 85.4053\n",
      "Epoch 166/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 85.9186 - val_loss: 85.3348\n",
      "Epoch 167/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 86.9654 - val_loss: 85.2725\n",
      "Epoch 168/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 86.1213 - val_loss: 85.2207\n",
      "Epoch 169/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 85.8247 - val_loss: 85.1727\n",
      "Epoch 170/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 85.9783 - val_loss: 85.1160\n",
      "Epoch 171/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 85.1103 - val_loss: 85.0556\n",
      "Epoch 172/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 85.7253 - val_loss: 84.9886\n",
      "Epoch 173/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 85.3470 - val_loss: 84.9186\n",
      "Epoch 174/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 86.5571 - val_loss: 84.8474\n",
      "Epoch 175/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 85.2970 - val_loss: 84.7778\n",
      "Epoch 176/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 85.0665 - val_loss: 84.7079\n",
      "Epoch 177/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 85.6187 - val_loss: 84.6393\n",
      "Epoch 178/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 85.7267 - val_loss: 84.5708\n",
      "Epoch 179/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 84.8775 - val_loss: 84.5030\n",
      "Epoch 180/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 85.1800 - val_loss: 84.4374\n",
      "Epoch 181/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 86.2991 - val_loss: 84.3729\n",
      "Epoch 182/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 85.6012 - val_loss: 84.3144\n",
      "Epoch 183/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 85.7318 - val_loss: 84.2532\n",
      "Epoch 184/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 85.5420 - val_loss: 84.1925\n",
      "Epoch 185/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 85.6229 - val_loss: 84.1303\n",
      "Epoch 186/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 85.4864 - val_loss: 84.0682\n",
      "Epoch 187/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 85.4606 - val_loss: 84.0031\n",
      "Epoch 188/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 84.7104 - val_loss: 83.9395\n",
      "Epoch 189/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 84.0953 - val_loss: 83.8731\n",
      "Epoch 190/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 84.5445 - val_loss: 83.8036\n",
      "Epoch 191/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 84.8690 - val_loss: 83.7408\n",
      "Epoch 192/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 85.2110 - val_loss: 83.6810\n",
      "Epoch 193/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 84.7904 - val_loss: 83.6251\n",
      "Epoch 194/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 84.1202 - val_loss: 83.5660\n",
      "Epoch 195/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 84.5593 - val_loss: 83.5102\n",
      "Epoch 196/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 83.7486 - val_loss: 83.4527\n",
      "Epoch 197/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 84.2626 - val_loss: 83.3933\n",
      "Epoch 198/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 84.2644 - val_loss: 83.3319\n",
      "Epoch 199/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 83.4321 - val_loss: 83.2684\n",
      "Epoch 200/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 83.2682 - val_loss: 83.1966\n",
      "Epoch 201/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 83.6550 - val_loss: 83.1252\n",
      "Epoch 202/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 83.6347 - val_loss: 83.0537\n",
      "Epoch 203/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 83.7535 - val_loss: 82.9843\n",
      "Epoch 204/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 83.9850 - val_loss: 82.9160\n",
      "Epoch 205/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 83.8698 - val_loss: 82.8500\n",
      "Epoch 206/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 83.6518 - val_loss: 82.7842\n",
      "Epoch 207/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 83.2997 - val_loss: 82.7179\n",
      "Epoch 208/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 84.0561 - val_loss: 82.6516\n",
      "Epoch 209/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 83.9984 - val_loss: 82.5853\n",
      "Epoch 210/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 83.7967 - val_loss: 82.5229\n",
      "Epoch 211/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 83.3923 - val_loss: 82.4618\n",
      "Epoch 212/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 83.3856 - val_loss: 82.4012\n",
      "Epoch 213/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 82.2794 - val_loss: 82.3426\n",
      "Epoch 214/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 82.9831 - val_loss: 82.2853\n",
      "Epoch 215/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 83.1098 - val_loss: 82.2289\n",
      "Epoch 216/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 82.0027 - val_loss: 82.1669\n",
      "Epoch 217/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 83.0045 - val_loss: 82.1049\n",
      "Epoch 218/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 83.2293 - val_loss: 82.0425\n",
      "Epoch 219/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 83.3773 - val_loss: 81.9761\n",
      "Epoch 220/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 83.2194 - val_loss: 81.9090\n",
      "Epoch 221/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 82.9701 - val_loss: 81.8400\n",
      "Epoch 222/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 82.4420 - val_loss: 81.7673\n",
      "Epoch 223/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 82.5791 - val_loss: 81.6943\n",
      "Epoch 224/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 83.8372 - val_loss: 81.6279\n",
      "Epoch 225/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 82.3992 - val_loss: 81.5639\n",
      "Epoch 226/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 81.5742 - val_loss: 81.5082\n",
      "Epoch 227/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 82.7262 - val_loss: 81.4544\n",
      "Epoch 228/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 82.1663 - val_loss: 81.3969\n",
      "Epoch 229/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 83.5961 - val_loss: 81.3428\n",
      "Epoch 230/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 82.7967 - val_loss: 81.2873\n",
      "Epoch 231/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 81.9495 - val_loss: 81.2251\n",
      "Epoch 232/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 81.9800 - val_loss: 81.1603\n",
      "Epoch 233/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 81.5056 - val_loss: 81.0950\n",
      "Epoch 234/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 82.4003 - val_loss: 81.0197\n",
      "Epoch 235/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 81.5455 - val_loss: 80.9467\n",
      "Epoch 236/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 80.9864 - val_loss: 80.8750\n",
      "Epoch 237/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 81.8734 - val_loss: 80.8116\n",
      "Epoch 238/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 81.3670 - val_loss: 80.7536\n",
      "Epoch 239/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 80.9010 - val_loss: 80.6971\n",
      "Epoch 240/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 81.5266 - val_loss: 80.6384\n",
      "Epoch 241/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 81.8771 - val_loss: 80.5781\n",
      "Epoch 242/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 81.7247 - val_loss: 80.5158\n",
      "Epoch 243/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 81.2518 - val_loss: 80.4511\n",
      "Epoch 244/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 81.6283 - val_loss: 80.3794\n",
      "Epoch 245/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 81.1953 - val_loss: 80.3081\n",
      "Epoch 246/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 80.8069 - val_loss: 80.2341\n",
      "Epoch 247/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 81.7880 - val_loss: 80.1656\n",
      "Epoch 248/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 82.1183 - val_loss: 80.1040\n",
      "Epoch 249/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 81.0734 - val_loss: 80.0469\n",
      "Epoch 250/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 81.1219 - val_loss: 79.9940\n",
      "Epoch 251/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 81.2320 - val_loss: 79.9380\n",
      "Epoch 252/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 80.5413 - val_loss: 79.8769\n",
      "Epoch 253/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 80.4751 - val_loss: 79.8096\n",
      "Epoch 254/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 80.9682 - val_loss: 79.7404\n",
      "Epoch 255/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 81.3683 - val_loss: 79.6709\n",
      "Epoch 256/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 80.3189 - val_loss: 79.6028\n",
      "Epoch 257/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 80.7448 - val_loss: 79.5394\n",
      "Epoch 258/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 79.6765 - val_loss: 79.4828\n",
      "Epoch 259/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 80.1016 - val_loss: 79.4296\n",
      "Epoch 260/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 80.1584 - val_loss: 79.3750\n",
      "Epoch 261/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 79.7223 - val_loss: 79.3198\n",
      "Epoch 262/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 79.7670 - val_loss: 79.2603\n",
      "Epoch 263/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 79.4485 - val_loss: 79.2052\n",
      "Epoch 264/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 80.0714 - val_loss: 79.1472\n",
      "Epoch 265/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 80.5921 - val_loss: 79.0850\n",
      "Epoch 266/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 80.2269 - val_loss: 79.0232\n",
      "Epoch 267/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 81.0868 - val_loss: 78.9584\n",
      "Epoch 268/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 79.3511 - val_loss: 78.8920\n",
      "Epoch 269/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 79.9941 - val_loss: 78.8177\n",
      "Epoch 270/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 79.2524 - val_loss: 78.7400\n",
      "Epoch 271/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 79.0775 - val_loss: 78.6655\n",
      "Epoch 272/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 79.5204 - val_loss: 78.5964\n",
      "Epoch 273/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 78.1464 - val_loss: 78.5270\n",
      "Epoch 274/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 78.0457 - val_loss: 78.4607\n",
      "Epoch 275/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 78.9022 - val_loss: 78.3953\n",
      "Epoch 276/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 77.8674 - val_loss: 78.3297\n",
      "Epoch 277/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 78.6098 - val_loss: 78.2652\n",
      "Epoch 278/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 79.4404 - val_loss: 78.1998\n",
      "Epoch 279/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 79.9968 - val_loss: 78.1309\n",
      "Epoch 280/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 78.2426 - val_loss: 78.0622\n",
      "Epoch 281/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 78.8773 - val_loss: 77.9949\n",
      "Epoch 282/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 79.1617 - val_loss: 77.9326\n",
      "Epoch 283/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 78.2028 - val_loss: 77.8743\n",
      "Epoch 284/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 78.4432 - val_loss: 77.8168\n",
      "Epoch 285/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 78.6964 - val_loss: 77.7648\n",
      "Epoch 286/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 78.3138 - val_loss: 77.7076\n",
      "Epoch 287/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 78.8836 - val_loss: 77.6430\n",
      "Epoch 288/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 79.4316 - val_loss: 77.5753\n",
      "Epoch 289/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 78.9434 - val_loss: 77.5063\n",
      "Epoch 290/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 77.2347 - val_loss: 77.4429\n",
      "Epoch 291/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 77.5013 - val_loss: 77.3810\n",
      "Epoch 292/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 77.2870 - val_loss: 77.3208\n",
      "Epoch 293/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 78.3819 - val_loss: 77.2588\n",
      "Epoch 294/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 78.0734 - val_loss: 77.1947\n",
      "Epoch 295/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 77.4723 - val_loss: 77.1306\n",
      "Epoch 296/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 76.6418 - val_loss: 77.0620\n",
      "Epoch 297/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 78.6507 - val_loss: 76.9953\n",
      "Epoch 298/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 77.2453 - val_loss: 76.9313\n",
      "Epoch 299/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 77.5669 - val_loss: 76.8701\n",
      "Epoch 300/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 77.4975 - val_loss: 76.8125\n",
      "Epoch 301/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 77.1090 - val_loss: 76.7544\n",
      "Epoch 302/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 78.0913 - val_loss: 76.6957\n",
      "Epoch 303/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 76.4162 - val_loss: 76.6352\n",
      "Epoch 304/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 76.9799 - val_loss: 76.5726\n",
      "Epoch 305/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 76.8298 - val_loss: 76.5080\n",
      "Epoch 306/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 77.9221 - val_loss: 76.4446\n",
      "Epoch 307/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 77.4614 - val_loss: 76.3866\n",
      "Epoch 308/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 78.0778 - val_loss: 76.3272\n",
      "Epoch 309/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 76.5254 - val_loss: 76.2613\n",
      "Epoch 310/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 76.4466 - val_loss: 76.1923\n",
      "Epoch 311/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 76.9613 - val_loss: 76.1199\n",
      "Epoch 312/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 76.6698 - val_loss: 76.0519\n",
      "Epoch 313/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 75.2574 - val_loss: 75.9830\n",
      "Epoch 314/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 76.1658 - val_loss: 75.9180\n",
      "Epoch 315/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 76.7628 - val_loss: 75.8522\n",
      "Epoch 316/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 77.3284 - val_loss: 75.7877\n",
      "Epoch 317/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 77.1712 - val_loss: 75.7253\n",
      "Epoch 318/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 77.0396 - val_loss: 75.6642\n",
      "Epoch 319/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 76.6175 - val_loss: 75.6044\n",
      "Epoch 320/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 76.8597 - val_loss: 75.5456\n",
      "Epoch 321/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 77.1450 - val_loss: 75.4847\n",
      "Epoch 322/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 76.4144 - val_loss: 75.4247\n",
      "Epoch 323/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 75.3876 - val_loss: 75.3637\n",
      "Epoch 324/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 75.9336 - val_loss: 75.3011\n",
      "Epoch 325/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 77.4417 - val_loss: 75.2356\n",
      "Epoch 326/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 75.6926 - val_loss: 75.1702\n",
      "Epoch 327/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 76.3894 - val_loss: 75.1060\n",
      "Epoch 328/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 74.0814 - val_loss: 75.0405\n",
      "Epoch 329/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 78.0516 - val_loss: 74.9735\n",
      "Epoch 330/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 76.1794 - val_loss: 74.9102\n",
      "Epoch 331/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 75.8874 - val_loss: 74.8471\n",
      "Epoch 332/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 76.3828 - val_loss: 74.7863\n",
      "Epoch 333/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 75.7898 - val_loss: 74.7289\n",
      "Epoch 334/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 76.6364 - val_loss: 74.6737\n",
      "Epoch 335/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 75.5998 - val_loss: 74.6162\n",
      "Epoch 336/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 75.8825 - val_loss: 74.5586\n",
      "Epoch 337/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 75.1560 - val_loss: 74.5013\n",
      "Epoch 338/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 74.6505 - val_loss: 74.4435\n",
      "Epoch 339/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 76.9029 - val_loss: 74.3820\n",
      "Epoch 340/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 76.5785 - val_loss: 74.3203\n",
      "Epoch 341/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 73.7867 - val_loss: 74.2549\n",
      "Epoch 342/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 74.4248 - val_loss: 74.1928\n",
      "Epoch 343/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 75.7368 - val_loss: 74.1327\n",
      "Epoch 344/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 75.5685 - val_loss: 74.0741\n",
      "Epoch 345/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 72.7832 - val_loss: 74.0141\n",
      "Epoch 346/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 75.6386 - val_loss: 73.9521\n",
      "Epoch 347/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 73.9165 - val_loss: 73.8885\n",
      "Epoch 348/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 74.7825 - val_loss: 73.8230\n",
      "Epoch 349/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 74.3776 - val_loss: 73.7560\n",
      "Epoch 350/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 73.8150 - val_loss: 73.6870\n",
      "Epoch 351/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 76.4853 - val_loss: 73.6193\n",
      "Epoch 352/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 75.7190 - val_loss: 73.5572\n",
      "Epoch 353/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 74.3558 - val_loss: 73.4996\n",
      "Epoch 354/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 74.4390 - val_loss: 73.4383\n",
      "Epoch 355/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 74.2344 - val_loss: 73.3837\n",
      "Epoch 356/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 76.1379 - val_loss: 73.3317\n",
      "Epoch 357/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 74.3407 - val_loss: 73.2738\n",
      "Epoch 358/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 74.3059 - val_loss: 73.2120\n",
      "Epoch 359/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 73.6788 - val_loss: 73.1464\n",
      "Epoch 360/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 74.2143 - val_loss: 73.0779\n",
      "Epoch 361/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 74.1741 - val_loss: 73.0075\n",
      "Epoch 362/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 75.4996 - val_loss: 72.9426\n",
      "Epoch 363/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 72.6893 - val_loss: 72.8759\n",
      "Epoch 364/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 73.4111 - val_loss: 72.8127\n",
      "Epoch 365/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 73.4943 - val_loss: 72.7555\n",
      "Epoch 366/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 73.8616 - val_loss: 72.6970\n",
      "Epoch 367/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 73.8441 - val_loss: 72.6354\n",
      "Epoch 368/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 74.1898 - val_loss: 72.5737\n",
      "Epoch 369/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 75.8855 - val_loss: 72.5127\n",
      "Epoch 370/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 71.9329 - val_loss: 72.4501\n",
      "Epoch 371/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 74.3544 - val_loss: 72.3846\n",
      "Epoch 372/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 74.1409 - val_loss: 72.3166\n",
      "Epoch 373/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 71.9670 - val_loss: 72.2521\n",
      "Epoch 374/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 72.5646 - val_loss: 72.1893\n",
      "Epoch 375/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 72.2298 - val_loss: 72.1279\n",
      "Epoch 376/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 70.4510 - val_loss: 72.0694\n",
      "Epoch 377/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 73.0850 - val_loss: 72.0103\n",
      "Epoch 378/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 72.5736 - val_loss: 71.9526\n",
      "Epoch 379/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 71.6273 - val_loss: 71.8953\n",
      "Epoch 380/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 75.1600 - val_loss: 71.8391\n",
      "Epoch 381/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 73.2325 - val_loss: 71.7846\n",
      "Epoch 382/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 71.9541 - val_loss: 71.7280\n",
      "Epoch 383/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 72.6797 - val_loss: 71.6675\n",
      "Epoch 384/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 71.9613 - val_loss: 71.6053\n",
      "Epoch 385/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 73.0999 - val_loss: 71.5445\n",
      "Epoch 386/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 73.3063 - val_loss: 71.4839\n",
      "Epoch 387/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 73.1744 - val_loss: 71.4280\n",
      "Epoch 388/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 72.2526 - val_loss: 71.3675\n",
      "Epoch 389/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 72.2553 - val_loss: 71.2978\n",
      "Epoch 390/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 71.9647 - val_loss: 71.2221\n",
      "Epoch 391/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 72.2006 - val_loss: 71.1588\n",
      "Epoch 392/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 72.3594 - val_loss: 71.1018\n",
      "Epoch 393/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 72.4524 - val_loss: 71.0327\n",
      "Epoch 394/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 72.7379 - val_loss: 70.9730\n",
      "Epoch 395/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 73.5052 - val_loss: 70.9138\n",
      "Epoch 396/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 71.3367 - val_loss: 70.8572\n",
      "Epoch 397/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 71.6577 - val_loss: 70.8007\n",
      "Epoch 398/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 71.7932 - val_loss: 70.7449\n",
      "Epoch 399/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 70.4628 - val_loss: 70.6881\n",
      "Epoch 400/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 71.7185 - val_loss: 70.6304\n",
      "Epoch 401/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 71.6930 - val_loss: 70.5712\n",
      "Epoch 402/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 71.6370 - val_loss: 70.5085\n",
      "Epoch 403/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 70.2595 - val_loss: 70.4462\n",
      "Epoch 404/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 71.4538 - val_loss: 70.3857\n",
      "Epoch 405/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 69.6864 - val_loss: 70.3249\n",
      "Epoch 406/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 70.7464 - val_loss: 70.2627\n",
      "Epoch 407/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 68.9108 - val_loss: 70.2037\n",
      "Epoch 408/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 70.5924 - val_loss: 70.1403\n",
      "Epoch 409/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 71.1784 - val_loss: 70.0819\n",
      "Epoch 410/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 70.2329 - val_loss: 70.0200\n",
      "Epoch 411/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 70.6592 - val_loss: 69.9516\n",
      "Epoch 412/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 69.5004 - val_loss: 69.8873\n",
      "Epoch 413/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 69.9925 - val_loss: 69.8228\n",
      "Epoch 414/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 69.6185 - val_loss: 69.7541\n",
      "Epoch 415/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 69.4877 - val_loss: 69.6827\n",
      "Epoch 416/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 70.6622 - val_loss: 69.6111\n",
      "Epoch 417/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 70.3966 - val_loss: 69.5422\n",
      "Epoch 418/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 69.8401 - val_loss: 69.4813\n",
      "Epoch 419/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 69.7238 - val_loss: 69.4229\n",
      "Epoch 420/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 70.9233 - val_loss: 69.3649\n",
      "Epoch 421/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 71.2064 - val_loss: 69.3061\n",
      "Epoch 422/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 70.5870 - val_loss: 69.2401\n",
      "Epoch 423/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 68.9430 - val_loss: 69.1722\n",
      "Epoch 424/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 68.8650 - val_loss: 69.1039\n",
      "Epoch 425/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 67.0536 - val_loss: 69.0427\n",
      "Epoch 426/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 67.7452 - val_loss: 68.9835\n",
      "Epoch 427/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 70.6872 - val_loss: 68.9280\n",
      "Epoch 428/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 69.8672 - val_loss: 68.8674\n",
      "Epoch 429/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 70.1889 - val_loss: 68.8078\n",
      "Epoch 430/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 69.4651 - val_loss: 68.7490\n",
      "Epoch 431/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 67.7889 - val_loss: 68.6941\n",
      "Epoch 432/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 70.3386 - val_loss: 68.6388\n",
      "Epoch 433/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 69.7970 - val_loss: 68.5854\n",
      "Epoch 434/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 68.0873 - val_loss: 68.5272\n",
      "Epoch 435/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 69.5857 - val_loss: 68.4629\n",
      "Epoch 436/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 69.7042 - val_loss: 68.3958\n",
      "Epoch 437/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 70.3784 - val_loss: 68.3215\n",
      "Epoch 438/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 67.0709 - val_loss: 68.2452\n",
      "Epoch 439/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 68.5744 - val_loss: 68.1705\n",
      "Epoch 440/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 67.7190 - val_loss: 68.1009\n",
      "Epoch 441/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 67.9772 - val_loss: 68.0289\n",
      "Epoch 442/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 68.6797 - val_loss: 67.9608\n",
      "Epoch 443/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 68.5699 - val_loss: 67.8977\n",
      "Epoch 444/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 67.9041 - val_loss: 67.8380\n",
      "Epoch 445/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 67.6107 - val_loss: 67.7792\n",
      "Epoch 446/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 68.3142 - val_loss: 67.7185\n",
      "Epoch 447/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 68.2938 - val_loss: 67.6570\n",
      "Epoch 448/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 68.9076 - val_loss: 67.5972\n",
      "Epoch 449/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 68.7640 - val_loss: 67.5372\n",
      "Epoch 450/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 67.7049 - val_loss: 67.4782\n",
      "Epoch 451/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 66.7981 - val_loss: 67.4177\n",
      "Epoch 452/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 68.7123 - val_loss: 67.3542\n",
      "Epoch 453/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 70.5400 - val_loss: 67.2919\n",
      "Epoch 454/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 66.9722 - val_loss: 67.2285\n",
      "Epoch 455/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 68.6036 - val_loss: 67.1646\n",
      "Epoch 456/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 67.5553 - val_loss: 67.0981\n",
      "Epoch 457/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 67.6448 - val_loss: 67.0309\n",
      "Epoch 458/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 69.1577 - val_loss: 66.9661\n",
      "Epoch 459/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 68.3794 - val_loss: 66.9035\n",
      "Epoch 460/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 66.9079 - val_loss: 66.8413\n",
      "Epoch 461/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 67.4958 - val_loss: 66.7773\n",
      "Epoch 462/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 69.0324 - val_loss: 66.7125\n",
      "Epoch 463/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 67.1090 - val_loss: 66.6472\n",
      "Epoch 464/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 67.6248 - val_loss: 66.5841\n",
      "Epoch 465/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 67.4167 - val_loss: 66.5202\n",
      "Epoch 466/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 66.8799 - val_loss: 66.4586\n",
      "Epoch 467/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 68.4922 - val_loss: 66.3976\n",
      "Epoch 468/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 65.3999 - val_loss: 66.3383\n",
      "Epoch 469/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 68.4885 - val_loss: 66.2792\n",
      "Epoch 470/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 67.4126 - val_loss: 66.2218\n",
      "Epoch 471/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 66.4594 - val_loss: 66.1681\n",
      "Epoch 472/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 67.2564 - val_loss: 66.1169\n",
      "Epoch 473/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 68.6666 - val_loss: 66.0638\n",
      "Epoch 474/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 67.2552 - val_loss: 66.0099\n",
      "Epoch 475/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 66.2745 - val_loss: 65.9553\n",
      "Epoch 476/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 69.3708 - val_loss: 65.8987\n",
      "Epoch 477/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 68.4258 - val_loss: 65.8426\n",
      "Epoch 478/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 65.6792 - val_loss: 65.7833\n",
      "Epoch 479/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 65.2717 - val_loss: 65.7213\n",
      "Epoch 480/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 67.0243 - val_loss: 65.6565\n",
      "Epoch 481/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 66.7228 - val_loss: 65.5916\n",
      "Epoch 482/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 67.8310 - val_loss: 65.5268\n",
      "Epoch 483/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 67.7395 - val_loss: 65.4643\n",
      "Epoch 484/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 63.9096 - val_loss: 65.4022\n",
      "Epoch 485/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 65.3701 - val_loss: 65.3397\n",
      "Epoch 486/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 67.6430 - val_loss: 65.2756\n",
      "Epoch 487/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 65.9078 - val_loss: 65.2108\n",
      "Epoch 488/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 68.1754 - val_loss: 65.1471\n",
      "Epoch 489/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 64.1828 - val_loss: 65.0838\n",
      "Epoch 490/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 67.1633 - val_loss: 65.0222\n",
      "Epoch 491/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 67.2994 - val_loss: 64.9618\n",
      "Epoch 492/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 65.1455 - val_loss: 64.9032\n",
      "Epoch 493/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 67.1596 - val_loss: 64.8450\n",
      "Epoch 494/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 67.3151 - val_loss: 64.7827\n",
      "Epoch 495/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 63.7463 - val_loss: 64.7196\n",
      "Epoch 496/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 66.5877 - val_loss: 64.6538\n",
      "Epoch 497/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 67.2396 - val_loss: 64.5911\n",
      "Epoch 498/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 65.8790 - val_loss: 64.5296\n",
      "Epoch 499/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 66.2945 - val_loss: 64.4714\n",
      "Epoch 500/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 64.4987 - val_loss: 64.4120\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x, y, validation_data=(vx, vy), epochs=epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "3406b84d-f3d9-41ff-91d9-1ac28687a789",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "a01fa082-b2d1-4328-87c0-92305ddc7f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[47,  4, 46, 35,  6, 66, 25, 20, 20, 59, 17, 61,  3, 81, 92, 43,\n",
       "        98, 84, 16, 49],\n",
       "       [59,  6, 56, 95, 81, 41, 90, 98, 99, 28, 74, 27, 40, 44, 88, 82,\n",
       "        34, 35, 52, 97],\n",
       "       [29, 78, 51, 59, 23, 50, 42, 72, 96, 62, 44, 90, 87, 61, 85, 13,\n",
       "        23, 44, 87, 36],\n",
       "       [94, 56, 42, 58, 13, 69, 52, 21, 20, 84, 33, 66, 88, 91, 75, 11,\n",
       "        76, 36, 87, 94],\n",
       "       [10, 74, 39, 99, 41, 93, 37, 95,  9, 77, 72, 63, 84, 31, 99, 63,\n",
       "        94, 37,  3, 28]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "79f3652d-04c7-4fa2-9d5b-2207ec6ab9fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([98, 99, 96, 94, 99])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "53cdbfe0-effd-4fc7-92c3-610393c316f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[32.140488, 31.596304, 30.737673, 32.544765, 30.967756, 32.502914,\n",
       "        30.81377 , 31.895735, 33.349293, 31.751057, 30.733702, 32.056904,\n",
       "        31.574518, 30.71697 , 32.0162  , 29.719246, 31.637049, 31.282038,\n",
       "        32.618504, 31.25456 , 30.50701 , 31.72131 , 30.528643, 30.808674,\n",
       "        31.128302, 32.584743, 30.95209 , 31.915285, 32.5556  , 32.076477,\n",
       "        30.682821, 32.31818 , 31.379875, 30.952822, 31.354685, 31.11815 ,\n",
       "        30.438461, 31.566011, 31.844831, 31.607655, 31.682974, 30.332716,\n",
       "        30.720293, 32.59922 , 31.113966, 31.043636, 29.973253, 32.110764,\n",
       "        31.531376, 30.71237 , 32.79897 , 30.894453, 30.57506 , 31.708084,\n",
       "        31.506144, 32.604286, 31.394396, 30.877941, 32.274048, 31.604258,\n",
       "        30.891043, 30.995413, 31.82604 , 32.312668, 32.039818, 31.06433 ,\n",
       "        32.422234, 30.516771, 32.598545, 31.704067, 32.62653 , 30.887737,\n",
       "        32.334496, 31.352152, 31.104177, 31.910337, 31.649847, 31.544945,\n",
       "        31.999865, 30.358385, 31.495918, 30.830761, 29.292353, 31.920053,\n",
       "        31.095861, 32.00981 , 32.474888, 30.891235, 31.578356, 31.51984 ,\n",
       "        30.755404, 31.872124, 32.073257, 31.918625, 32.631786, 31.387217,\n",
       "        30.950953, 30.750967, 30.328438, 32.874317],\n",
       "       [32.140503, 31.596367, 30.7374  , 32.544895, 30.967747, 32.50279 ,\n",
       "        30.813665, 31.89571 , 33.349003, 31.751076, 30.733692, 32.05699 ,\n",
       "        31.574417, 30.717167, 32.016068, 29.719183, 31.636713, 31.282219,\n",
       "        32.618206, 31.254425, 30.506994, 31.721455, 30.528624, 30.80876 ,\n",
       "        31.127981, 32.58461 , 30.951925, 31.915625, 32.555515, 32.076412,\n",
       "        30.682682, 32.31803 , 31.379705, 30.95249 , 31.354816, 31.118082,\n",
       "        30.438517, 31.566092, 31.844847, 31.607534, 31.683098, 30.332268,\n",
       "        30.720419, 32.599155, 31.114132, 31.043657, 29.973452, 32.11083 ,\n",
       "        31.531267, 30.712431, 32.798798, 30.894062, 30.574862, 31.707697,\n",
       "        31.506126, 32.604458, 31.394548, 30.877953, 32.27398 , 31.60446 ,\n",
       "        30.891283, 30.995424, 31.82588 , 32.31287 , 32.03965 , 31.063997,\n",
       "        32.422085, 30.516474, 32.59806 , 31.703861, 32.626488, 30.887917,\n",
       "        32.334072, 31.351887, 31.103882, 31.910118, 31.650179, 31.544682,\n",
       "        31.999624, 30.358675, 31.495728, 30.830442, 29.292534, 31.920214,\n",
       "        31.095604, 32.00985 , 32.474606, 30.891306, 31.578503, 31.519642,\n",
       "        30.755323, 31.872004, 32.073097, 31.918467, 32.631863, 31.387007,\n",
       "        30.951117, 30.751005, 30.32792 , 32.874157],\n",
       "       [32.1405  , 31.596334, 30.737478, 32.544838, 30.967745, 32.502808,\n",
       "        30.813705, 31.895721, 33.349094, 31.75106 , 30.733671, 32.056965,\n",
       "        31.574442, 30.717098, 32.016113, 29.719204, 31.636793, 31.282148,\n",
       "        32.61828 , 31.254457, 30.506985, 31.72141 , 30.528616, 30.80874 ,\n",
       "        31.128086, 32.58465 , 30.95198 , 31.915522, 32.555534, 32.076435,\n",
       "        30.68271 , 32.318077, 31.379766, 30.952593, 31.354778, 31.118086,\n",
       "        30.438505, 31.566063, 31.844835, 31.607553, 31.68307 , 30.332384,\n",
       "        30.72038 , 32.599163, 31.114065, 31.043633, 29.973394, 32.110786,\n",
       "        31.53129 , 30.71242 , 32.79884 , 30.894178, 30.574915, 31.707808,\n",
       "        31.506136, 32.604404, 31.39451 , 30.877943, 32.273994, 31.604399,\n",
       "        30.891214, 30.995405, 31.825935, 32.312798, 32.039684, 31.0641  ,\n",
       "        32.42214 , 30.516567, 32.598198, 31.703909, 32.626503, 30.887863,\n",
       "        32.334198, 31.351948, 31.103971, 31.910173, 31.65008 , 31.544752,\n",
       "        31.999697, 30.358595, 31.495764, 30.830542, 29.292492, 31.920158,\n",
       "        31.095673, 32.00982 , 32.47469 , 30.891272, 31.578444, 31.519691,\n",
       "        30.755346, 31.872055, 32.073135, 31.918522, 32.631844, 31.387058,\n",
       "        30.951052, 30.750996, 30.32808 , 32.874187],\n",
       "       [32.140503, 31.596357, 30.737446, 32.544872, 30.967749, 32.502804,\n",
       "        30.813683, 31.895716, 33.349052, 31.751074, 30.733688, 32.05698 ,\n",
       "        31.574436, 30.71713 , 32.016094, 29.719194, 31.636765, 31.282183,\n",
       "        32.61825 , 31.254446, 30.506996, 31.72143 , 30.528625, 30.808748,\n",
       "        31.128042, 32.584633, 30.951956, 31.915567, 32.555527, 32.076427,\n",
       "        30.682701, 32.318058, 31.37974 , 30.952547, 31.3548  , 31.118088,\n",
       "        30.438513, 31.56608 , 31.844843, 31.607552, 31.68308 , 30.332338,\n",
       "        30.720398, 32.599167, 31.1141  , 31.043648, 29.973421, 32.110817,\n",
       "        31.531282, 30.712423, 32.798824, 30.894125, 30.574892, 31.707762,\n",
       "        31.506128, 32.60443 , 31.394526, 30.877953, 32.27399 , 31.604427,\n",
       "        30.891245, 30.995419, 31.825909, 32.312836, 32.039673, 31.064058,\n",
       "        32.422115, 30.516527, 32.59814 , 31.703892, 32.626495, 30.88789 ,\n",
       "        32.33414 , 31.351927, 31.103935, 31.910154, 31.650126, 31.544724,\n",
       "        31.999662, 30.35863 , 31.495754, 30.830498, 29.292511, 31.92019 ,\n",
       "        31.095642, 32.00984 , 32.474655, 30.89129 , 31.578478, 31.519674,\n",
       "        30.755337, 31.872032, 32.07312 , 31.918497, 32.63185 , 31.387037,\n",
       "        30.951084, 30.750996, 30.328007, 32.87418 ],\n",
       "       [32.140507, 31.596367, 30.737448, 32.54488 , 30.967752, 32.50282 ,\n",
       "        30.813671, 31.895712, 33.34905 , 31.751081, 30.733706, 32.056976,\n",
       "        31.574438, 30.717134, 32.016083, 29.71919 , 31.636782, 31.28219 ,\n",
       "        32.618263, 31.25445 , 30.507006, 31.721426, 30.528631, 30.808737,\n",
       "        31.12803 , 32.584633, 30.951948, 31.915564, 32.555527, 32.076424,\n",
       "        30.682707, 32.318054, 31.379728, 30.95254 , 31.354792, 31.118093,\n",
       "        30.438505, 31.566078, 31.844845, 31.607563, 31.68307 , 30.332357,\n",
       "        30.720392, 32.59918 , 31.114105, 31.04366 , 29.973412, 32.110832,\n",
       "        31.53129 , 30.712408, 32.798832, 30.894125, 30.5749  , 31.707766,\n",
       "        31.50612 , 32.604427, 31.39451 , 30.877956, 32.273994, 31.604427,\n",
       "        30.891237, 30.995428, 31.825901, 32.31284 , 32.03968 , 31.064053,\n",
       "        32.422104, 30.516523, 32.59815 , 31.703901, 32.626488, 30.887884,\n",
       "        32.33414 , 31.351944, 31.103937, 31.91016 , 31.650118, 31.54473 ,\n",
       "        31.999659, 30.358618, 31.495771, 30.830488, 29.292501, 31.92019 ,\n",
       "        31.095644, 32.00985 , 32.474655, 30.891302, 31.57849 , 31.51968 ,\n",
       "        30.755335, 31.872019, 32.07313 , 31.91849 , 32.631847, 31.387045,\n",
       "        30.951094, 30.750986, 30.328005, 32.874187]], dtype=float32)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "92fc19e8-4331-47cc-b055-6d48dd6aa9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([0, 0, 0, 0, 0])>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tf.argmax(model.predict(x[:5]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "e0419e46-04dc-4ee5-ba94-0665e4a61276",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "169bfd38-be4c-4693-ae6e-574da519677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = gen_data(batch_size=30, length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670ae1a1-56ad-4b63-89fe-4e7ba70187c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([90, 92, 76, 95, 81])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "435adf74-c8b0-490d-a52b-171360d0a396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([0, 0, 0, 0, 0])>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(model.predict(x[:5]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeec3f6-76fe-4329-923b-a4d667463a24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87276ce9-8691-4143-85de-6eb04f9ff869",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
