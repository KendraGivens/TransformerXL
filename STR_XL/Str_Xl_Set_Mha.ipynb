{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ccea5cd-89de-452b-afa6-703df9e9a604",
   "metadata": {},
   "source": [
    "---\n",
    "# Transformer XL\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2c14f8-8dd8-459f-81fe-a6708e6a8355",
   "metadata": {},
   "source": [
    "---\n",
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dcad43d-67f7-49b9-84ce-6d91a0f468a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e7cb9df-5746-44a3-998a-b671b284957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c507a85-fe6f-4d3c-ae1f-16914fdec693",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../../deep-learning-dna\")\n",
    "sys.path.append(\"../../settransformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fda64db1-8058-4e15-b2d4-1a3e64f23fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "import math\n",
    "import string\n",
    "\n",
    "import settransformer as stf\n",
    "from common.models import dnabert\n",
    "from common import dna\n",
    "from lmdbm import Lmdb\n",
    "from common.data import DnaSequenceGenerator, DnaLabelType, DnaSampleGenerator, find_dbs\n",
    "import wandb\n",
    "\n",
    "import tf_utils as tfu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dfb3b1a-2db7-4463-a208-b5d29e20f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tfu.devices.select_gpu(0, use_dynamic_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54593a0f-b5b4-401e-91e2-8c1a1598432e",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "153ec0c2-f4ac-418f-93a5-7f8b21b286ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<common.models.dnabert.DnaBertPretrainModel at 0x7f508764be20>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import pretrained model\n",
    "api = wandb.Api()\n",
    "model_path = api.artifact(\"sirdavidludwig/deep-learning-dna/dnabert-pretrain-ablation-dim:8dim\").download()\n",
    "pretrained_model = dnabert.DnaBertModel.load(model_path)\n",
    "pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4eb8a00e-6821-4a74-87cb-ecd6c405d3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact dnasamples:v1, 4086.55MB. 1260 files... Done. 0:0:0.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/data/dna_samples:v1/train/WS-CCW-Jul2015_S82_L001_R1_001.db'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load datafiles\n",
    "dataset_path = api.artifact(\"sirdavidludwig/nachusa-dna/dnasamples:v1\").download('/data/dna_samples:v1')\n",
    "samples = find_dbs(dataset_path + '/train')\n",
    "samples[13]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6523647-1042-4dc6-aa46-1ad979f03892",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a14bf26f-f2d8-4907-991e-7dcbbc68f206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_len = 400\n",
    "sequence_len = 150\n",
    "kmer = 3\n",
    "batch_size = 20\n",
    "batches_per_epoch = 20\n",
    "augument = True\n",
    "labels = DnaLabelType.SampleIds\n",
    "seed = 0\n",
    "rng = np.random.default_rng(seed)\n",
    "random_samples = samples.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1116c1ae-77e7-44c9-a010-efb23c23db48",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DnaSampleGenerator(samples=samples[0:5], subsample_length = set_len, sequence_length=sequence_len,kmer=kmer,batch_size=batch_size,batches_per_epoch=batches_per_epoch,augment=augument,labels=labels, rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d827ca7f-1900-4e88-a514-ae29daf45858",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng.shuffle(random_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aac0a881-4dc6-4916-a18c-8c59e026e552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/data/dna_samples:v1/train/WS-MU-Oct2016_S71_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/WS-HPN-Jul2016_S78_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/Wes21-8-TCR_S22_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/WS-CCE-Sep2015_S60_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/Wesley056-NegCtrl_S195_L001_R1_001.db']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_samples[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd855356-4e5a-4047-ae81-323277b11b44",
   "metadata": {
    "tags": []
   },
   "source": [
    "--- \n",
    "# Batch Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e23e34f9-3066-4756-ad0b-ccc1a3194438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 200\n",
    "seq_len = set_len\n",
    "maxlen = set_len\n",
    "vocab_size = 5\n",
    "num_chars_data = set_len*sequence_len\n",
    "max_files = len(dataset.samples)\n",
    "max_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e768970c-7af4-4626-9d6e-d712ab7de959",
   "metadata": {},
   "outputs": [],
   "source": [
    "if block_size-2 > seq_len:\n",
    "    raise ValueError(\"Block size should not be bigger than sequence length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dce0777e-c8af-4835-833d-f1717dbf2501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "5\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "print(maxlen)\n",
    "print(vocab_size)\n",
    "print(num_chars_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3649b2-2e47-4b30-88e5-6792f1646058",
   "metadata": {},
   "source": [
    "---\n",
    "# Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf74ee78-6d96-41ed-9989-e66cfd5cb1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create 8 dimensional embeddings\n",
    "pretrained_encoder = dnabert.DnaBertEncoderModel(pretrained_model.base)\n",
    "pretrained_encoder.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b454f30-5e0f-4f62-8a6f-fb2d47a52cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(400, 8), dtype=float32, numpy=\n",
       "array([[ -7.4475875 ,   1.1663623 ,   6.6939783 , ...,  -7.992336  ,\n",
       "          1.3616216 ,  -1.1660289 ],\n",
       "       [  7.202399  ,   2.545476  ,   5.7800546 , ...,  -0.35330856,\n",
       "          6.0580387 ,   1.5731785 ],\n",
       "       [ -7.3592477 ,   4.8254547 ,  -6.1533613 , ..., -12.36576   ,\n",
       "          5.8514266 ,  -5.187319  ],\n",
       "       ...,\n",
       "       [ -9.9023075 ,  -1.4885719 ,   0.3919065 , ...,  -8.553962  ,\n",
       "         -4.919237  ,   4.0787582 ],\n",
       "       [ -0.14813566,   1.0515904 ,   7.329042  , ...,   3.18255   ,\n",
       "          4.312391  ,   3.7256906 ],\n",
       "       [-12.081191  ,  -1.2224596 ,   2.1566005 , ..., -13.340243  ,\n",
       "         -7.8641644 ,   2.6917217 ]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_encoder(dataset[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d33b634-4927-4f9a-a241-8c33c9249a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Create_Embeddings(keras.layers.Layer):\n",
    "    def __init__(self, encoder):\n",
    "        super(Create_Embeddings, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        \n",
    "    def subbatch_predict(self, model, batch, subbatch_size, concat=lambda old, new: tf.concat((old, new), axis=0)):\n",
    "        def predict(i, result=None):\n",
    "            n = i + subbatch_size\n",
    "            pred = tf.stop_gradient(model(batch[i:n]))\n",
    "            if result is None:\n",
    "                return [n, pred]\n",
    "            return [n, concat(result, pred)]\n",
    "        i, result = predict(0)\n",
    "        batch_size = tf.shape(batch)[0]\n",
    "        i, result = tf.while_loop(\n",
    "            cond=lambda i, _: i < batch_size,\n",
    "            body=predict,\n",
    "            loop_vars=[i, result],\n",
    "            parallel_iterations=1)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def modify_data_for_input(self, data):\n",
    "        batch_size = tf.shape(data)[0]\n",
    "        subsample_size = tf.shape(data)[1]\n",
    "        flat_data = tf.reshape(data, (batch_size*subsample_size, -1))\n",
    "        encoded = self.subbatch_predict(self.encoder, flat_data, subsample_size)\n",
    "        return tf.reshape(encoded, (batch_size, subsample_size, -1))\n",
    "    \n",
    "    def call(self, data):\n",
    "        return  self.modify_data_for_input(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b66c98-f8b5-4b80-9736-66a9769cc966",
   "metadata": {},
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9af4ac04-1eab-4d74-bfc0-716408efeed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_initializer(initializer):\n",
    "    if isinstance(initializer, tf.keras.initializers.Initializer):\n",
    "        return initializer.__class__.from_config(initializer.get_config())\n",
    "    return initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aadfeaaa-08b0-4155-8381-328ac71f4c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape_list(tensor, expected_rank=None, name=None):\n",
    "    if expected_rank is not None:\n",
    "        assert_rank(tensor, expected_rank, name)\n",
    "\n",
    "    shape = tensor.shape.as_list()\n",
    "\n",
    "    non_static_indexes = []\n",
    "    for (index, dim) in enumerate(shape):\n",
    "        if dim is None:\n",
    "            non_static_indexes.append(index)\n",
    "\n",
    "    if not non_static_indexes:\n",
    "        return shape\n",
    "\n",
    "    dyn_shape = tf.shape(tensor)\n",
    "    for index in non_static_indexes:\n",
    "        shape[index] = dyn_shape[index]\n",
    "    return shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befee659-3a43-4372-ae56-860a836497e1",
   "metadata": {},
   "source": [
    "---\n",
    "# Masked Token and Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a27a7c63-8d17-4e49-bfd7-4e05d92eb915",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedTokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim, **kwargs):\n",
    "        super(MaskedTokenAndPositionEmbedding, self).__init__(**kwargs)\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size,\n",
    "        output_dim=embed_dim,\n",
    "        mask_zero=True)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen+1,\n",
    "        output_dim=embed_dim,\n",
    "        mask_zero=True)\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=1, limit=maxlen+1, delta=1)\n",
    "        positions = positions * tf.cast(tf.sign(x),tf.int32)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348583b3-a8cf-4d03-bc57-cbd8c106b890",
   "metadata": {},
   "source": [
    "---\n",
    "# Custom Loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "160d05e4-b851-4348-85df-508526fb154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "def MaskedSparseCategoricalCrossentropy(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "def MaskedSparseCategoricalAccuracy(real, pred):\n",
    "    accuracies = tf.equal(tf.cast(real,tf.int64), tf.argmax(pred, axis=2))\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835c39dd-9fb5-49bf-8763-14ca694a1a35",
   "metadata": {},
   "source": [
    "---\n",
    "# Cache Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da4cb5e4-2d26-4e40-a00f-3e71f470b273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_state: `Tensor`, the current state.\n",
    "# previous_state: `Tensor`, the previous state.\n",
    "# memory_length: `int`, the number of tokens to cache.\n",
    "# reuse_length: `int`, the number of tokens in the current batch to be cached\n",
    "#     and reused in the future.\n",
    "# Returns:  `Tensor`, representing the cached state with stopped gradients.\n",
    "def _cache_memory(current_state, previous_state, memory_length, reuse_length=0):\n",
    "    if memory_length is None or memory_length == 0:\n",
    "        return None\n",
    "    else:\n",
    "        if reuse_length > 0:\n",
    "            current_state = current_state[:, :reuse_length, :]\n",
    "\n",
    "        if previous_state is None:\n",
    "            new_mem = current_state[:, -memory_length:, :]\n",
    "        else:\n",
    "            new_mem = tf.concat(\n",
    "                    [previous_state, current_state], 1)[:, -memory_length:, :]\n",
    "\n",
    "    return tf.stop_gradient(new_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3126b9-a96e-4d6a-ae65-3c9579514cf1",
   "metadata": {},
   "source": [
    "---\n",
    "# MultiHead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b32faad-fd06-4448-b817-b3a1f5943aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query: Query `Tensor` of shape `[B, T, dim]`.\n",
    "# value: Value `Tensor` of shape `[B, S, dim]`.\n",
    "# content_attention_bias: Bias `Tensor` for content based attention of shape\n",
    "# `[num_heads, dim]`.\n",
    "# positional_attention_bias: Bias `Tensor` for position based attention of\n",
    "# shape `[num_heads, dim]`.\n",
    "# key: Optional key `Tensor` of shape `[B, S, dim]`. If not given, will use\n",
    "# `value` for both `key` and `value`, which is the most common case.\n",
    "# relative_position_encoding: Relative positional encoding `Tensor` of shape\n",
    "# `[B, L, dim]`.\n",
    "# state: Optional `Tensor` of shape `[B, M, E]` where M is the length of the\n",
    "# state or memory. If passed, this is also attended over as in Transformer\n",
    "# XL.\n",
    "# attention_mask: A boolean mask of shape `[B, T, S]` that prevents attention\n",
    "# to certain positions.\n",
    "class MultiHeadRelativeAttention(tf.keras.layers.MultiHeadAttention):\n",
    "    def __init__(self,\n",
    "                 kernel_initializer=\"variance_scaling\",\n",
    "                 **kwargs):\n",
    "        super().__init__(kernel_initializer=kernel_initializer,\n",
    "                                         **kwargs)\n",
    "\n",
    "    def _build_from_signature(self, query, value, key=None):\n",
    "        super(MultiHeadRelativeAttention, self)._build_from_signature(\n",
    "                query=query,\n",
    "                value=value,\n",
    "                key=key)\n",
    "        if hasattr(value, \"shape\"):\n",
    "            value_shape = tf.TensorShape(value.shape)\n",
    "        else:\n",
    "            value_shape = value\n",
    "        if key is None:\n",
    "            key_shape = value_shape\n",
    "        elif hasattr(key, \"shape\"):\n",
    "            key_shape = tf.TensorShape(key.shape)\n",
    "        else:\n",
    "            key_shape = key\n",
    "\n",
    "        common_kwargs = dict(\n",
    "                kernel_initializer=self._kernel_initializer,\n",
    "                bias_initializer=self._bias_initializer,\n",
    "                kernel_regularizer=self._kernel_regularizer,\n",
    "                bias_regularizer=self._bias_regularizer,\n",
    "                activity_regularizer=self._activity_regularizer,\n",
    "                kernel_constraint=self._kernel_constraint,\n",
    "                bias_constraint=self._bias_constraint)\n",
    "\n",
    "        with tf.init_scope():\n",
    "            einsum_equation, _, output_rank = _build_proj_equation(\n",
    "                    key_shape.rank - 1, bound_dims=1, output_dims=2)\n",
    "            self._encoding_dense = tf.keras.layers.experimental.EinsumDense(\n",
    "                    einsum_equation,\n",
    "                    output_shape=_get_output_shape(\n",
    "                        output_rank - 1,\n",
    "                        [self._num_heads, self._key_dim]),\n",
    "                        bias_axes=None,\n",
    "                        name=\"encoding\",\n",
    "                        **common_kwargs)\n",
    "\n",
    "# query: Projected query `Tensor` of shape `[B, T, N, key_dim]`.\n",
    "# key: Projected key `Tensor` of shape `[B, S + M, N, key_dim]`.\n",
    "# value: Projected value `Tensor` of shape `[B, S + M, N, key_dim]`.\n",
    "# position: Projected position `Tensor` of shape `[B, L, N, key_dim]`.\n",
    "# content_attention_bias: Trainable bias parameter added to the query head\n",
    "#     when calculating the content-based attention score.\n",
    "# positional_attention_bias: Trainable bias parameter added to the query\n",
    "#     head when calculating the position-based attention score.\n",
    "# attention_mask: (default None) Optional mask that is added to attention\n",
    "#     logits. If state is not None, the mask source sequence dimension should\n",
    "#     extend M.\n",
    "# Returns:\n",
    "# attention_output: Multi-headed output of attention computation of shape\n",
    "#     `[B, S, N, key_dim]`.\n",
    "    def compute_attention(\n",
    "        self,\n",
    "        query,\n",
    "        key,\n",
    "        value,\n",
    "        position,\n",
    "        content_attention_bias,\n",
    "        positional_attention_bias,\n",
    "        attention_mask=None\n",
    "    ):\n",
    "        \n",
    "        #AC\n",
    "        content_attention = tf.einsum(self._dot_product_equation, key, query + content_attention_bias)\n",
    "        \n",
    "        #positional_attention = tf.einsum(self._dot_product_equation, position, query + positional_attention_bias)\n",
    "        \n",
    "        \n",
    "        #BD\n",
    "        #positional_attention = _rel_shift(positional_attention, klen=tf.shape(content_attention)[3])\n",
    "        \n",
    "        attention_sum = content_attention# + positional_attention\n",
    "\n",
    "        attention_scores = tf.multiply(attention_sum, 1.0 / math.sqrt(float(self._key_dim)))\n",
    "\n",
    "        attention_scores = self._masked_softmax(attention_scores, attention_mask)\n",
    "\n",
    "        attention_output = self._dropout_layer(attention_scores)\n",
    "\n",
    "        attention_output = tf.einsum(self._combine_equation,\n",
    "                                                                 attention_output,\n",
    "                                                                 value)\n",
    "        return attention_output\n",
    "\n",
    "# * Number of heads (H): the number of attention heads.\n",
    "# * Value size (V): the size of each value embedding per head.\n",
    "# * Key size (K): the size of each key embedding per head. Equally, the size\n",
    "#     of each query embedding per head. Typically K <= V.\n",
    "# * Batch dimensions (B).\n",
    "# * Query (target) attention axes shape (T).\n",
    "# * Value (source) attention axes shape (S), the rank must match the target.\n",
    "# * Encoding length (L): The relative positional encoding length.\n",
    "# query: attention input.\n",
    "# value: attention input.\n",
    "# content_attention_bias: A trainable bias parameter added to the query head\n",
    "#     when calculating the content-based attention score.\n",
    "# positional_attention_bias: A trainable bias parameter added to the query\n",
    "#     head when calculating the position-based attention score.\n",
    "# key: attention input.\n",
    "# relative_position_encoding: relative positional encoding for key and\n",
    "#     value.\n",
    "# state: (default None) optional state. If passed, this is also attended\n",
    "#     over as in TransformerXL.\n",
    "# attention_mask: (default None) Optional mask that is added to attention\n",
    "#     logits. If state is not None, the mask source sequence dimension should\n",
    "#     extend M.\n",
    "# Returns:\n",
    "# attention_output: The result of the computation, of shape [B, T, E],\n",
    "#     where `T` is for target sequence shapes and `E` is the query input last\n",
    "#     dimension if `output_shape` is `None`. Otherwise, the multi-head outputs\n",
    "#     are projected to the shape specified by `output_shape`.\n",
    "    def call(self,\n",
    "             query,\n",
    "             value,\n",
    "             content_attention_bias,\n",
    "             positional_attention_bias,\n",
    "             key=None,\n",
    "             relative_position_encoding=None,\n",
    "             state=None,\n",
    "             attention_mask=None):\n",
    "        \n",
    "        if not self._built_from_signature:\n",
    "            self._build_from_signature(query, value, key=key)\n",
    "        if key is None:\n",
    "            key = value\n",
    "        if state is not None and state.shape.ndims > 1:\n",
    "            value = tf.concat([state, value], 1)\n",
    "            key = tf.concat([state, key], 1)\n",
    "\n",
    "        # `query` = [B, T, N ,H]\n",
    "        query = self._query_dense(query)\n",
    "\n",
    "        # `key` = [B, S + M, N, H]\n",
    "        key = self._key_dense(key)\n",
    "\n",
    "        # `value` = [B, S + M, N, H]\n",
    "        value = self._value_dense(value)\n",
    "\n",
    "        # `position` = [B, L, N, H]\n",
    "        #position = self._encoding_dense(relative_position_encoding)\n",
    "        position = None\n",
    "        \n",
    "        attention_output = self.compute_attention(\n",
    "                query=query,\n",
    "                key=key,\n",
    "                value=value,\n",
    "                position=position,\n",
    "                content_attention_bias=content_attention_bias,\n",
    "                positional_attention_bias=positional_attention_bias,\n",
    "                attention_mask=attention_mask)\n",
    "\n",
    "        # `attention_output` = [B, S, N, H]\n",
    "        attention_output = self._output_dense(attention_output)\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95abdafa-2c98-4c04-b9e8-aae23e53265e",
   "metadata": {},
   "source": [
    "---\n",
    "# Relative Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc8c99a8-3755-48f9-9704-c1dbd04e0ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rel_shift(x, klen=-1):    \n",
    "    x = tf.transpose(x, perm=[2, 3, 0, 1])\n",
    "    x = tf.pad(x, [[0, 0], [1, 0], [0, 0], [0, 0]])\n",
    "    x_size = tf.shape(x)\n",
    "    x = tf.reshape(x, [x_size[1], x_size[0], x_size[2], x_size[3]])\n",
    "    x = tf.slice(x, [1, 0, 0, 0], [-1, -1, -1, -1])\n",
    "    x = tf.reshape(x, [x_size[0], x_size[1] - 1, x_size[2], x_size[3]])\n",
    "    x = tf.transpose(x, perm=[2, 3, 0, 1])\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a026fd-2cbe-45b7-88f7-a3ffd3548806",
   "metadata": {},
   "source": [
    "---\n",
    "# Build Einsum Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5102422c-82af-49d4-a8d5-5081262b21e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_CHR_IDX = string.ascii_lowercase\n",
    "\n",
    "# Builds an einsum equation for projections inside multi-head attention\n",
    "def _build_proj_equation(free_dims, bound_dims, output_dims):\n",
    "    input_str = \"\"\n",
    "    kernel_str = \"\"\n",
    "    output_str = \"\"\n",
    "    bias_axes = \"\"\n",
    "    letter_offset = 0\n",
    "    for i in range(free_dims):\n",
    "        char = _CHR_IDX[i + letter_offset]\n",
    "        input_str += char\n",
    "        output_str += char\n",
    "\n",
    "    letter_offset += free_dims\n",
    "    for i in range(bound_dims):\n",
    "        char = _CHR_IDX[i + letter_offset]\n",
    "        input_str += char\n",
    "        kernel_str += char\n",
    "\n",
    "    letter_offset += bound_dims\n",
    "    for i in range(output_dims):\n",
    "        char = _CHR_IDX[i + letter_offset]\n",
    "        kernel_str += char\n",
    "        output_str += char\n",
    "        bias_axes += char\n",
    "    equation = \"%s,%s->%s\" % (input_str, kernel_str, output_str)\n",
    "\n",
    "    return equation, bias_axes, len(output_str)\n",
    "\n",
    "\n",
    "def _get_output_shape(output_rank, known_last_dims):\n",
    "    return [None] * (output_rank - len(known_last_dims)) + list(known_last_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5343f3e3-8536-46d5-9d1b-28f8c3f7908b",
   "metadata": {},
   "source": [
    "---\n",
    "# Relative Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5f5eeac-4c07-4796-8281-f5c039fe7c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_size: Size of the hidden layer.\n",
    "# min_timescale: Minimum scale that will be applied at each position\n",
    "# max_timescale: Maximum scale that will be applied at each position.\n",
    "class RelativePositionEmbedding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 hidden_size: int,\n",
    "                 min_timescale: float = 1.0,\n",
    "                 max_timescale: float = 1.0e4,\n",
    "                 **kwargs):\n",
    "        \n",
    "        if \"dtype\" not in kwargs:\n",
    "            kwargs[\"dtype\"] = \"float32\"\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self._hidden_size = hidden_size\n",
    "        self._min_timescale = min_timescale\n",
    "        self._max_timescale = max_timescale\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "                \"hidden_size\": self._hidden_size,\n",
    "                \"min_timescale\": self._min_timescale,\n",
    "                \"max_timescale\": self._max_timescale,\n",
    "        }\n",
    "        base_config = super(RelativePositionEmbedding, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "# inputs: An tensor whose second dimension will be used as `length`. If\n",
    "# `None`, the other `length` argument must be specified.\n",
    "# length: An optional integer specifying the number of positions. If both\n",
    "# `inputs` and `length` are spcified, `length` must be equal to the second\n",
    "# dimension of `inputs`.\n",
    "# Returns:\n",
    "# A tensor in shape of `(length, hidden_size)`.\n",
    "    def call(self, inputs, length=None):\n",
    "        if inputs is None and length is None:\n",
    "            raise ValueError(\"If inputs is None, `length` must be set in \"\n",
    "                                             \"RelativePositionEmbedding().\")\n",
    "        if inputs is not None:\n",
    "            input_shape = get_shape_list(inputs)\n",
    "            if length is not None and length != input_shape[1]:\n",
    "                raise ValueError(\n",
    "                        \"If inputs is not None, `length` must equal to input_shape[1].\")\n",
    "            length = input_shape[1]\n",
    "        position = tf.cast(tf.range(length), tf.float32)\n",
    "        num_timescales = self._hidden_size // 2\n",
    "        min_timescale, max_timescale = self._min_timescale, self._max_timescale\n",
    "        log_timescale_increment = (\n",
    "                math.log(float(max_timescale) / float(min_timescale)) /\n",
    "                (tf.cast(num_timescales, tf.float32) - 1))\n",
    "        inv_timescales = min_timescale * tf.exp(\n",
    "                tf.cast(tf.range(num_timescales), tf.float32) *\n",
    "                -log_timescale_increment)\n",
    "        scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(\n",
    "                inv_timescales, 0)\n",
    "        position_embeddings = tf.concat(\n",
    "                [tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n",
    "        position_embeddings = tf.expand_dims(position_embeddings, axis=0)\n",
    "        return tf.tile(position_embeddings, (tf.shape(inputs)[0], 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c32bb5-4520-4802-a8f0-5f7849905a83",
   "metadata": {},
   "source": [
    "---\n",
    "# XL Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7245b7dd-a3b2-464a-9f5b-7efd4a9d0ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size: The size of the token vocabulary.\n",
    "# hidden_size: The size of the transformer hidden layers.\n",
    "# num_attention_heads: The number of attention heads.\n",
    "# head_size: The dimension size of each attention head.\n",
    "# inner_size: The inner size for the transformer layers.\n",
    "# dropout_rate: Dropout rate for the output of this layer.\n",
    "# attention_dropout_rate: Dropout rate on attention probabilities.\n",
    "# norm_epsilon: Epsilon value to initialize normalization layers.\n",
    "# inner_activation: The activation to use for the inner\n",
    "#     FFN layers.\n",
    "# kernel_initializer: Initializer for dense layer kernels.\n",
    "# inner_dropout: Dropout probability for the inner dropout\n",
    "#     layer.\n",
    "class TransformerXLBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 hidden_size,\n",
    "                 num_attention_heads,\n",
    "                 head_size,\n",
    "                 inner_size,\n",
    "                 dropout_rate,\n",
    "                 attention_dropout_rate,\n",
    "                 norm_epsilon=1e-12,\n",
    "                 inner_activation=\"relu\",\n",
    "                 kernel_initializer=\"variance_scaling\",\n",
    "                 inner_dropout=0.0,\n",
    "                 **kwargs):\n",
    "        \"\"\"Initializes TransformerXLBlock layer.\"\"\"\n",
    "\n",
    "        super(TransformerXLBlock, self).__init__(**kwargs)\n",
    "        self._vocab_size = vocab_size\n",
    "        self._num_heads = num_attention_heads\n",
    "        self._head_size = head_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._inner_size = inner_size\n",
    "        self._dropout_rate = dropout_rate\n",
    "        self._attention_dropout_rate = attention_dropout_rate\n",
    "        self._inner_activation = inner_activation\n",
    "        self._norm_epsilon = norm_epsilon\n",
    "        self._kernel_initializer = kernel_initializer\n",
    "        self._inner_dropout = inner_dropout\n",
    "        self._attention_layer_type = Set_Transformer\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        input_tensor = input_shape[0] if len(input_shape) == 2 else input_shape\n",
    "        input_tensor_shape = tf.TensorShape(input_tensor)\n",
    "        if len(input_tensor_shape.as_list()) != 3:\n",
    "            raise ValueError(\"TransformerLayer expects a three-dimensional input of \"\n",
    "                                             \"shape [batch, sequence, width].\")\n",
    "        batch_size, sequence_length, hidden_size = input_tensor_shape\n",
    "\n",
    "        if len(input_shape) == 2:\n",
    "            mask_tensor_shape = tf.TensorShape(input_shape[1])\n",
    "            expected_mask_tensor_shape = tf.TensorShape(\n",
    "                    [batch_size, sequence_length, sequence_length])\n",
    "            if not expected_mask_tensor_shape.is_compatible_with(mask_tensor_shape):\n",
    "                raise ValueError(\"When passing a mask tensor to TransformerXLBlock, \"\n",
    "                                                 \"the mask tensor must be of shape [batch, \"\n",
    "                                                 \"sequence_length, sequence_length] (here %s). Got a \"\n",
    "                                                 \"mask tensor of shape %s.\" %\n",
    "                                                 (expected_mask_tensor_shape, mask_tensor_shape))\n",
    "        if hidden_size % self._num_heads != 0:\n",
    "            raise ValueError(\n",
    "                    \"The input size (%d) is not a multiple of the number of attention \"\n",
    "                    \"heads (%d)\" % (hidden_size, self._num_heads))\n",
    "            \n",
    "            \n",
    "        self._attention_layer= self._attention_layer_type(embed_dim=self._head_size,num_heads=self._num_heads, stack = 4,\n",
    "                                                   use_layernorm=True,pre_layernorm=True,use_keras_mha=True)\n",
    "        \n",
    " \n",
    "        \n",
    "        # self._attention_layer = self._attention_layer_type(\n",
    "        #         num_heads=self._num_heads,\n",
    "        #         key_dim=self._head_size,\n",
    "        #         value_dim=self._head_size,\n",
    "        #         dropout=self._attention_dropout_rate,\n",
    "        #         use_bias=False,\n",
    "        #         kernel_initializer=clone_initializer(self._kernel_initializer),\n",
    "        #         name=\"rel_attn\")\n",
    "        self._attention_dropout = tf.keras.layers.Dropout(\n",
    "                rate=self._attention_dropout_rate)\n",
    "        self._attention_layer_norm = tf.keras.layers.LayerNormalization(\n",
    "                name=\"self_attention_layer_norm\",\n",
    "                axis=-1,\n",
    "                epsilon=self._norm_epsilon,\n",
    "                dtype=tf.float32)\n",
    "        self._inner_dense = tf.keras.layers.experimental.EinsumDense(\n",
    "                \"abc,cd->abd\",\n",
    "                output_shape=(None, self._inner_size),\n",
    "                bias_axes=\"d\",\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer),\n",
    "                name=\"inner\")\n",
    "\n",
    "        self._inner_activation_layer = tf.keras.layers.Activation(\n",
    "                self._inner_activation)\n",
    "        self._inner_dropout_layer = tf.keras.layers.Dropout(\n",
    "                rate=self._inner_dropout)\n",
    "        self._output_dense = tf.keras.layers.experimental.EinsumDense(\n",
    "                \"abc,cd->abd\",\n",
    "                output_shape=(None, hidden_size),\n",
    "                bias_axes=\"d\",\n",
    "                name=\"output\",\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer))\n",
    "        self._output_dropout = tf.keras.layers.Dropout(rate=self._dropout_rate)\n",
    "        self._output_layer_norm = tf.keras.layers.LayerNormalization(\n",
    "                name=\"output_layer_norm\",\n",
    "                axis=-1,\n",
    "                epsilon=self._norm_epsilon)\n",
    "\n",
    "        super(TransformerXLBlock, self).build(input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "                \"vocab_size\":\n",
    "                        self._vocab_size,\n",
    "                \"hidden_size\":\n",
    "                        self._hidden_size,\n",
    "                \"num_attention_heads\":\n",
    "                        self._num_heads,\n",
    "                \"head_size\":\n",
    "                        self._head_size,\n",
    "                \"inner_size\":\n",
    "                        self._inner_size,\n",
    "                \"dropout_rate\":\n",
    "                        self._dropout_rate,\n",
    "                \"attention_dropout_rate\":\n",
    "                        self._attention_dropout_rate,\n",
    "                \"norm_epsilon\":\n",
    "                        self._norm_epsilon,\n",
    "                \"inner_activation\":\n",
    "                        self._inner_activation,\n",
    "                \"kernel_initializer\":\n",
    "                        self._kernel_initializer,\n",
    "                \"inner_dropout\":\n",
    "                        self._inner_dropout,\n",
    "        }\n",
    "        base_config = super(TransformerXLBlock, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "# content_stream: `Tensor`, the input content stream. This is the standard\n",
    "# input to Transformer XL and is commonly referred to as `h` in XLNet.\n",
    "# content_attention_bias: Bias `Tensor` for content based attention of shape\n",
    "# `[num_heads, dim]`.\n",
    "# positional_attention_bias: Bias `Tensor` for position based attention of\n",
    "# shape `[num_heads, dim]`.\n",
    "# relative_position_encoding: Relative positional encoding `Tensor` of shape\n",
    "# `[B, L, dim]`.\n",
    "# state: Optional `Tensor` of shape `[B, M, E]`, where M is the length of\n",
    "# the state or memory. If passed, this is also attended over as in\n",
    "# Transformer XL.\n",
    "# content_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to content attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# query_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to query attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# target_mapping: Optional `Tensor` representing the target mapping when\n",
    "# calculating query attention.\n",
    "# Returns:\n",
    "# A `dict` object, containing the key value pairs for `content_attention`\n",
    "    def call(self,\n",
    "             content_stream,\n",
    "             content_attention_bias,\n",
    "             positional_attention_bias,\n",
    "             relative_position_encoding=None,\n",
    "             state=None,\n",
    "             content_attention_mask=None,\n",
    "             query_attention_mask=None,\n",
    "             target_mapping=None):\n",
    "        \n",
    "        attention_kwargs = dict(\n",
    "                query=content_stream,\n",
    "                value=content_stream,\n",
    "                key=content_stream,\n",
    "                attention_mask=content_attention_mask)\n",
    "\n",
    "        common_attention_kwargs = dict(\n",
    "                content_attention_bias=content_attention_bias,\n",
    "                relative_position_encoding=relative_position_encoding,\n",
    "                positional_attention_bias=positional_attention_bias,\n",
    "                state=state)\n",
    "\n",
    "        attention_kwargs.update(common_attention_kwargs)\n",
    "        #attention_output = self._attention_layer(**attention_kwargs)\n",
    "        attention_output = {\"content_attention\": self._attention_layer(content_stream)}\n",
    "        \n",
    "#         attention_stream = attention_output\n",
    "#         input_stream = content_stream\n",
    "#         attention_key = \"content_attention\"\n",
    "#        attention_output = {}\n",
    "        \n",
    "#         attention_stream = self._attention_dropout(attention_stream)\n",
    "#         attention_stream = self._attention_layer_norm(attention_stream + input_stream)\n",
    "#         inner_output = self._inner_dense(attention_stream)\n",
    "#         inner_output = self._inner_activation_layer(\n",
    "#                 inner_output)\n",
    "#         inner_output = self._inner_dropout_layer(\n",
    "#                 inner_output)\n",
    "#         layer_output = self._output_dense(inner_output)\n",
    "#         layer_output = self._output_dropout(layer_output)\n",
    "#         layer_output = self._output_layer_norm(layer_output + attention_stream)\n",
    "#         attention_output[attention_key] = layer_output\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8749ec30-1281-4f9d-8baa-54b890fb7949",
   "metadata": {},
   "source": [
    "---\n",
    "# Transformer XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "019ca25d-e899-4e17-8f41-dfdc3d9bdee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_layers: The number of layers.\n",
    "# hidden_size: The hidden size.\n",
    "# num_attention_heads: The number of attention heads.\n",
    "# head_size: The dimension size of each attention head.\n",
    "# inner_size: The hidden size in feed-forward layers.\n",
    "# dropout_rate: Dropout rate used in each Transformer XL block.\n",
    "# attention_dropout_rate: Dropout rate on attention probabilities.\n",
    "# initializer: The initializer to use for attention biases.\n",
    "# tie_attention_biases: Whether or not to tie biases together. If `True`, then\n",
    "# each Transformer XL block shares the same trainable attention bias. If\n",
    "# `False`, then each block has its own attention bias. This is usually set\n",
    "# to `True`.\n",
    "# memory_length: The number of tokens to cache.\n",
    "# reuse_length: The number of tokens in the current batch to be cached\n",
    "# and reused in the future.\n",
    "# inner_activation: The activation to use in the inner layers\n",
    "# for Transformer XL blocks. Typically \"relu\" or \"gelu\".\n",
    "class TransformerXL(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 num_layers,\n",
    "                 hidden_size,\n",
    "                 maxlen,\n",
    "                 embed_dim,\n",
    "                 num_attention_heads,\n",
    "                 head_size,\n",
    "                 inner_size,\n",
    "                 dropout_rate,\n",
    "                 attention_dropout_rate,\n",
    "                 initializer,\n",
    "                 tie_attention_biases=True,\n",
    "                 memory_length=None,\n",
    "                 reuse_length=None,\n",
    "                 inner_activation=\"relu\",\n",
    "                 **kwargs):\n",
    "        super(TransformerXL, self).__init__(**kwargs)\n",
    "\n",
    "        self._vocab_size = vocab_size\n",
    "        self._initializer = initializer\n",
    "        self._num_layers = num_layers\n",
    "        self._hidden_size = hidden_size\n",
    "        self._num_attention_heads = num_attention_heads\n",
    "        self._head_size = head_size\n",
    "        self._inner_size = inner_size\n",
    "        self._inner_activation = inner_activation\n",
    "        self._dropout_rate = dropout_rate\n",
    "        self._attention_dropout_rate = attention_dropout_rate\n",
    "        self._tie_attention_biases = tie_attention_biases\n",
    "\n",
    "        self._memory_length = memory_length\n",
    "        self._reuse_length = reuse_length\n",
    "\n",
    "        if self._tie_attention_biases:\n",
    "            attention_bias_shape = [self._num_attention_heads, self._head_size]\n",
    "        else:\n",
    "            attention_bias_shape = [self._num_layers, self._num_attention_heads, self._head_size]\n",
    "\n",
    "        self.content_attention_bias = self.add_weight(\n",
    "                \"content_attention_bias\",\n",
    "                shape=attention_bias_shape,\n",
    "                dtype=tf.float32,\n",
    "                initializer=clone_initializer(self._initializer))\n",
    "        self.positional_attention_bias = self.add_weight(\n",
    "                \"positional_attention_bias\",\n",
    "                shape=attention_bias_shape,\n",
    "                dtype=tf.float32,\n",
    "                initializer=clone_initializer(self._initializer))\n",
    "\n",
    "        self.transformer_xl_layers = []\n",
    "        for i in range(self._num_layers):\n",
    "            self.transformer_xl_layers.append(\n",
    "                    TransformerXLBlock(\n",
    "                            vocab_size=self._vocab_size,\n",
    "                            hidden_size=self._head_size * self._num_attention_heads,\n",
    "                            num_attention_heads=self._num_attention_heads,\n",
    "                            head_size=self._head_size,\n",
    "                            inner_size=self._inner_size,\n",
    "                            dropout_rate=self._dropout_rate,\n",
    "                            attention_dropout_rate=self._attention_dropout_rate,\n",
    "                            norm_epsilon=1e-12,\n",
    "                            inner_activation=self._inner_activation,\n",
    "                            kernel_initializer=\"variance_scaling\",\n",
    "                            name=\"layer_%d\" % i))\n",
    "\n",
    "        self.output_dropout = tf.keras.layers.Dropout(rate=self._dropout_rate)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "                \"vocab_size\":\n",
    "                        self._vocab_size,\n",
    "                \"num_layers\":\n",
    "                        self._num_layers,\n",
    "                \"hidden_size\":\n",
    "                        self._hidden_size,\n",
    "                \"num_attention_heads\":\n",
    "                        self._num_attention_heads,\n",
    "                \"head_size\":\n",
    "                        self._head_size,\n",
    "                \"inner_size\":\n",
    "                        self._inner_size,\n",
    "                \"dropout_rate\":\n",
    "                        self._dropout_rate,\n",
    "                \"attention_dropout_rate\":\n",
    "                        self._attention_dropout_rate,\n",
    "                \"initializer\":\n",
    "                        self._initializer,\n",
    "                \"tie_attention_biases\":\n",
    "                        self._tie_attention_biases,\n",
    "                \"memory_length\":\n",
    "                        self._memory_length,\n",
    "                \"reuse_length\":\n",
    "                        self._reuse_length,\n",
    "                \"inner_activation\":\n",
    "                        self._inner_activation,\n",
    "        }\n",
    "        base_config = super(TransformerXL, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "# content_stream: `Tensor`, the input content stream. This is the standard\n",
    "# input to Transformer XL and is commonly referred to as `h` in XLNet.\n",
    "# relative_position_encoding: Relative positional encoding `Tensor` of shape\n",
    "# `[B, L, dim]`.\n",
    "# state: Optional `Tensor` of shape `[B, M, E]`, where M is the length of\n",
    "# the state or memory. If passed, this is also attended over as in\n",
    "# Transformer XL.\n",
    "# content_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to content attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# query_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to query attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# target_mapping: Optional `Tensor` representing the target mapping when\n",
    "# calculating query attention.\n",
    "# Returns:\n",
    "# A tuple consisting of the attention output and the list of cached memory\n",
    "# states.\n",
    "# The attention output is `content_attention`\n",
    "    def call(self,\n",
    "             content_stream,\n",
    "             relative_position_encoding,\n",
    "             state=None,\n",
    "             content_attention_mask=None,\n",
    "             query_attention_mask=None,\n",
    "             target_mapping=None):\n",
    "        \n",
    "        new_mems = []\n",
    "\n",
    "        if state is None:\n",
    "            state = [None] * self._num_layers\n",
    "        for i in range(self._num_layers):\n",
    "            # cache new mems\n",
    "            new_mems.append(\n",
    "                    _cache_memory(content_stream, state[i],\n",
    "                                                self._memory_length, self._reuse_length))\n",
    "\n",
    "            if self._tie_attention_biases:\n",
    "                content_attention_bias = self.content_attention_bias\n",
    "            else:\n",
    "                content_attention_bias = self.content_attention_bias[i]\n",
    "                \n",
    "            if self._tie_attention_biases:\n",
    "                positional_attention_bias = self.positional_attention_bias\n",
    "            else:\n",
    "                positional_attention_bias = self.positional_attention_bias[i]\n",
    "\n",
    "            transformer_xl_layer = self.transformer_xl_layers[i]\n",
    "            \n",
    "            transformer_xl_output = transformer_xl_layer(\n",
    "                    content_stream=content_stream,\n",
    "                    content_attention_bias=content_attention_bias,\n",
    "                    positional_attention_bias=positional_attention_bias,\n",
    "                    relative_position_encoding=relative_position_encoding,\n",
    "                    state=state[i],\n",
    "                    content_attention_mask=content_attention_mask,\n",
    "                    query_attention_mask=query_attention_mask,\n",
    "                    target_mapping=target_mapping)\n",
    "            content_stream = transformer_xl_output[\"content_attention\"]\n",
    "\n",
    "        output_stream = content_stream\n",
    "        return output_stream, new_mems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b77a89f5-b11a-48e4-bf4f-cc70f1e74e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Set_Transformer(keras.Model):\n",
    "    def __init__(self, embed_dim, num_heads, stack, use_layernorm, pre_layernorm, use_keras_mha):\n",
    "        super(Set_Transformer, self).__init__()\n",
    "        \n",
    "        self.isabs = []\n",
    "        \n",
    "        for i in range(stack):\n",
    "            self.isabs.append(stf.InducedSetAttentionBlock(embed_dim=embed_dim,num_heads=num_heads,num_induce=24,use_layernorm=use_layernorm,pre_layernorm=pre_layernorm,use_keras_mha=use_keras_mha))\n",
    "      \n",
    "    def call(self, data):\n",
    "            \n",
    "            attention = data\n",
    "            \n",
    "            for isab in self.isabs:\n",
    "                attention = isab(attention)\n",
    "                \n",
    "            return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a127a4b5-15e3-46e3-838b-a5e18e4d8dc8",
   "metadata": {},
   "source": [
    "---\n",
    "# Xl Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "855d43dd-e182-4c2a-961a-ff9781141f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XlModel(keras.Model):\n",
    "    def __init__(self, max_files, encoder, block_size, seq_len_padded, embed_dim, vocab_size, num_layers, hidden_size, num_attention_heads, maxlen, memory_length, reuse_length, head_size, inner_size, dropout_rate, attention_dropout_rate, initializer):\n",
    "        super(XlModel, self).__init__()\n",
    "        \n",
    "        self.block_size = block_size\n",
    "        self.seq_len_padded = seq_len_padded\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_attention_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.maxlen = maxlen\n",
    "        self.memory_length = memory_length\n",
    "        self.max_files = max_files\n",
    "        self.encoder = encoder\n",
    "        self.isabs = []\n",
    "        \n",
    "        self.embedding_layer = Create_Embeddings(encoder)\n",
    "\n",
    "        self.linear_layer = keras.layers.Dense(embed_dim)\n",
    "        \n",
    "        self.transformer_xl = TransformerXL(\n",
    "                vocab_size=vocab_size,\n",
    "                num_layers=num_layers,\n",
    "                hidden_size=hidden_size,\n",
    "                num_attention_heads=num_attention_heads,\n",
    "                maxlen=maxlen,\n",
    "                embed_dim=embed_dim,\n",
    "                memory_length=memory_length,\n",
    "                reuse_length=reuse_length,\n",
    "                head_size=head_size,\n",
    "                inner_size=inner_size,\n",
    "                dropout_rate=dropout_rate,\n",
    "                attention_dropout_rate=attention_dropout_rate,\n",
    "                initializer=initializer, \n",
    "            )\n",
    "        \n",
    "\n",
    "        self.pooling_layer = stf.PoolingByMultiHeadAttention(num_seeds=1,embed_dim=embed_dim,num_heads=1,use_layernorm=True,pre_layernorm=True, use_keras_mha=True,is_final_block=True)\n",
    "    \n",
    "        self.reshape_layer = keras.layers.Reshape((embed_dim,))\n",
    "   \n",
    "        self.output_layer = keras.layers.Dense(self.max_files, activation=keras.activations.softmax)\n",
    "        \n",
    "    \n",
    "    def call(self, x, training=None):        \n",
    " \n",
    "        mems = tf.zeros((self.num_layers, tf.shape(x)[0], self.memory_length, self.embed_dim))\n",
    "        \n",
    "        embeddings = self.embedding_layer(x)\n",
    "            \n",
    "        linear_transform = self.linear_layer(embeddings)    \n",
    "            \n",
    "        for i in range(0, self.seq_len_padded, self.block_size):\n",
    "            block = embeddings[:,i:i+block_size]\n",
    "            \n",
    "            output, mems = self.transformer_xl(content_stream=block, relative_position_encoding=None, state=mems)\n",
    "                \n",
    "        pooling = self.pooling_layer(output)\n",
    "\n",
    "        reshape = self.reshape_layer(pooling)\n",
    "\n",
    "        output = self.output_layer(reshape)          \n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3469de1-250d-4c14-bb79-a4e74f0fe93e",
   "metadata": {},
   "source": [
    "---\n",
    "# Xl Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2bc3ea7d-d4a1-4a1e-bf2a-7cd1c91430cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xl Parameters \n",
    "embed_dim = 8\n",
    "num_layers = 10\n",
    "hidden_size = 32\n",
    "num_attention_heads = 8\n",
    "memory_length = 200\n",
    "reuse_length = 0\n",
    "head_size = 8\n",
    "inner_size = 32\n",
    "dropout_rate = 0.01\n",
    "attention_dropout_rate = 0.01\n",
    "initializer = keras.initializers.RandomNormal(stddev=0.1) \n",
    "\n",
    "encoder = pretrained_encoder\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89649663-7d18-48bc-8678-ce5f093908cc",
   "metadata": {},
   "source": [
    "---\n",
    "# Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "821b0c4a-6d05-4867-a713-474bfdbff881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['xl_model_2/dense_385/kernel:0', 'xl_model_2/dense_385/bias:0', 'content_attention_bias:0', 'positional_attention_bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['xl_model_2/dense_385/kernel:0', 'xl_model_2/dense_385/bias:0', 'content_attention_bias:0', 'positional_attention_bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "20/20 [==============================] - 174s 3s/step - loss: 1.8640 - sparse_categorical_accuracy: 0.2550\n",
      "Epoch 2/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 1.6861 - sparse_categorical_accuracy: 0.1950\n",
      "Epoch 3/100\n",
      "20/20 [==============================] - 63s 3s/step - loss: 1.5989 - sparse_categorical_accuracy: 0.2575\n",
      "Epoch 4/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 1.6171 - sparse_categorical_accuracy: 0.2625\n",
      "Epoch 5/100\n",
      "20/20 [==============================] - 63s 3s/step - loss: 1.6002 - sparse_categorical_accuracy: 0.3275\n",
      "Epoch 6/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 1.5489 - sparse_categorical_accuracy: 0.3750\n",
      "Epoch 7/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 1.5522 - sparse_categorical_accuracy: 0.3200\n",
      "Epoch 8/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 1.5219 - sparse_categorical_accuracy: 0.3450\n",
      "Epoch 9/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 1.4683 - sparse_categorical_accuracy: 0.3600\n",
      "Epoch 10/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 1.3862 - sparse_categorical_accuracy: 0.4175\n",
      "Epoch 11/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 1.3912 - sparse_categorical_accuracy: 0.4125\n",
      "Epoch 12/100\n",
      "20/20 [==============================] - 63s 3s/step - loss: 1.3579 - sparse_categorical_accuracy: 0.3725\n",
      "Epoch 13/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 1.2896 - sparse_categorical_accuracy: 0.4000\n",
      "Epoch 14/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 1.2853 - sparse_categorical_accuracy: 0.4300\n",
      "Epoch 15/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 1.2240 - sparse_categorical_accuracy: 0.5125\n",
      "Epoch 16/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 1.1813 - sparse_categorical_accuracy: 0.5325\n",
      "Epoch 17/100\n",
      "20/20 [==============================] - 63s 3s/step - loss: 1.1099 - sparse_categorical_accuracy: 0.5700\n",
      "Epoch 18/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 1.0415 - sparse_categorical_accuracy: 0.6200\n",
      "Epoch 19/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 1.0228 - sparse_categorical_accuracy: 0.5975\n",
      "Epoch 20/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 1.0179 - sparse_categorical_accuracy: 0.6100\n",
      "Epoch 21/100\n",
      "20/20 [==============================] - 63s 3s/step - loss: 1.0006 - sparse_categorical_accuracy: 0.5950\n",
      "Epoch 22/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 1.0007 - sparse_categorical_accuracy: 0.6025\n",
      "Epoch 23/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 1.0297 - sparse_categorical_accuracy: 0.5825\n",
      "Epoch 24/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.9868 - sparse_categorical_accuracy: 0.5875\n",
      "Epoch 25/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.9387 - sparse_categorical_accuracy: 0.6425\n",
      "Epoch 26/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.9913 - sparse_categorical_accuracy: 0.6050\n",
      "Epoch 27/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.9246 - sparse_categorical_accuracy: 0.6475\n",
      "Epoch 28/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.9430 - sparse_categorical_accuracy: 0.6125\n",
      "Epoch 29/100\n",
      "20/20 [==============================] - 63s 3s/step - loss: 0.9396 - sparse_categorical_accuracy: 0.6175\n",
      "Epoch 30/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.9484 - sparse_categorical_accuracy: 0.6300\n",
      "Epoch 31/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.8907 - sparse_categorical_accuracy: 0.6700\n",
      "Epoch 32/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.9191 - sparse_categorical_accuracy: 0.6450\n",
      "Epoch 33/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.9015 - sparse_categorical_accuracy: 0.6575\n",
      "Epoch 34/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.9291 - sparse_categorical_accuracy: 0.6475\n",
      "Epoch 35/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.8540 - sparse_categorical_accuracy: 0.6725\n",
      "Epoch 36/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.8517 - sparse_categorical_accuracy: 0.6550\n",
      "Epoch 37/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.8973 - sparse_categorical_accuracy: 0.6525\n",
      "Epoch 38/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.9431 - sparse_categorical_accuracy: 0.6200\n",
      "Epoch 39/100\n",
      "20/20 [==============================] - 63s 3s/step - loss: 0.8635 - sparse_categorical_accuracy: 0.6650\n",
      "Epoch 40/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.8438 - sparse_categorical_accuracy: 0.6600\n",
      "Epoch 41/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.9076 - sparse_categorical_accuracy: 0.6250\n",
      "Epoch 42/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.8677 - sparse_categorical_accuracy: 0.6625\n",
      "Epoch 43/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.8535 - sparse_categorical_accuracy: 0.6875\n",
      "Epoch 44/100\n",
      "20/20 [==============================] - 63s 3s/step - loss: 0.8590 - sparse_categorical_accuracy: 0.6525\n",
      "Epoch 45/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.8746 - sparse_categorical_accuracy: 0.6525\n",
      "Epoch 46/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.8246 - sparse_categorical_accuracy: 0.6775\n",
      "Epoch 47/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.9077 - sparse_categorical_accuracy: 0.6200\n",
      "Epoch 48/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.8674 - sparse_categorical_accuracy: 0.6575\n",
      "Epoch 49/100\n",
      "20/20 [==============================] - 59s 3s/step - loss: 0.8137 - sparse_categorical_accuracy: 0.6900\n",
      "Epoch 50/100\n",
      "20/20 [==============================] - 34s 2s/step - loss: 0.8398 - sparse_categorical_accuracy: 0.6825\n",
      "Epoch 51/100\n",
      "20/20 [==============================] - 43s 2s/step - loss: 0.8422 - sparse_categorical_accuracy: 0.6875\n",
      "Epoch 52/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.8452 - sparse_categorical_accuracy: 0.6700\n",
      "Epoch 53/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.8714 - sparse_categorical_accuracy: 0.6500\n",
      "Epoch 54/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.8385 - sparse_categorical_accuracy: 0.6950\n",
      "Epoch 55/100\n",
      "20/20 [==============================] - 63s 3s/step - loss: 0.8274 - sparse_categorical_accuracy: 0.6800\n",
      "Epoch 56/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.8107 - sparse_categorical_accuracy: 0.6800\n",
      "Epoch 57/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.8226 - sparse_categorical_accuracy: 0.6750\n",
      "Epoch 58/100\n",
      "20/20 [==============================] - 63s 3s/step - loss: 0.8484 - sparse_categorical_accuracy: 0.6750\n",
      "Epoch 59/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.7972 - sparse_categorical_accuracy: 0.6950\n",
      "Epoch 60/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.7756 - sparse_categorical_accuracy: 0.6925\n",
      "Epoch 61/100\n",
      "20/20 [==============================] - 63s 3s/step - loss: 0.8056 - sparse_categorical_accuracy: 0.6700\n",
      "Epoch 62/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.8073 - sparse_categorical_accuracy: 0.7025\n",
      "Epoch 63/100\n",
      "20/20 [==============================] - 63s 3s/step - loss: 0.8556 - sparse_categorical_accuracy: 0.6675\n",
      "Epoch 64/100\n",
      "20/20 [==============================] - 63s 3s/step - loss: 0.7858 - sparse_categorical_accuracy: 0.7050\n",
      "Epoch 65/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.7954 - sparse_categorical_accuracy: 0.6850\n",
      "Epoch 66/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.7906 - sparse_categorical_accuracy: 0.6975\n",
      "Epoch 67/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.8104 - sparse_categorical_accuracy: 0.6750\n",
      "Epoch 68/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.7816 - sparse_categorical_accuracy: 0.6800\n",
      "Epoch 69/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.7567 - sparse_categorical_accuracy: 0.7250\n",
      "Epoch 70/100\n",
      "20/20 [==============================] - 63s 3s/step - loss: 0.7892 - sparse_categorical_accuracy: 0.6925\n",
      "Epoch 71/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.7635 - sparse_categorical_accuracy: 0.7425\n",
      "Epoch 72/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.7973 - sparse_categorical_accuracy: 0.6650\n",
      "Epoch 73/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.7912 - sparse_categorical_accuracy: 0.6700\n",
      "Epoch 74/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.7755 - sparse_categorical_accuracy: 0.7450\n",
      "Epoch 75/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.7435 - sparse_categorical_accuracy: 0.7050\n",
      "Epoch 76/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.7643 - sparse_categorical_accuracy: 0.7100\n",
      "Epoch 77/100\n",
      "20/20 [==============================] - 63s 3s/step - loss: 0.7565 - sparse_categorical_accuracy: 0.7000\n",
      "Epoch 78/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.7740 - sparse_categorical_accuracy: 0.6900\n",
      "Epoch 79/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.8320 - sparse_categorical_accuracy: 0.6675\n",
      "Epoch 80/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.7445 - sparse_categorical_accuracy: 0.7300\n",
      "Epoch 81/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.7774 - sparse_categorical_accuracy: 0.6925\n",
      "Epoch 82/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.7864 - sparse_categorical_accuracy: 0.6750\n",
      "Epoch 83/100\n",
      "20/20 [==============================] - 63s 3s/step - loss: 0.7339 - sparse_categorical_accuracy: 0.6925\n",
      "Epoch 84/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.7503 - sparse_categorical_accuracy: 0.6975\n",
      "Epoch 85/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.6854 - sparse_categorical_accuracy: 0.7375\n",
      "Epoch 86/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.7657 - sparse_categorical_accuracy: 0.6925\n",
      "Epoch 87/100\n",
      "20/20 [==============================] - 63s 3s/step - loss: 0.7744 - sparse_categorical_accuracy: 0.7200\n",
      "Epoch 88/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.7345 - sparse_categorical_accuracy: 0.7125\n",
      "Epoch 89/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.7384 - sparse_categorical_accuracy: 0.7175\n",
      "Epoch 90/100\n",
      "20/20 [==============================] - 63s 3s/step - loss: 0.7336 - sparse_categorical_accuracy: 0.7150\n",
      "Epoch 91/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.7058 - sparse_categorical_accuracy: 0.7525\n",
      "Epoch 92/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.7085 - sparse_categorical_accuracy: 0.6975\n",
      "Epoch 93/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.7181 - sparse_categorical_accuracy: 0.7175\n",
      "Epoch 94/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.7122 - sparse_categorical_accuracy: 0.7025\n",
      "Epoch 95/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.6828 - sparse_categorical_accuracy: 0.7450\n",
      "Epoch 96/100\n",
      "20/20 [==============================] - 63s 3s/step - loss: 0.7145 - sparse_categorical_accuracy: 0.7125\n",
      "Epoch 97/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.7785 - sparse_categorical_accuracy: 0.6725\n",
      "Epoch 98/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.7156 - sparse_categorical_accuracy: 0.7025\n",
      "Epoch 99/100\n",
      "20/20 [==============================] - 63s 3s/step - loss: 0.6810 - sparse_categorical_accuracy: 0.7650\n",
      "Epoch 100/100\n",
      "20/20 [==============================] - 64s 3s/step - loss: 0.7085 - sparse_categorical_accuracy: 0.7375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4dd4df3970>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XlModel(max_files, encoder, block_size, seq_len, embed_dim, vocab_size, num_layers, hidden_size, num_attention_heads, maxlen, memory_length, reuse_length, head_size, inner_size, dropout_rate, attention_dropout_rate, initializer)\n",
    "model.compile(loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False), optimizer = keras.optimizers.Nadam(1e-4), metrics = keras.metrics.SparseCategoricalAccuracy())\n",
    "model.fit(dataset, epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c565d8f6-2e7a-4a97-8a6a-aa3cfc571008",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = model.evaluate(dataset[0][0], dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab0feaa-ccf4-4352-bddc-728529531858",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = model.predict(dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67c3f83-2d73-4c19-a50e-eb8a5c636edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = u.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ae9ebaae-0583-4553-a957-7d1f45958d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 2, 4, 0, 2, 4, 4, 0, 4, 0, 1, 1, 4, 0, 2, 2, 1, 2, 2])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "52eef17b-b311-4277-9bb7-87f05d897473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 3, 0, 4, 4, 4, 3, 3, 0, 1, 1, 3, 4, 3, 2, 1, 2, 2],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7d2c99b9-a5d1-4d9e-a1ee-8d39f8e6dba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/data/dna_samples:v1/train/WS-AG-Apr2016_S85_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/WS-AG-Jul2015_S3_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/WS-AG-Jul2016_S31_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/WS-AG-May2015_S65_L001_R1_001.db',\n",
       " '/data/dna_samples:v1/train/WS-AG-Oct2016_S72_L001_R1_001.db']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[0:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
