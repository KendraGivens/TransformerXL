2023-05-10 13:27:05.437634: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-10 13:27:05.778914: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
wandb: wandb version 0.15.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /home/klg6z/work/TransformerXL/STR_XL/Training/wandb/run-20230510_132710-ebs9ib7f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run toasty-tree-72
wandb: ⭐️ View project at https://wandb.ai/kendragivens/StrXL_Compressed
wandb: 🚀 View run at https://wandb.ai/kendragivens/StrXL_Compressed/runs/ebs9ib7f
2023-05-10 13:27:20.039025: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-10 13:27:22.810395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9632 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:68:00.0, compute capability: 7.5
wandb: Downloading large artifact nachusa-dna:latest, 4079.09MB. 420 files... 
wandb: \ 1 of 420 files downloaded...wandb: | 10 of 420 files downloaded...wandb: / 55 of 420 files downloaded...wandb: - 59 of 420 files downloaded...wandb: \ 63 of 420 files downloaded...wandb: | 64 of 420 files downloaded...wandb: / 65 of 420 files downloaded...wandb: - 72 of 420 files downloaded...wandb: \ 74 of 420 files downloaded...wandb: | 78 of 420 files downloaded...wandb: / 87 of 420 files downloaded...wandb: - 108 of 420 files downloaded...wandb: \ 115 of 420 files downloaded...wandb: | 124 of 420 files downloaded...wandb: / 125 of 420 files downloaded...wandb: - 126 of 420 files downloaded...wandb: \ 127 of 420 files downloaded...wandb: | 128 of 420 files downloaded...wandb: / 129 of 420 files downloaded...wandb: - 130 of 420 files downloaded...wandb: \ 134 of 420 files downloaded...wandb: | 141 of 420 files downloaded...wandb: / 151 of 420 files downloaded...wandb: - 161 of 420 files downloaded...wandb: \ 165 of 420 files downloaded...wandb: | 169 of 420 files downloaded...wandb: / 170 of 420 files downloaded...wandb: - 173 of 420 files downloaded...wandb: \ 175 of 420 files downloaded...wandb: | 178 of 420 files downloaded...wandb: / 182 of 420 files downloaded...wandb: - 186 of 420 files downloaded...wandb: \ 192 of 420 files downloaded...wandb: | 195 of 420 files downloaded...wandb: / 201 of 420 files downloaded...wandb: - 206 of 420 files downloaded...wandb: \ 214 of 420 files downloaded...wandb: | 218 of 420 files downloaded...wandb: / 225 of 420 files downloaded...wandb: - 230 of 420 files downloaded...wandb: \ 233 of 420 files downloaded...wandb: | 235 of 420 files downloaded...wandb: / 237 of 420 files downloaded...wandb: - 238 of 420 files downloaded...wandb: \ 241 of 420 files downloaded...wandb: | 244 of 420 files downloaded...wandb: / 250 of 420 files downloaded...wandb: - 257 of 420 files downloaded...wandb: \ 262 of 420 files downloaded...wandb: | 269 of 420 files downloaded...wandb: / 275 of 420 files downloaded...wandb: - 283 of 420 files downloaded...wandb: \ 293 of 420 files downloaded...wandb: | 299 of 420 files downloaded...wandb: / 315 of 420 files downloaded...wandb: - 319 of 420 files downloaded...wandb: \ 321 of 420 files downloaded...wandb: | 323 of 420 files downloaded...wandb: / 324 of 420 files downloaded...wandb: - 329 of 420 files downloaded...wandb: \ 336 of 420 files downloaded...wandb: | 342 of 420 files downloaded...wandb: / 353 of 420 files downloaded...wandb: - 358 of 420 files downloaded...wandb: \ 367 of 420 files downloaded...wandb: | 372 of 420 files downloaded...wandb: / 383 of 420 files downloaded...wandb: - 386 of 420 files downloaded...wandb: \ 399 of 420 files downloaded...wandb: | 417 of 420 files downloaded...wandb:   420 of 420 files downloaded.  
Done. 0:0:12.4
wandb: \ 4 of 5 files downloaded...wandb:   5 of 5 files downloaded.  
2023-05-10 13:27:47.458728: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_2"
op: "FlatMapDataset"
input: "TensorDataset/_1"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_flat_map_fn_64261"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\020FlatMapDataset:1"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
        dim {
          size: -1
        }
        dim {
          size: -1
        }
      }
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT32
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT32
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
Using GPU Strategy. Selected GPUs: [0]
Sample './artifacts/nachusa-dna:v0/Wes7-PCRblank1_S8_L001_R1_001.db' does not contain enough sequences. This sample will be ignored.
Sample './artifacts/nachusa-dna:v0/Wes24-PCRblank2_S25_L001_R1_001.db' does not contain enough sequences. This sample will be ignored.
Sample './artifacts/nachusa-dna:v0/Wes44-PCRblank3_S45_L001_R1_001.db' does not contain enough sequences. This sample will be ignored.
Sample './artifacts/nachusa-dna:v0/Wes7-PCRblank1_S8_L001_R1_001.db' does not contain enough sequences. This sample will be ignored.
Sample './artifacts/nachusa-dna:v0/Wesley056-NegCtrl_S195_L001_R1_001.db' does not contain enough sequences. This sample will be ignored.
Sample './artifacts/nachusa-dna:v0/Wes24-PCRblank2_S25_L001_R1_001.db' does not contain enough sequences. This sample will be ignored.
Sample './artifacts/nachusa-dna:v0/Wesley049-SB-100420_S188_L001_R1_001.db' does not contain enough sequences. This sample will be ignored.
 1/20 [>.............................] - ETA: 15:28 - loss: 5.7216 - sparse_categorical_accuracy: 0.0000e+00 2/20 [==>...........................] - ETA: 1:52 - loss: 5.6398 - sparse_categorical_accuracy: 0.0000e+00  3/20 [===>..........................] - ETA: 1:46 - loss: 5.6483 - sparse_categorical_accuracy: 0.0000e+00 4/20 [=====>........................] - ETA: 1:40 - loss: 5.6259 - sparse_categorical_accuracy: 0.0000e+00 5/20 [======>.......................] - ETA: 1:34 - loss: 5.6672 - sparse_categorical_accuracy: 0.0000e+00 6/20 [========>.....................] - ETA: 1:28 - loss: 5.7173 - sparse_categorical_accuracy: 0.0000e+00 7/20 [=========>....................] - ETA: 1:21 - loss: 5.7059 - sparse_categorical_accuracy: 0.0000e+00 8/20 [===========>..................] - ETA: 1:15 - loss: 5.6606 - sparse_categorical_accuracy: 0.0000e+00 9/20 [============>.................] - ETA: 1:09 - loss: 5.6428 - sparse_categorical_accuracy: 0.0000e+0010/20 [==============>...............] - ETA: 1:03 - loss: 5.6228 - sparse_categorical_accuracy: 0.0000e+0011/20 [===============>..............] - ETA: 56s - loss: 5.6217 - sparse_categorical_accuracy: 0.0000e+00 12/20 [=================>............] - ETA: 50s - loss: 5.6288 - sparse_categorical_accuracy: 0.0000e+0013/20 [==================>...........] - ETA: 44s - loss: 5.6280 - sparse_categorical_accuracy: 0.0000e+0014/20 [====================>.........] - ETA: 37s - loss: 5.6183 - sparse_categorical_accuracy: 0.0000e+0015/20 [=====================>........] - ETA: 31s - loss: 5.6325 - sparse_categorical_accuracy: 0.0000e+0016/20 [=======================>......] - ETA: 25s - loss: 5.6122 - sparse_categorical_accuracy: 0.0000e+0017/20 [========================>.....] - ETA: 18s - loss: 5.5844 - sparse_categorical_accuracy: 0.0000e+0018/20 [==========================>...] - ETA: 12s - loss: 5.5853 - sparse_categorical_accuracy: 0.0000e+0019/20 [===========================>..] - ETA: 6s - loss: 5.5806 - sparse_categorical_accuracy: 0.0000e+00 20/20 [==============================] - ETA: 0s - loss: 5.5685 - sparse_categorical_accuracy: 0.0000e+002023-05-10 13:30:36.521529: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_2"
op: "FlatMapDataset"
input: "TensorDataset/_1"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_flat_map_fn_145340"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\021FlatMapDataset:17"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
        dim {
          size: -1
        }
        dim {
          size: -1
        }
      }
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT32
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT32
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
20/20 [==============================] - 206s 8s/step - loss: 5.5685 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 5.4622 - val_sparse_categorical_accuracy: 0.0000e+00
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: | 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: / 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: \ 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                           epoch ▁
wandb:                            loss ▁
wandb:     sparse_categorical_accuracy ▁
wandb:                        val_loss ▁
wandb: val_sparse_categorical_accuracy ▁
wandb: 
wandb: Run summary:
wandb:                      best_epoch 0
wandb:                   best_val_loss 5.46221
wandb:                           epoch 0
wandb:                            loss 5.56846
wandb:     sparse_categorical_accuracy 0.0
wandb:                        val_loss 5.46221
wandb: val_sparse_categorical_accuracy 0.0
wandb: 
wandb: 🚀 View run toasty-tree-72 at: https://wandb.ai/kendragivens/StrXL_Compressed/runs/ebs9ib7f
wandb: Synced 6 W&B file(s), 1 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20230510_132710-ebs9ib7f/logs
