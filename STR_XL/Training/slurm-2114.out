2023-05-06 20:14:10.661885: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-06 20:14:11.015993: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
wandb: wandb version 0.15.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /home/klg6z/work/TransformerXL/STR_XL/Training/wandb/run-20230506_201415-ecblc38c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zany-cosmos-46
wandb: ⭐️ View project at https://wandb.ai/kendragivens/StrXL_Compressed
wandb: 🚀 View run at https://wandb.ai/kendragivens/StrXL_Compressed/runs/ecblc38c
2023-05-06 20:14:25.576418: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-06 20:14:27.712740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9632 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:68:00.0, compute capability: 7.5
wandb: Downloading large artifact nachusa-dna:latest, 4079.09MB. 420 files... 
wandb: \ 1 of 420 files downloaded...wandb: | 45 of 420 files downloaded...wandb: / 62 of 420 files downloaded...wandb: - 65 of 420 files downloaded...wandb: \ 70 of 420 files downloaded...wandb: | 87 of 420 files downloaded...wandb: / 118 of 420 files downloaded...wandb: - 126 of 420 files downloaded...wandb: \ 137 of 420 files downloaded...wandb: | 147 of 420 files downloaded...wandb: / 163 of 420 files downloaded...wandb: - 173 of 420 files downloaded...wandb: \ 182 of 420 files downloaded...wandb: | 191 of 420 files downloaded...wandb: / 198 of 420 files downloaded...wandb: - 209 of 420 files downloaded...wandb: \ 230 of 420 files downloaded...wandb: | 246 of 420 files downloaded...wandb: / 259 of 420 files downloaded...wandb: - 270 of 420 files downloaded...wandb: \ 286 of 420 files downloaded...wandb: | 302 of 420 files downloaded...wandb: / 334 of 420 files downloaded...wandb: - 352 of 420 files downloaded...wandb: \ 390 of 420 files downloaded...wandb: | 398 of 420 files downloaded...wandb: / 409 of 420 files downloaded...wandb: - 419 of 420 files downloaded...wandb:   420 of 420 files downloaded.  
Done. 0:0:4.0
wandb:   5 of 5 files downloaded.  
2023-05-06 20:14:43.934826: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_2"
op: "FlatMapDataset"
input: "TensorDataset/_1"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_flat_map_fn_70442"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\020FlatMapDataset:1"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
        dim {
          size: -1
        }
        dim {
          size: -1
        }
      }
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT32
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT32
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
Using GPU Strategy. Selected GPUs: [0]
Sample './artifacts/nachusa-dna:v0/Wes7-PCRblank1_S8_L001_R1_001.db' does not contain enough sequences. This sample will be ignored.
Sample './artifacts/nachusa-dna:v0/Wes24-PCRblank2_S25_L001_R1_001.db' does not contain enough sequences. This sample will be ignored.
Sample './artifacts/nachusa-dna:v0/Wes44-PCRblank3_S45_L001_R1_001.db' does not contain enough sequences. This sample will be ignored.
Sample './artifacts/nachusa-dna:v0/Wes7-PCRblank1_S8_L001_R1_001.db' does not contain enough sequences. This sample will be ignored.
Sample './artifacts/nachusa-dna:v0/Wesley056-NegCtrl_S195_L001_R1_001.db' does not contain enough sequences. This sample will be ignored.
Sample './artifacts/nachusa-dna:v0/Wes24-PCRblank2_S25_L001_R1_001.db' does not contain enough sequences. This sample will be ignored.
Sample './artifacts/nachusa-dna:v0/Wesley049-SB-100420_S188_L001_R1_001.db' does not contain enough sequences. This sample will be ignored.
Epoch 1/750
WARNING:tensorflow:Gradients do not exist for variables ['Seeds:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention/multi_head_attention_block/multi_head_attention_1/query/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention/multi_head_attention_block/multi_head_attention_1/query/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention/multi_head_attention_block/multi_head_attention_1/key/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention/multi_head_attention_block/multi_head_attention_1/key/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention/multi_head_attention_block/multi_head_attention_1/value/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention/multi_head_attention_block/multi_head_attention_1/value/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention/multi_head_attention_block/multi_head_attention_1/attention_output/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention/multi_head_attention_block/multi_head_attention_1/attention_output/bias:0', 'dense_20/kernel:0', 'dense_20/bias:0', 'dense_21/kernel:0', 'dense_21/bias:0', 'Seeds:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_1/multi_head_attention_block_1/multi_head_attention_3/query/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_1/multi_head_attention_block_1/multi_head_attention_3/query/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_1/multi_head_attention_block_1/multi_head_attention_3/key/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_1/multi_head_attention_block_1/multi_head_attention_3/key/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_1/multi_head_attention_block_1/multi_head_attention_3/value/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_1/multi_head_attention_block_1/multi_head_attention_3/value/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_1/multi_head_attention_block_1/multi_head_attention_3/attention_output/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_1/multi_head_attention_block_1/multi_head_attention_3/attention_output/bias:0', 'dense_24/kernel:0', 'dense_24/bias:0', 'dense_25/kernel:0', 'dense_25/bias:0', 'Seeds:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_2/multi_head_attention_block_2/multi_head_attention_5/query/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_2/multi_head_attention_block_2/multi_head_attention_5/query/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_2/multi_head_attention_block_2/multi_head_attention_5/key/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_2/multi_head_attention_block_2/multi_head_attention_5/key/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_2/multi_head_attention_block_2/multi_head_attention_5/value/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_2/multi_head_attention_block_2/multi_head_attention_5/value/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_2/multi_head_attention_block_2/multi_head_attention_5/attention_output/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_2/multi_head_attention_block_2/multi_head_attention_5/attention_output/bias:0', 'dense_28/kernel:0', 'dense_28/bias:0', 'dense_29/kernel:0', 'dense_29/bias:0', 'Seeds:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_3/multi_head_attention_block_3/multi_head_attention_7/query/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_3/multi_head_attention_block_3/multi_head_attention_7/query/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_3/multi_head_attention_block_3/multi_head_attention_7/key/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_3/multi_head_attention_block_3/multi_head_attention_7/key/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_3/multi_head_attention_block_3/multi_head_attention_7/value/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_3/multi_head_attention_block_3/multi_head_attention_7/value/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_3/multi_head_attention_block_3/multi_head_attention_7/attention_output/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_3/multi_head_attention_block_3/multi_head_attention_7/attention_output/bias:0', 'dense_32/kernel:0', 'dense_32/bias:0', 'dense_33/kernel:0', 'dense_33/bias:0', 'Seeds:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_4/multi_head_attention_block_4/multi_head_attention_9/query/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_4/multi_head_attention_block_4/multi_head_attention_9/query/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_4/multi_head_attention_block_4/multi_head_attention_9/key/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_4/multi_head_attention_block_4/multi_head_attention_9/key/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_4/multi_head_attention_block_4/multi_head_attention_9/value/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_4/multi_head_attention_block_4/multi_head_attention_9/value/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_4/multi_head_attention_block_4/multi_head_attention_9/attention_output/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_4/multi_head_attention_block_4/multi_head_attention_9/attention_output/bias:0', 'dense_36/kernel:0', 'dense_36/bias:0', 'dense_37/kernel:0', 'dense_37/bias:0', 'Seeds:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_5/multi_head_attention_block_5/multi_head_attention_11/query/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_5/multi_head_attention_block_5/multi_head_attention_11/query/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_5/multi_head_attention_block_5/multi_head_attention_11/key/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_5/multi_head_attention_block_5/multi_head_attention_11/key/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_5/multi_head_attention_block_5/multi_head_attention_11/value/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_5/multi_head_attention_block_5/multi_head_attention_11/value/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_5/multi_head_attention_block_5/multi_head_attention_11/attention_output/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_5/multi_head_attention_block_5/multi_head_attention_11/attention_output/bias:0', 'dense_40/kernel:0', 'dense_40/bias:0', 'dense_41/kernel:0', 'dense_41/bias:0', 'Seeds:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_6/multi_head_attention_block_6/multi_head_attention_13/query/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_6/multi_head_attention_block_6/multi_head_attention_13/query/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_6/multi_head_attention_block_6/multi_head_attention_13/key/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_6/multi_head_attention_block_6/multi_head_attention_13/key/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_6/multi_head_attention_block_6/multi_head_attention_13/value/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_6/multi_head_attention_block_6/multi_head_attention_13/value/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_6/multi_head_attention_block_6/multi_head_attention_13/attention_output/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_6/multi_head_attention_block_6/multi_head_attention_13/attention_output/bias:0', 'dense_44/kernel:0', 'dense_44/bias:0', 'dense_45/kernel:0', 'dense_45/bias:0', 'Seeds:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_7/multi_head_attention_block_7/multi_head_attention_15/query/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_7/multi_head_attention_block_7/multi_head_attention_15/query/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_7/multi_head_attention_block_7/multi_head_attention_15/key/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_7/multi_head_attention_block_7/multi_head_attention_15/key/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_7/multi_head_attention_block_7/multi_head_attention_15/value/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_7/multi_head_attention_block_7/multi_head_attention_15/value/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_7/multi_head_attention_block_7/multi_head_attention_15/attention_output/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_7/multi_head_attention_block_7/multi_head_attention_15/attention_output/bias:0', 'dense_48/kernel:0', 'dense_48/bias:0', 'dense_49/kernel:0', 'dense_49/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?
WARNING:tensorflow:Gradients do not exist for variables ['Seeds:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention/multi_head_attention_block/multi_head_attention_1/query/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention/multi_head_attention_block/multi_head_attention_1/query/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention/multi_head_attention_block/multi_head_attention_1/key/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention/multi_head_attention_block/multi_head_attention_1/key/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention/multi_head_attention_block/multi_head_attention_1/value/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention/multi_head_attention_block/multi_head_attention_1/value/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention/multi_head_attention_block/multi_head_attention_1/attention_output/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention/multi_head_attention_block/multi_head_attention_1/attention_output/bias:0', 'dense_20/kernel:0', 'dense_20/bias:0', 'dense_21/kernel:0', 'dense_21/bias:0', 'Seeds:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_1/multi_head_attention_block_1/multi_head_attention_3/query/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_1/multi_head_attention_block_1/multi_head_attention_3/query/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_1/multi_head_attention_block_1/multi_head_attention_3/key/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_1/multi_head_attention_block_1/multi_head_attention_3/key/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_1/multi_head_attention_block_1/multi_head_attention_3/value/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_1/multi_head_attention_block_1/multi_head_attention_3/value/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_1/multi_head_attention_block_1/multi_head_attention_3/attention_output/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_1/multi_head_attention_block_1/multi_head_attention_3/attention_output/bias:0', 'dense_24/kernel:0', 'dense_24/bias:0', 'dense_25/kernel:0', 'dense_25/bias:0', 'Seeds:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_2/multi_head_attention_block_2/multi_head_attention_5/query/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_2/multi_head_attention_block_2/multi_head_attention_5/query/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_2/multi_head_attention_block_2/multi_head_attention_5/key/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_2/multi_head_attention_block_2/multi_head_attention_5/key/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_2/multi_head_attention_block_2/multi_head_attention_5/value/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_2/multi_head_attention_block_2/multi_head_attention_5/value/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_2/multi_head_attention_block_2/multi_head_attention_5/attention_output/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_2/multi_head_attention_block_2/multi_head_attention_5/attention_output/bias:0', 'dense_28/kernel:0', 'dense_28/bias:0', 'dense_29/kernel:0', 'dense_29/bias:0', 'Seeds:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_3/multi_head_attention_block_3/multi_head_attention_7/query/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_3/multi_head_attention_block_3/multi_head_attention_7/query/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_3/multi_head_attention_block_3/multi_head_attention_7/key/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_3/multi_head_attention_block_3/multi_head_attention_7/key/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_3/multi_head_attention_block_3/multi_head_attention_7/value/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_3/multi_head_attention_block_3/multi_head_attention_7/value/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_3/multi_head_attention_block_3/multi_head_attention_7/attention_output/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_3/multi_head_attention_block_3/multi_head_attention_7/attention_output/bias:0', 'dense_32/kernel:0', 'dense_32/bias:0', 'dense_33/kernel:0', 'dense_33/bias:0', 'Seeds:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_4/multi_head_attention_block_4/multi_head_attention_9/query/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_4/multi_head_attention_block_4/multi_head_attention_9/query/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_4/multi_head_attention_block_4/multi_head_attention_9/key/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_4/multi_head_attention_block_4/multi_head_attention_9/key/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_4/multi_head_attention_block_4/multi_head_attention_9/value/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_4/multi_head_attention_block_4/multi_head_attention_9/value/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_4/multi_head_attention_block_4/multi_head_attention_9/attention_output/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_4/multi_head_attention_block_4/multi_head_attention_9/attention_output/bias:0', 'dense_36/kernel:0', 'dense_36/bias:0', 'dense_37/kernel:0', 'dense_37/bias:0', 'Seeds:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_5/multi_head_attention_block_5/multi_head_attention_11/query/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_5/multi_head_attention_block_5/multi_head_attention_11/query/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_5/multi_head_attention_block_5/multi_head_attention_11/key/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_5/multi_head_attention_block_5/multi_head_attention_11/key/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_5/multi_head_attention_block_5/multi_head_attention_11/value/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_5/multi_head_attention_block_5/multi_head_attention_11/value/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_5/multi_head_attention_block_5/multi_head_attention_11/attention_output/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_5/multi_head_attention_block_5/multi_head_attention_11/attention_output/bias:0', 'dense_40/kernel:0', 'dense_40/bias:0', 'dense_41/kernel:0', 'dense_41/bias:0', 'Seeds:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_6/multi_head_attention_block_6/multi_head_attention_13/query/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_6/multi_head_attention_block_6/multi_head_attention_13/query/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_6/multi_head_attention_block_6/multi_head_attention_13/key/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_6/multi_head_attention_block_6/multi_head_attention_13/key/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_6/multi_head_attention_block_6/multi_head_attention_13/value/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_6/multi_head_attention_block_6/multi_head_attention_13/value/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_6/multi_head_attention_block_6/multi_head_attention_13/attention_output/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_6/multi_head_attention_block_6/multi_head_attention_13/attention_output/bias:0', 'dense_44/kernel:0', 'dense_44/bias:0', 'dense_45/kernel:0', 'dense_45/bias:0', 'Seeds:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_7/multi_head_attention_block_7/multi_head_attention_15/query/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_7/multi_head_attention_block_7/multi_head_attention_15/query/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_7/multi_head_attention_block_7/multi_head_attention_15/key/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_7/multi_head_attention_block_7/multi_head_attention_15/key/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_7/multi_head_attention_block_7/multi_head_attention_15/value/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_7/multi_head_attention_block_7/multi_head_attention_15/value/bias:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_7/multi_head_attention_block_7/multi_head_attention_15/attention_output/kernel:0', 'xl_model/transformer_xl/pooling_by_multi_head_attention_7/multi_head_attention_block_7/multi_head_attention_15/attention_output/bias:0', 'dense_48/kernel:0', 'dense_48/bias:0', 'dense_49/kernel:0', 'dense_49/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?
 1/20 [>.............................] - ETA: 11:39 - loss: 5.6893 - sparse_categorical_accuracy: 0.0000e+00 2/20 [==>...........................] - ETA: 1:51 - loss: 5.5940 - sparse_categorical_accuracy: 0.0250      3/20 [===>..........................] - ETA: 1:45 - loss: 5.5995 - sparse_categorical_accuracy: 0.0167 4/20 [=====>........................] - ETA: 1:39 - loss: 5.5882 - sparse_categorical_accuracy: 0.0125 5/20 [======>.......................] - ETA: 1:33 - loss: 5.6396 - sparse_categorical_accuracy: 0.0100 6/20 [========>.....................] - ETA: 1:26 - loss: 5.6730 - sparse_categorical_accuracy: 0.0083 7/20 [=========>....................] - ETA: 1:20 - loss: 5.6767 - sparse_categorical_accuracy: 0.0071 8/20 [===========>..................] - ETA: 1:14 - loss: 5.6487 - sparse_categorical_accuracy: 0.0063 9/20 [============>.................] - ETA: 1:08 - loss: 5.6106 - sparse_categorical_accuracy: 0.005610/20 [==============>...............] - ETA: 1:01 - loss: 5.5912 - sparse_categorical_accuracy: 0.005011/20 [===============>..............] - ETA: 55s - loss: 5.5942 - sparse_categorical_accuracy: 0.0045 12/20 [=================>............] - ETA: 49s - loss: 5.5981 - sparse_categorical_accuracy: 0.004213/20 [==================>...........] - ETA: 43s - loss: 5.5914 - sparse_categorical_accuracy: 0.003814/20 [====================>.........] - ETA: 37s - loss: 5.5900 - sparse_categorical_accuracy: 0.003615/20 [=====================>........] - ETA: 30s - loss: 5.6004 - sparse_categorical_accuracy: 0.003316/20 [=======================>......] - ETA: 24s - loss: 5.5843 - sparse_categorical_accuracy: 0.003117/20 [========================>.....] - ETA: 18s - loss: 5.5647 - sparse_categorical_accuracy: 0.002918/20 [==========================>...] - ETA: 12s - loss: 5.5672 - sparse_categorical_accuracy: 0.002819/20 [===========================>..] - ETA: 6s - loss: 5.5585 - sparse_categorical_accuracy: 0.0026 20/20 [==============================] - ETA: 0s - loss: 5.5488 - sparse_categorical_accuracy: 0.00252023-05-06 20:17:18.654299: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: "FlatMapDataset/_2"
op: "FlatMapDataset"
input: "TensorDataset/_1"
attr {
  key: "Targuments"
  value {
    list {
    }
  }
}
attr {
  key: "_cardinality"
  value {
    i: -2
  }
}
attr {
  key: "f"
  value {
    func {
      name: "__inference_Dataset_flat_map_flat_map_fn_131163"
    }
  }
}
attr {
  key: "metadata"
  value {
    s: "\n\021FlatMapDataset:17"
  }
}
attr {
  key: "output_shapes"
  value {
    list {
      shape {
        dim {
          size: -1
        }
        dim {
          size: -1
        }
        dim {
          size: -1
        }
      }
      shape {
        dim {
          size: -1
        }
      }
    }
  }
}
attr {
  key: "output_types"
  value {
    list {
      type: DT_FLOAT
      type: DT_INT32
    }
  }
}
experimental_type {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_DATASET
    args {
      type_id: TFT_PRODUCT
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_FLOAT
        }
      }
      args {
        type_id: TFT_TENSOR
        args {
          type_id: TFT_INT32
        }
      }
    }
  }
}
. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.
20/20 [==============================] - 196s 8s/step - loss: 5.5488 - sparse_categorical_accuracy: 0.0025 - val_loss: 5.4385 - val_sparse_categorical_accuracy: 0.0100
Epoch 2/750
 1/20 [>.............................] - ETA: 2:00 - loss: 5.6197 - sparse_categorical_accuracy: 0.0000e+00 2/20 [==>...........................] - ETA: 1:51 - loss: 5.6230 - sparse_categorical_accuracy: 0.0000e+00 3/20 [===>..........................] - ETA: 1:45 - loss: 5.5947 - sparse_categorical_accuracy: 0.0000e+00 4/20 [=====>........................] - ETA: 1:39 - loss: 5.5795 - sparse_categorical_accuracy: 0.0000e+00 5/20 [======>.......................] - ETA: 1:32 - loss: 5.5693 - sparse_categorical_accuracy: 0.0000e+00 6/20 [========>.....................] - ETA: 1:26 - loss: 5.5517 - sparse_categorical_accuracy: 0.0000e+00 7/20 [=========>....................] - ETA: 1:20 - loss: 5.5354 - sparse_categorical_accuracy: 0.0071     8/20 [===========>..................] - ETA: 1:14 - loss: 5.5301 - sparse_categorical_accuracy: 0.0063 9/20 [============>.................] - ETA: 1:08 - loss: 5.5287 - sparse_categorical_accuracy: 0.005610/20 [==============>...............] - ETA: 1:02 - loss: 5.5266 - sparse_categorical_accuracy: 0.005011/20 [===============>..............] - ETA: 55s - loss: 5.5376 - sparse_categorical_accuracy: 0.0045 12/20 [=================>............] - ETA: 49s - loss: 5.5384 - sparse_categorical_accuracy: 0.004213/20 [==================>...........] - ETA: 43s - loss: 5.5463 - sparse_categorical_accuracy: 0.003814/20 [====================>.........] - ETA: 37s - loss: 5.5453 - sparse_categorical_accuracy: 0.003615/20 [=====================>........] - ETA: 31s - loss: 5.5392 - sparse_categorical_accuracy: 0.003316/20 [=======================>......] - ETA: 24s - loss: 5.5521 - sparse_categorical_accuracy: 0.003117/20 [========================>.....] - ETA: 18s - loss: 5.5450 - sparse_categorical_accuracy: 0.002918/20 [==========================>...] - ETA: 12s - loss: 5.5308 - sparse_categorical_accuracy: 0.002819/20 [===========================>..] - ETA: 6s - loss: 5.5297 - sparse_categorical_accuracy: 0.0026 20/20 [==============================] - ETA: 0s - loss: 5.5208 - sparse_categorical_accuracy: 0.002520/20 [==============================] - 154s 8s/step - loss: 5.5208 - sparse_categorical_accuracy: 0.0025 - val_loss: 5.3984 - val_sparse_categorical_accuracy: 0.0000e+00
Epoch 3/750
 1/20 [>.............................] - ETA: 2:00 - loss: 5.3818 - sparse_categorical_accuracy: 0.0000e+00 2/20 [==>...........................] - ETA: 1:51 - loss: 5.4542 - sparse_categorical_accuracy: 0.0000e+00 3/20 [===>..........................] - ETA: 1:45 - loss: 5.3637 - sparse_categorical_accuracy: 0.0000e+00 4/20 [=====>........................] - ETA: 1:39 - loss: 5.3915 - sparse_categorical_accuracy: 0.0000e+00 5/20 [======>.......................] - ETA: 1:33 - loss: 5.4266 - sparse_categorical_accuracy: 0.0000e+00 6/20 [========>.....................] - ETA: 1:26 - loss: 5.3805 - sparse_categorical_accuracy: 0.0000e+00 7/20 [=========>....................] - ETA: 1:20 - loss: 5.3876 - sparse_categorical_accuracy: 0.0000e+00 8/20 [===========>..................] - ETA: 1:14 - loss: 5.3565 - sparse_categorical_accuracy: 0.0000e+00 9/20 [============>.................] - ETA: 1:08 - loss: 5.3616 - sparse_categorical_accuracy: 0.0000e+0010/20 [==============>...............] - ETA: 1:02 - loss: 5.3518 - sparse_categorical_accuracy: 0.0000e+0011/20 [===============>..............] - ETA: 55s - loss: 5.3520 - sparse_categorical_accuracy: 0.0000e+00 12/20 [=================>............] - ETA: 49s - loss: 5.3680 - sparse_categorical_accuracy: 0.0000e+0013/20 [==================>...........] - ETA: 43s - loss: 5.3542 - sparse_categorical_accuracy: 0.0000e+0014/20 [====================>.........] - ETA: 37s - loss: 5.3543 - sparse_categorical_accuracy: 0.0000e+0015/20 [=====================>........] - ETA: 31s - loss: 5.3286 - sparse_categorical_accuracy: 0.0000e+0016/20 [=======================>......] - ETA: 24s - loss: 5.3298 - sparse_categorical_accuracy: 0.0031    17/20 [========================>.....] - ETA: 18s - loss: 5.3316 - sparse_categorical_accuracy: 0.005918/20 [==========================>...] - ETA: 12s - loss: 5.3291 - sparse_categorical_accuracy: 0.005619/20 [===========================>..] - ETA: 6s - loss: 5.3157 - sparse_categorical_accuracy: 0.0105 20/20 [==============================] - ETA: 0s - loss: 5.3067 - sparse_categorical_accuracy: 0.010020/20 [==============================] - 154s 8s/step - loss: 5.3067 - sparse_categorical_accuracy: 0.0100 - val_loss: 5.1004 - val_sparse_categorical_accuracy: 0.0200
Epoch 4/750
 1/20 [>.............................] - ETA: 2:00 - loss: 5.1667 - sparse_categorical_accuracy: 0.0000e+00 2/20 [==>...........................] - ETA: 1:51 - loss: 5.1409 - sparse_categorical_accuracy: 0.0250     3/20 [===>..........................] - ETA: 1:45 - loss: 5.1291 - sparse_categorical_accuracy: 0.0167 4/20 [=====>........................] - ETA: 1:39 - loss: 5.1307 - sparse_categorical_accuracy: 0.0125 5/20 [======>.......................] - ETA: 1:33 - loss: 5.1329 - sparse_categorical_accuracy: 0.0100 6/20 [========>.....................] - ETA: 1:27 - loss: 5.1253 - sparse_categorical_accuracy: 0.0083 7/20 [=========>....................] - ETA: 1:20 - loss: 5.1245 - sparse_categorical_accuracy: 0.0071 8/20 [===========>..................] - ETA: 1:14 - loss: 5.1134 - sparse_categorical_accuracy: 0.0063 9/20 [============>.................] - ETA: 1:08 - loss: 5.1071 - sparse_categorical_accuracy: 0.005610/20 [==============>...............] - ETA: 1:02 - loss: 5.1371 - sparse_categorical_accuracy: 0.005011/20 [===============>..............] - ETA: 55s - loss: 5.1410 - sparse_categorical_accuracy: 0.0045 12/20 [=================>............] - ETA: 49s - loss: 5.1384 - sparse_categorical_accuracy: 0.004213/20 [==================>...........] - ETA: 43s - loss: 5.1256 - sparse_categorical_accuracy: 0.0038slurmstepd: error: *** JOB 2114 ON c3 CANCELLED AT 2023-05-06T20:24:32 ***
