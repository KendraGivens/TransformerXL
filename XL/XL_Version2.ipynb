{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dcad43d-67f7-49b9-84ce-6d91a0f468a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e7cb9df-5746-44a3-998a-b671b284957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fda64db1-8058-4e15-b2d4-1a3e64f23fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "import math\n",
    "import string\n",
    "\n",
    "import tf_utils as tfu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dfb3b1a-2db7-4463-a208-b5d29e20f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tfu.strategy.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54593a0f-b5b4-401e-91e2-8c1a1598432e",
   "metadata": {},
   "source": [
    "---\n",
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddb61447-8228-4b92-8f3f-0eddb8e33574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-07-14 11:51:00--  https://www.cs.mtsu.edu/~jphillips/courses/CSCI4850-5850/public/PandP_Jane_Austen.txt\n",
      "Resolving www.cs.mtsu.edu (www.cs.mtsu.edu)... 161.45.162.100\n",
      "Connecting to www.cs.mtsu.edu (www.cs.mtsu.edu)|161.45.162.100|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 683838 (668K) [text/plain]\n",
      "Saving to: ‘PandP_Jane_Austen.txt.2’\n",
      "\n",
      "PandP_Jane_Austen.t 100%[===================>] 667.81K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2022-07-14 11:51:00 (11.7 MB/s) - ‘PandP_Jane_Austen.txt.2’ saved [683838/683838]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10657, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!wget https://www.cs.mtsu.edu/~jphillips/courses/CSCI4850-5850/public/PandP_Jane_Austen.txt\n",
    "with open('PandP_Jane_Austen.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "# Paragraphs are separated by blank\n",
    "# lines -> just drop those lines...\n",
    "text = []\n",
    "for i in range(len(lines)):\n",
    "     if lines[i] != '':\n",
    "        text = text + [lines[i]]\n",
    "data = np.vstack([[text[0:-1]],[text[1:]]]).T\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5f65884-85ad-4ebd-ba51-29430504b0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq = 100\n",
    "split_point = 80\n",
    "data = data[0:n_seq]\n",
    "np.random.shuffle(data) # In-place modification\n",
    "max_length = np.max([len(i) for i in data.flatten()]) + 2 # Add start/stop\n",
    "max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689bbd23-6cbf-472f-bd6f-dc4a008b9072",
   "metadata": {},
   "source": [
    "---\n",
    "# Encode Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09c4c6ef-93df-4294-af8f-2fbaf01ca9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_seq(x,mapping,max_length=0):\n",
    "    # String to integer\n",
    "    return [mapping['<START>']] + \\\n",
    "    [mapping[i] for i in list(x)] + \\\n",
    "    [mapping['<STOP>']] + \\\n",
    "    [0]*(max_length-len(list(x))-2)\n",
    "def decode_seq(x,mapping):\n",
    "    # Integer-to-string\n",
    "    try:\n",
    "        idx = list(x).index(2) # Stop token?\n",
    "    except:\n",
    "        idx = len(list(x)) # No stop token found\n",
    "    return ''.join([mapping[i] for i in list(x)[0:idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76b7c608-fe15-4420-a4fb-8356ebdcefc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_to_c_pandp = ['','<START>','<STOP>'] + list({char for word in data[:,0] for char in word})\n",
    "c_to_i_pandp = {i_to_c_pandp[i]:i for i in range(len(i_to_c_pandp))}\n",
    "i_to_c_pandp[1] = i_to_c_pandp[2] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6889059-fdd9-43e2-b0bc-199ff07c4199",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack([encode_seq(x,c_to_i_pandp,max_length) for x in data[:,0]])\n",
    "Y = np.vstack([encode_seq(x,c_to_i_pandp,max_length) for x in data[:,1]])\n",
    "x_train = X[:split_point]\n",
    "x_test = X[split_point:]\n",
    "pre_y_train = Y[:,0:-1][:split_point]\n",
    "pre_y_test = Y[:,0:-1][split_point:]\n",
    "post_y_train = Y[:,1:][:split_point]\n",
    "post_y_test = Y[:,1:][split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca664bb9-544b-4264-8da4-34acaca2d379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 74)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6a03188-6146-4487-8483-b0a3296d4426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 36, 51,  3, 29,  8, 50, 33, 11, 23, 19, 28, 51, 11, 11, 35, 33,\n",
       "       36,  6, 19, 35, 50, 23, 19, 23, 20,  9, 51, 26, 33, 19, 37, 51, 23,\n",
       "       19, 55, 35, 23, 35, 50, 35, 45, 29, 19, 51, 45, 36, 19, 45, 33, 37,\n",
       "       23, 40,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "154bc9ef-0e64-4ad5-96d4-13012994718f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([36, 51,  3, 29,  8, 50, 33, 11, 23, 19, 28, 51, 11, 11, 35, 33, 36,\n",
       "        6, 19, 35, 50, 23, 19, 23, 20,  9, 51, 26, 33, 19, 37, 51, 23, 19,\n",
       "       55, 35, 23, 35, 50, 35, 45, 29, 19, 51, 45, 36, 19, 45, 33, 37, 23,\n",
       "       40,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2e33b84-c781-43ae-a1f6-5c721f8a5697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1, 23,  8, ...,  0,  0,  0],\n",
       "       [ 1, 44, 51, ..., 29,  2,  0],\n",
       "       [ 1, 20, 17, ..., 45,  2,  0],\n",
       "       ...,\n",
       "       [ 1, 39, 14, ..., 20, 11,  2],\n",
       "       [ 1, 39, 25, ...,  0,  0,  0],\n",
       "       [ 1, 32, 11, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9be57f48-a1da-4715-a5ea-fcc9def518a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 23  8 ...  0  0  0] \n",
      " [ 1 36 51 ... 51 36  2] \n",
      " [36 51  3 ... 36  2  0]\n"
     ]
    }
   ],
   "source": [
    "x_flattened = x_train.flatten()\n",
    "pre_y_flattened = pre_y_train.flatten()\n",
    "post_y_flattened = post_y_train.flatten()\n",
    "print(x_flattened, '\\n', pre_y_flattened, '\\n', post_y_flattened)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd768ff5-2158-4e46-9852-2551bf73d350",
   "metadata": {},
   "source": [
    "---\n",
    "# Flatten Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7368c128-c19b-403d-9408-a08347705c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_flattened = x_flattened[x_flattened != 0]\n",
    "x_flattened = x_flattened[x_flattened != 1]\n",
    "x_flattened = x_flattened[x_flattened != 2]\n",
    "\n",
    "pre_y_flattened = pre_y_flattened[pre_y_flattened != 0]\n",
    "pre_y_flattened = pre_y_flattened[pre_y_flattened != 1]\n",
    "pre_y_flattened = pre_y_flattened[pre_y_flattened != 2]\n",
    "\n",
    "post_y_flattened = post_y_flattened[post_y_flattened != 0]\n",
    "post_y_flattened = post_y_flattened[post_y_flattened != 1]\n",
    "post_y_flattened = post_y_flattened[post_y_flattened != 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1293d03c-68bd-496d-b9fb-91ae84688fe0",
   "metadata": {},
   "source": [
    "--- \n",
    "# Batch Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e23e34f9-3066-4756-ad0b-ccc1a3194438",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "rng = np.random.default_rng(seed)\n",
    "batch_size = 4\n",
    "block_size = 5\n",
    "seq_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e768970c-7af4-4626-9d6e-d712ab7de959",
   "metadata": {},
   "outputs": [],
   "source": [
    "if block_size > seq_len:\n",
    "    raise ValueError(\"Block size should not be bigger than sequence length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c882a5-c136-4b42-9bdc-8f5428103411",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0807ca2-449e-452b-9f66-938828fa64d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = seq_len + 2 #Add start/stop tokens\n",
    "vocab_size = len(i_to_c_pandp)\n",
    "num_chars_data = x_flattened.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dce0777e-c8af-4835-833d-f1717dbf2501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "57\n",
      "4548\n"
     ]
    }
   ],
   "source": [
    "print(maxlen)\n",
    "print(vocab_size)\n",
    "print(num_chars_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56decdad-3c1d-496f-8f04-d783de100a1c",
   "metadata": {},
   "source": [
    "---\n",
    "# Generate Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc198d86-abfe-4a39-86fb-d8722200c228",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "def Partial_Batch(dataset, pre_y, post_y, num_chars, seq_len, rng):\n",
    "    rints = rng.integers(low=0, high=num_chars-seq_len, size=1)[0]\n",
    "    \n",
    "    end_x = rints + seq_len\n",
    "    end_y = end_x + seq_len\n",
    "    \n",
    "    x = dataset[rints:end_x]\n",
    "    x = np.insert(x, 0, 1)\n",
    "    x = np.insert(x, x.shape[0], 2)\n",
    "    \n",
    "    pre_y = dataset[end_x:end_y]\n",
    "    pre_y = np.insert(pre_y, 0, 1)\n",
    "    pre_y[-1] = 2\n",
    "    \n",
    "    post_y = dataset[end_x:end_y]\n",
    "    post_y = np.insert(post_y, post_y.shape[0], 2)\n",
    "    \n",
    "    batch = [x, pre_y, post_y]    \n",
    "    \n",
    "    batch = tf.keras.preprocessing.sequence.pad_sequences(batch, maxlen=maxlen, padding='post', value=0)\n",
    "\n",
    "    padding = maxlen + (block_size-(maxlen%block_size))\n",
    "                        \n",
    "    if (batch.shape[1] % block_size) != 0:\n",
    "        batch = tf.keras.preprocessing.sequence.pad_sequences(batch, maxlen=padding, padding='post', value=0)\n",
    "   \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ccc910a-fc5a-47c0-bf90-83587350fb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Full_Batch(batch_size, x, pre_y, post_y, num_chars, seq_len, rng):\n",
    "    x0 = []\n",
    "    y1 = []\n",
    "    y2 = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        a, b, c, = Partial_Batch(x, pre_y, post_y, num_chars, seq_len, rng)\n",
    "    \n",
    "        x0.append(a)\n",
    "        y1.append(b)\n",
    "        y2.append(c) \n",
    "    \n",
    "    return np.asarray(x0), np.asarray(y1), np.asarray(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33326847-2235-4ec9-ae39-a808a34050c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = Full_Batch(batch_size, x_flattened, pre_y_flattened, post_y_flattened, num_chars_data, seq_len, rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5da4c075-daba-48b5-83c8-f112be57cd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len_padded = a.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e672517f-e46d-47a9-9ef9-6eff18be2647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "print(seq_len_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45d5d65-b728-42de-b529-85643adef96d",
   "metadata": {},
   "source": [
    "---\n",
    "# Masked Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a199f10-268d-4153-863c-2d430a4d2c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedTransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(MaskedTransformerBlock, self).__init__()\n",
    "        self.att1 = keras.layers.MultiHeadAttention(num_heads=num_heads,\n",
    "        key_dim=embed_dim)\n",
    "        self.att2 = keras.layers.MultiHeadAttention(num_heads=num_heads,\n",
    "        key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "        [keras.layers.Dense(ff_dim, activation=\"gelu\"),\n",
    "        keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "        self.dropout3 = keras.layers.Dropout(rate)\n",
    "    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n",
    "        i = tf.range(n_dest)[:, None]\n",
    "        j = tf.range(n_src)\n",
    "        m = i >= j - n_src + n_dest\n",
    "        mask = tf.cast(m, dtype)\n",
    "        mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "        mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "    def call(self, inputs, training):\n",
    "        input_shape = tf.shape(inputs[0])\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        mask = self.causal_attention_mask(batch_size,\n",
    "        seq_len, seq_len,\n",
    "        tf.bool)\n",
    "        attn_output1 = self.att1(inputs[0], inputs[0],\n",
    "        attention_mask = mask)\n",
    "        attn_output1 = self.dropout1(attn_output1, training=training)\n",
    "        out1 = self.layernorm1(inputs[0] + attn_output1)\n",
    "        attn_output2 = self.att2(out1, inputs[1])\n",
    "        attn_output2 = self.dropout2(attn_output2, training=training)\n",
    "        out2 = self.layernorm1(out1 + attn_output2)\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        return self.layernorm2(out2 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befee659-3a43-4372-ae56-860a836497e1",
   "metadata": {},
   "source": [
    "---\n",
    "# Masked Token and Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a27a7c63-8d17-4e49-bfd7-4e05d92eb915",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedTokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim, **kwargs):\n",
    "        super(MaskedTokenAndPositionEmbedding, self).__init__(**kwargs)\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size,\n",
    "        output_dim=embed_dim,\n",
    "        mask_zero=True)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen+1,\n",
    "        output_dim=embed_dim,\n",
    "        mask_zero=True)\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=1, limit=maxlen+1, delta=1)\n",
    "        positions = positions * tf.cast(tf.sign(x),tf.int32)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348583b3-a8cf-4d03-bc57-cbd8c106b890",
   "metadata": {},
   "source": [
    "---\n",
    "# Custom Loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "160d05e4-b851-4348-85df-508526fb154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "def MaskedSparseCategoricalCrossentropy(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "def MaskedSparseCategoricalAccuracy(real, pred):\n",
    "    accuracies = tf.equal(tf.cast(real,tf.int64), tf.argmax(pred, axis=2))\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b66c98-f8b5-4b80-9736-66a9769cc966",
   "metadata": {},
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9af4ac04-1eab-4d74-bfc0-716408efeed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_initializer(initializer):\n",
    "    if isinstance(initializer, tf.keras.initializers.Initializer):\n",
    "        return initializer.__class__.from_config(initializer.get_config())\n",
    "    return initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aadfeaaa-08b0-4155-8381-328ac71f4c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape_list(tensor, expected_rank=None, name=None):\n",
    "    if expected_rank is not None:\n",
    "        assert_rank(tensor, expected_rank, name)\n",
    "\n",
    "    shape = tensor.shape.as_list()\n",
    "\n",
    "    non_static_indexes = []\n",
    "    for (index, dim) in enumerate(shape):\n",
    "        if dim is None:\n",
    "            non_static_indexes.append(index)\n",
    "\n",
    "    if not non_static_indexes:\n",
    "        return shape\n",
    "\n",
    "    dyn_shape = tf.shape(tensor)\n",
    "    for index in non_static_indexes:\n",
    "        shape[index] = dyn_shape[index]\n",
    "    return shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835c39dd-9fb5-49bf-8763-14ca694a1a35",
   "metadata": {},
   "source": [
    "---\n",
    "# Cache Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da4cb5e4-2d26-4e40-a00f-3e71f470b273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_state: `Tensor`, the current state.\n",
    "# previous_state: `Tensor`, the previous state.\n",
    "# memory_length: `int`, the number of tokens to cache.\n",
    "# reuse_length: `int`, the number of tokens in the current batch to be cached\n",
    "#     and reused in the future.\n",
    "# Returns:  `Tensor`, representing the cached state with stopped gradients.\n",
    "def _cache_memory(current_state, previous_state, memory_length, reuse_length=0):\n",
    "    if memory_length is None or memory_length == 0:\n",
    "        return None\n",
    "    else:\n",
    "        if reuse_length > 0:\n",
    "            current_state = current_state[:, :reuse_length, :]\n",
    "\n",
    "        if previous_state is None:\n",
    "            new_mem = current_state[:, -memory_length:, :]\n",
    "        else:\n",
    "            new_mem = tf.concat(\n",
    "                    [previous_state, current_state], 1)[:, -memory_length:, :]\n",
    "\n",
    "    return tf.stop_gradient(new_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3126b9-a96e-4d6a-ae65-3c9579514cf1",
   "metadata": {},
   "source": [
    "---\n",
    "# MultiHead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "5b32faad-fd06-4448-b817-b3a1f5943aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query: Query `Tensor` of shape `[B, T, dim]`.\n",
    "# value: Value `Tensor` of shape `[B, S, dim]`.\n",
    "# content_attention_bias: Bias `Tensor` for content based attention of shape\n",
    "# `[num_heads, dim]`.\n",
    "# positional_attention_bias: Bias `Tensor` for position based attention of\n",
    "# shape `[num_heads, dim]`.\n",
    "# key: Optional key `Tensor` of shape `[B, S, dim]`. If not given, will use\n",
    "# `value` for both `key` and `value`, which is the most common case.\n",
    "# relative_position_encoding: Relative positional encoding `Tensor` of shape\n",
    "# `[B, L, dim]`.\n",
    "# state: Optional `Tensor` of shape `[B, M, E]` where M is the length of the\n",
    "# state or memory. If passed, this is also attended over as in Transformer\n",
    "# XL.\n",
    "# attention_mask: A boolean mask of shape `[B, T, S]` that prevents attention\n",
    "# to certain positions.\n",
    "class MultiHeadRelativeAttention(tf.keras.layers.MultiHeadAttention):\n",
    "    def __init__(self,\n",
    "                 kernel_initializer=\"variance_scaling\",\n",
    "                 **kwargs):\n",
    "        super().__init__(kernel_initializer=kernel_initializer,\n",
    "                                         **kwargs)\n",
    "\n",
    "    def _build_from_signature(self, query, value, key=None):\n",
    "        super(MultiHeadRelativeAttention, self)._build_from_signature(\n",
    "                query=query,\n",
    "                value=value,\n",
    "                key=key)\n",
    "        if hasattr(value, \"shape\"):\n",
    "            value_shape = tf.TensorShape(value.shape)\n",
    "        else:\n",
    "            value_shape = value\n",
    "        if key is None:\n",
    "            key_shape = value_shape\n",
    "        elif hasattr(key, \"shape\"):\n",
    "            key_shape = tf.TensorShape(key.shape)\n",
    "        else:\n",
    "            key_shape = key\n",
    "\n",
    "        common_kwargs = dict(\n",
    "                kernel_initializer=self._kernel_initializer,\n",
    "                bias_initializer=self._bias_initializer,\n",
    "                kernel_regularizer=self._kernel_regularizer,\n",
    "                bias_regularizer=self._bias_regularizer,\n",
    "                activity_regularizer=self._activity_regularizer,\n",
    "                kernel_constraint=self._kernel_constraint,\n",
    "                bias_constraint=self._bias_constraint)\n",
    "\n",
    "        with tf.init_scope():\n",
    "            einsum_equation, _, output_rank = _build_proj_equation(\n",
    "                    key_shape.rank - 1, bound_dims=1, output_dims=2)\n",
    "            self._encoding_dense = tf.keras.layers.experimental.EinsumDense(\n",
    "                    einsum_equation,\n",
    "                    output_shape=_get_output_shape(\n",
    "                        output_rank - 1,\n",
    "                        [self._num_heads, self._key_dim]),\n",
    "                        bias_axes=None,\n",
    "                        name=\"encoding\",\n",
    "                        **common_kwargs)\n",
    "\n",
    "# query: Projected query `Tensor` of shape `[B, T, N, key_dim]`.\n",
    "# key: Projected key `Tensor` of shape `[B, S + M, N, key_dim]`.\n",
    "# value: Projected value `Tensor` of shape `[B, S + M, N, key_dim]`.\n",
    "# position: Projected position `Tensor` of shape `[B, L, N, key_dim]`.\n",
    "# content_attention_bias: Trainable bias parameter added to the query head\n",
    "#     when calculating the content-based attention score.\n",
    "# positional_attention_bias: Trainable bias parameter added to the query\n",
    "#     head when calculating the position-based attention score.\n",
    "# attention_mask: (default None) Optional mask that is added to attention\n",
    "#     logits. If state is not None, the mask source sequence dimension should\n",
    "#     extend M.\n",
    "# Returns:\n",
    "# attention_output: Multi-headed output of attention computation of shape\n",
    "#     `[B, S, N, key_dim]`.\n",
    "    def compute_attention(\n",
    "        self,\n",
    "        query,\n",
    "        key,\n",
    "        value,\n",
    "        position,\n",
    "        content_attention_bias,\n",
    "        positional_attention_bias,\n",
    "        attention_mask=None\n",
    "    ):\n",
    "        \n",
    "        tf.print(query.shape)\n",
    "        tf.print(key.shape)\n",
    "        tf.print(value.shape)\n",
    "        #AC\n",
    "        content_attention = tf.einsum(self._dot_product_equation, key, query + content_attention_bias)\n",
    "        \n",
    "        positional_attention = tf.einsum(self._dot_product_equation, position, query + positional_attention_bias)\n",
    "        \n",
    "        #BD\n",
    "        positional_attention = _rel_shift(positional_attention, klen=tf.shape(content_attention)[3])\n",
    "        \n",
    "        tf.print(content_attention.shape, positional_attention.shape)\n",
    "        \n",
    "        attention_sum = content_attention + positional_attention\n",
    "\n",
    "        attention_scores = tf.multiply(attention_sum, 1.0 / math.sqrt(float(self._key_dim)))\n",
    "\n",
    "        attention_scores = self._masked_softmax(attention_scores, attention_mask)\n",
    "\n",
    "        attention_output = self._dropout_layer(attention_scores)\n",
    "\n",
    "        attention_output = tf.einsum(self._combine_equation,\n",
    "                                                                 attention_output,\n",
    "                                                                 value)\n",
    "        return attention_output\n",
    "\n",
    "# * Number of heads (H): the number of attention heads.\n",
    "# * Value size (V): the size of each value embedding per head.\n",
    "# * Key size (K): the size of each key embedding per head. Equally, the size\n",
    "#     of each query embedding per head. Typically K <= V.\n",
    "# * Batch dimensions (B).\n",
    "# * Query (target) attention axes shape (T).\n",
    "# * Value (source) attention axes shape (S), the rank must match the target.\n",
    "# * Encoding length (L): The relative positional encoding length.\n",
    "# query: attention input.\n",
    "# value: attention input.\n",
    "# content_attention_bias: A trainable bias parameter added to the query head\n",
    "#     when calculating the content-based attention score.\n",
    "# positional_attention_bias: A trainable bias parameter added to the query\n",
    "#     head when calculating the position-based attention score.\n",
    "# key: attention input.\n",
    "# relative_position_encoding: relative positional encoding for key and\n",
    "#     value.\n",
    "# state: (default None) optional state. If passed, this is also attended\n",
    "#     over as in TransformerXL.\n",
    "# attention_mask: (default None) Optional mask that is added to attention\n",
    "#     logits. If state is not None, the mask source sequence dimension should\n",
    "#     extend M.\n",
    "# Returns:\n",
    "# attention_output: The result of the computation, of shape [B, T, E],\n",
    "#     where `T` is for target sequence shapes and `E` is the query input last\n",
    "#     dimension if `output_shape` is `None`. Otherwise, the multi-head outputs\n",
    "#     are projected to the shape specified by `output_shape`.\n",
    "    def call(self,\n",
    "             query,\n",
    "             value,\n",
    "             content_attention_bias,\n",
    "             positional_attention_bias,\n",
    "             key=None,\n",
    "             relative_position_encoding=None,\n",
    "             state=None,\n",
    "             attention_mask=None):\n",
    "        \n",
    "        if not self._built_from_signature:\n",
    "            self._build_from_signature(query, value, key=key)\n",
    "        if key is None:\n",
    "            key = value\n",
    "        if state is not None and state.shape.ndims > 1:\n",
    "            value = tf.concat([state, value], 1)\n",
    "            key = tf.concat([state, key], 1)\n",
    "\n",
    "        # `query` = [B, T, N ,H]\n",
    "        query = self._query_dense(query)\n",
    "\n",
    "        # `key` = [B, S + M, N, H]\n",
    "        key = self._key_dense(key)\n",
    "\n",
    "        # `value` = [B, S + M, N, H]\n",
    "        value = self._value_dense(value)\n",
    "\n",
    "        # `position` = [B, L, N, H]\n",
    "        position = self._encoding_dense(relative_position_encoding)\n",
    "\n",
    "        attention_output = self.compute_attention(\n",
    "                query=query,\n",
    "                key=key,\n",
    "                value=value,\n",
    "                position=position,\n",
    "                content_attention_bias=content_attention_bias,\n",
    "                positional_attention_bias=positional_attention_bias,\n",
    "                attention_mask=attention_mask)\n",
    "\n",
    "        # `attention_output` = [B, S, N, H]\n",
    "        attention_output = self._output_dense(attention_output)\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95abdafa-2c98-4c04-b9e8-aae23e53265e",
   "metadata": {},
   "source": [
    "---\n",
    "# Relative Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "cc8c99a8-3755-48f9-9704-c1dbd04e0ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rel_shift(x, klen=-1):    \n",
    "    x = tf.transpose(x, perm=[2, 3, 0, 1])\n",
    "    x = tf.pad(x, [[0, 0], [1, 0], [0, 0], [0, 0]])\n",
    "    x_size = tf.shape(x)\n",
    "    x = tf.reshape(x, [x_size[1], x_size[0], x_size[2], x_size[3]])\n",
    "    x = tf.slice(x, [1, 0, 0, 0], [-1, -1, -1, -1])\n",
    "    x = tf.reshape(x, [x_size[0], x_size[1] - 1, x_size[2], x_size[3]])\n",
    "    x = tf.transpose(x, perm=[2, 3, 0, 1])\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a026fd-2cbe-45b7-88f7-a3ffd3548806",
   "metadata": {},
   "source": [
    "---\n",
    "# Build Einsum Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "5102422c-82af-49d4-a8d5-5081262b21e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_CHR_IDX = string.ascii_lowercase\n",
    "\n",
    "# Builds an einsum equation for projections inside multi-head attention\n",
    "def _build_proj_equation(free_dims, bound_dims, output_dims):\n",
    "    input_str = \"\"\n",
    "    kernel_str = \"\"\n",
    "    output_str = \"\"\n",
    "    bias_axes = \"\"\n",
    "    letter_offset = 0\n",
    "    for i in range(free_dims):\n",
    "        char = _CHR_IDX[i + letter_offset]\n",
    "        input_str += char\n",
    "        output_str += char\n",
    "\n",
    "    letter_offset += free_dims\n",
    "    for i in range(bound_dims):\n",
    "        char = _CHR_IDX[i + letter_offset]\n",
    "        input_str += char\n",
    "        kernel_str += char\n",
    "\n",
    "    letter_offset += bound_dims\n",
    "    for i in range(output_dims):\n",
    "        char = _CHR_IDX[i + letter_offset]\n",
    "        kernel_str += char\n",
    "        output_str += char\n",
    "        bias_axes += char\n",
    "    equation = \"%s,%s->%s\" % (input_str, kernel_str, output_str)\n",
    "\n",
    "    return equation, bias_axes, len(output_str)\n",
    "\n",
    "\n",
    "def _get_output_shape(output_rank, known_last_dims):\n",
    "    return [None] * (output_rank - len(known_last_dims)) + list(known_last_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5343f3e3-8536-46d5-9d1b-28f8c3f7908b",
   "metadata": {},
   "source": [
    "---\n",
    "# Relative Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "a5f5eeac-4c07-4796-8281-f5c039fe7c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_size: Size of the hidden layer.\n",
    "# min_timescale: Minimum scale that will be applied at each position\n",
    "# max_timescale: Maximum scale that will be applied at each position.\n",
    "class RelativePositionEmbedding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 hidden_size: int,\n",
    "                 min_timescale: float = 1.0,\n",
    "                 max_timescale: float = 1.0e4,\n",
    "                 **kwargs):\n",
    "        \n",
    "        if \"dtype\" not in kwargs:\n",
    "            kwargs[\"dtype\"] = \"float32\"\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self._hidden_size = hidden_size\n",
    "        self._min_timescale = min_timescale\n",
    "        self._max_timescale = max_timescale\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "                \"hidden_size\": self._hidden_size,\n",
    "                \"min_timescale\": self._min_timescale,\n",
    "                \"max_timescale\": self._max_timescale,\n",
    "        }\n",
    "        base_config = super(RelativePositionEmbedding, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "# inputs: An tensor whose second dimension will be used as `length`. If\n",
    "# `None`, the other `length` argument must be specified.\n",
    "# length: An optional integer specifying the number of positions. If both\n",
    "# `inputs` and `length` are spcified, `length` must be equal to the second\n",
    "# dimension of `inputs`.\n",
    "# Returns:\n",
    "# A tensor in shape of `(length, hidden_size)`.\n",
    "    def call(self, inputs, length=None):\n",
    "        if inputs is None and length is None:\n",
    "            raise ValueError(\"If inputs is None, `length` must be set in \"\n",
    "                                             \"RelativePositionEmbedding().\")\n",
    "        if inputs is not None:\n",
    "            input_shape = get_shape_list(inputs)\n",
    "            if length is not None and length != input_shape[1]:\n",
    "                raise ValueError(\n",
    "                        \"If inputs is not None, `length` must equal to input_shape[1].\")\n",
    "            length = input_shape[1]\n",
    "        position = tf.cast(tf.range(length), tf.float32)\n",
    "        num_timescales = self._hidden_size // 2\n",
    "        min_timescale, max_timescale = self._min_timescale, self._max_timescale\n",
    "        log_timescale_increment = (\n",
    "                math.log(float(max_timescale) / float(min_timescale)) /\n",
    "                (tf.cast(num_timescales, tf.float32) - 1))\n",
    "        inv_timescales = min_timescale * tf.exp(\n",
    "                tf.cast(tf.range(num_timescales), tf.float32) *\n",
    "                -log_timescale_increment)\n",
    "        scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(\n",
    "                inv_timescales, 0)\n",
    "        position_embeddings = tf.concat(\n",
    "                [tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n",
    "        position_embeddings = tf.expand_dims(position_embeddings, axis=0)\n",
    "        return tf.tile(position_embeddings, (tf.shape(inputs)[0], 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c32bb5-4520-4802-a8f0-5f7849905a83",
   "metadata": {},
   "source": [
    "---\n",
    "# XL Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "7245b7dd-a3b2-464a-9f5b-7efd4a9d0ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size: The size of the token vocabulary.\n",
    "# hidden_size: The size of the transformer hidden layers.\n",
    "# num_attention_heads: The number of attention heads.\n",
    "# head_size: The dimension size of each attention head.\n",
    "# inner_size: The inner size for the transformer layers.\n",
    "# dropout_rate: Dropout rate for the output of this layer.\n",
    "# attention_dropout_rate: Dropout rate on attention probabilities.\n",
    "# norm_epsilon: Epsilon value to initialize normalization layers.\n",
    "# inner_activation: The activation to use for the inner\n",
    "#     FFN layers.\n",
    "# kernel_initializer: Initializer for dense layer kernels.\n",
    "# inner_dropout: Dropout probability for the inner dropout\n",
    "#     layer.\n",
    "class TransformerXLBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 hidden_size,\n",
    "                 num_attention_heads,\n",
    "                 head_size,\n",
    "                 inner_size,\n",
    "                 dropout_rate,\n",
    "                 attention_dropout_rate,\n",
    "                 norm_epsilon=1e-12,\n",
    "                 inner_activation=\"relu\",\n",
    "                 kernel_initializer=\"variance_scaling\",\n",
    "                 inner_dropout=0.0,\n",
    "                 **kwargs):\n",
    "        \"\"\"Initializes TransformerXLBlock layer.\"\"\"\n",
    "\n",
    "        super(TransformerXLBlock, self).__init__(**kwargs)\n",
    "        self._vocab_size = vocab_size\n",
    "        self._num_heads = num_attention_heads\n",
    "        self._head_size = head_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._inner_size = inner_size\n",
    "        self._dropout_rate = dropout_rate\n",
    "        self._attention_dropout_rate = attention_dropout_rate\n",
    "        self._inner_activation = inner_activation\n",
    "        self._norm_epsilon = norm_epsilon\n",
    "        self._kernel_initializer = kernel_initializer\n",
    "        self._inner_dropout = inner_dropout\n",
    "        self._attention_layer_type = MultiHeadRelativeAttention\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_tensor = input_shape[0] if len(input_shape) == 2 else input_shape\n",
    "        input_tensor_shape = tf.TensorShape(input_tensor)\n",
    "        if len(input_tensor_shape.as_list()) != 3:\n",
    "            raise ValueError(\"TransformerLayer expects a three-dimensional input of \"\n",
    "                                             \"shape [batch, sequence, width].\")\n",
    "        batch_size, sequence_length, hidden_size = input_tensor_shape\n",
    "\n",
    "        if len(input_shape) == 2:\n",
    "            mask_tensor_shape = tf.TensorShape(input_shape[1])\n",
    "            expected_mask_tensor_shape = tf.TensorShape(\n",
    "                    [batch_size, sequence_length, sequence_length])\n",
    "            if not expected_mask_tensor_shape.is_compatible_with(mask_tensor_shape):\n",
    "                raise ValueError(\"When passing a mask tensor to TransformerXLBlock, \"\n",
    "                                                 \"the mask tensor must be of shape [batch, \"\n",
    "                                                 \"sequence_length, sequence_length] (here %s). Got a \"\n",
    "                                                 \"mask tensor of shape %s.\" %\n",
    "                                                 (expected_mask_tensor_shape, mask_tensor_shape))\n",
    "        if hidden_size % self._num_heads != 0:\n",
    "            raise ValueError(\n",
    "                    \"The input size (%d) is not a multiple of the number of attention \"\n",
    "                    \"heads (%d)\" % (hidden_size, self._num_heads))\n",
    "        self._attention_layer = self._attention_layer_type(\n",
    "                num_heads=self._num_heads,\n",
    "                key_dim=self._head_size,\n",
    "                value_dim=self._head_size,\n",
    "                dropout=self._attention_dropout_rate,\n",
    "                use_bias=False,\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer),\n",
    "                name=\"rel_attn\")\n",
    "        self._attention_dropout = tf.keras.layers.Dropout(\n",
    "                rate=self._attention_dropout_rate)\n",
    "        self._attention_layer_norm = tf.keras.layers.LayerNormalization(\n",
    "                name=\"self_attention_layer_norm\",\n",
    "                axis=-1,\n",
    "                epsilon=self._norm_epsilon,\n",
    "                dtype=tf.float32)\n",
    "        self._inner_dense = tf.keras.layers.experimental.EinsumDense(\n",
    "                \"abc,cd->abd\",\n",
    "                output_shape=(None, self._inner_size),\n",
    "                bias_axes=\"d\",\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer),\n",
    "                name=\"inner\")\n",
    "\n",
    "        self._inner_activation_layer = tf.keras.layers.Activation(\n",
    "                self._inner_activation)\n",
    "        self._inner_dropout_layer = tf.keras.layers.Dropout(\n",
    "                rate=self._inner_dropout)\n",
    "        self._output_dense = tf.keras.layers.experimental.EinsumDense(\n",
    "                \"abc,cd->abd\",\n",
    "                output_shape=(None, hidden_size),\n",
    "                bias_axes=\"d\",\n",
    "                name=\"output\",\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer))\n",
    "        self._output_dropout = tf.keras.layers.Dropout(rate=self._dropout_rate)\n",
    "        self._output_layer_norm = tf.keras.layers.LayerNormalization(\n",
    "                name=\"output_layer_norm\",\n",
    "                axis=-1,\n",
    "                epsilon=self._norm_epsilon)\n",
    "\n",
    "        super(TransformerXLBlock, self).build(input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "                \"vocab_size\":\n",
    "                        self._vocab_size,\n",
    "                \"hidden_size\":\n",
    "                        self._hidden_size,\n",
    "                \"num_attention_heads\":\n",
    "                        self._num_heads,\n",
    "                \"head_size\":\n",
    "                        self._head_size,\n",
    "                \"inner_size\":\n",
    "                        self._inner_size,\n",
    "                \"dropout_rate\":\n",
    "                        self._dropout_rate,\n",
    "                \"attention_dropout_rate\":\n",
    "                        self._attention_dropout_rate,\n",
    "                \"norm_epsilon\":\n",
    "                        self._norm_epsilon,\n",
    "                \"inner_activation\":\n",
    "                        self._inner_activation,\n",
    "                \"kernel_initializer\":\n",
    "                        self._kernel_initializer,\n",
    "                \"inner_dropout\":\n",
    "                        self._inner_dropout,\n",
    "        }\n",
    "        base_config = super(TransformerXLBlock, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "# content_stream: `Tensor`, the input content stream. This is the standard\n",
    "# input to Transformer XL and is commonly referred to as `h` in XLNet.\n",
    "# content_attention_bias: Bias `Tensor` for content based attention of shape\n",
    "# `[num_heads, dim]`.\n",
    "# positional_attention_bias: Bias `Tensor` for position based attention of\n",
    "# shape `[num_heads, dim]`.\n",
    "# relative_position_encoding: Relative positional encoding `Tensor` of shape\n",
    "# `[B, L, dim]`.\n",
    "# state: Optional `Tensor` of shape `[B, M, E]`, where M is the length of\n",
    "# the state or memory. If passed, this is also attended over as in\n",
    "# Transformer XL.\n",
    "# content_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to content attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# query_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to query attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# target_mapping: Optional `Tensor` representing the target mapping when\n",
    "# calculating query attention.\n",
    "# Returns:\n",
    "# A `dict` object, containing the key value pairs for `content_attention`\n",
    "    def call(self,\n",
    "             content_stream,\n",
    "             content_attention_bias,\n",
    "             positional_attention_bias,\n",
    "             relative_position_encoding=None,\n",
    "             state=None,\n",
    "             content_attention_mask=None,\n",
    "             query_attention_mask=None,\n",
    "             target_mapping=None):\n",
    "        \n",
    "        attention_kwargs = dict(\n",
    "                query=content_stream,\n",
    "                value=content_stream,\n",
    "                key=content_stream,\n",
    "                attention_mask=content_attention_mask)\n",
    "\n",
    "        common_attention_kwargs = dict(\n",
    "                content_attention_bias=content_attention_bias,\n",
    "                relative_position_encoding=relative_position_encoding,\n",
    "                positional_attention_bias=positional_attention_bias,\n",
    "                state=state)\n",
    "\n",
    "        attention_kwargs.update(common_attention_kwargs)\n",
    "        attention_output = self._attention_layer(**attention_kwargs)\n",
    "\n",
    "        attention_stream = attention_output\n",
    "        input_stream = content_stream\n",
    "        attention_key = \"content_attention\"\n",
    "        attention_output = {}\n",
    "        \n",
    "        attention_stream = self._attention_dropout(attention_stream)\n",
    "        attention_stream = self._attention_layer_norm(attention_stream + input_stream)\n",
    "        inner_output = self._inner_dense(attention_stream)\n",
    "        inner_output = self._inner_activation_layer(\n",
    "                inner_output)\n",
    "        inner_output = self._inner_dropout_layer(\n",
    "                inner_output)\n",
    "        layer_output = self._output_dense(inner_output)\n",
    "        layer_output = self._output_dropout(layer_output)\n",
    "        layer_output = self._output_layer_norm(layer_output + attention_stream)\n",
    "        attention_output[attention_key] = layer_output\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8749ec30-1281-4f9d-8baa-54b890fb7949",
   "metadata": {},
   "source": [
    "---\n",
    "# Transformer XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "019ca25d-e899-4e17-8f41-dfdc3d9bdee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_layers: The number of layers.\n",
    "# hidden_size: The hidden size.\n",
    "# num_attention_heads: The number of attention heads.\n",
    "# head_size: The dimension size of each attention head.\n",
    "# inner_size: The hidden size in feed-forward layers.\n",
    "# dropout_rate: Dropout rate used in each Transformer XL block.\n",
    "# attention_dropout_rate: Dropout rate on attention probabilities.\n",
    "# initializer: The initializer to use for attention biases.\n",
    "# tie_attention_biases: Whether or not to tie biases together. If `True`, then\n",
    "# each Transformer XL block shares the same trainable attention bias. If\n",
    "# `False`, then each block has its own attention bias. This is usually set\n",
    "# to `True`.\n",
    "# memory_length: The number of tokens to cache.\n",
    "# reuse_length: The number of tokens in the current batch to be cached\n",
    "# and reused in the future.\n",
    "# inner_activation: The activation to use in the inner layers\n",
    "# for Transformer XL blocks. Typically \"relu\" or \"gelu\".\n",
    "class TransformerXL(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 num_layers,\n",
    "                 hidden_size,\n",
    "                 maxlen,\n",
    "                 embed_dim,\n",
    "                 num_attention_heads,\n",
    "                 head_size,\n",
    "                 inner_size,\n",
    "                 dropout_rate,\n",
    "                 attention_dropout_rate,\n",
    "                 initializer,\n",
    "                 tie_attention_biases=True,\n",
    "                 memory_length=None,\n",
    "                 reuse_length=None,\n",
    "                 inner_activation=\"relu\",\n",
    "                 **kwargs):\n",
    "        super(TransformerXL, self).__init__(**kwargs)\n",
    "\n",
    "        self._vocab_size = vocab_size\n",
    "        self._initializer = initializer\n",
    "        self._num_layers = num_layers\n",
    "        self._hidden_size = hidden_size\n",
    "        self._num_attention_heads = num_attention_heads\n",
    "        self._head_size = head_size\n",
    "        self._inner_size = inner_size\n",
    "        self._inner_activation = inner_activation\n",
    "        self._dropout_rate = dropout_rate\n",
    "        self._attention_dropout_rate = attention_dropout_rate\n",
    "        self._tie_attention_biases = tie_attention_biases\n",
    "\n",
    "        self._memory_length = memory_length\n",
    "        self._reuse_length = reuse_length\n",
    "\n",
    "        if self._tie_attention_biases:\n",
    "            attention_bias_shape = [self._num_attention_heads, self._head_size]\n",
    "        else:\n",
    "            attention_bias_shape = [self._num_layers, self._num_attention_heads, self._head_size]\n",
    "\n",
    "        self.content_attention_bias = self.add_weight(\n",
    "                \"content_attention_bias\",\n",
    "                shape=attention_bias_shape,\n",
    "                dtype=tf.float32,\n",
    "                initializer=clone_initializer(self._initializer))\n",
    "        self.positional_attention_bias = self.add_weight(\n",
    "                \"positional_attention_bias\",\n",
    "                shape=attention_bias_shape,\n",
    "                dtype=tf.float32,\n",
    "                initializer=clone_initializer(self._initializer))\n",
    "\n",
    "        self.transformer_xl_layers = []\n",
    "        for i in range(self._num_layers):\n",
    "            self.transformer_xl_layers.append(\n",
    "                    TransformerXLBlock(\n",
    "                            vocab_size=self._vocab_size,\n",
    "                            hidden_size=self._head_size * self._num_attention_heads,\n",
    "                            num_attention_heads=self._num_attention_heads,\n",
    "                            head_size=self._head_size,\n",
    "                            inner_size=self._inner_size,\n",
    "                            dropout_rate=self._dropout_rate,\n",
    "                            attention_dropout_rate=self._attention_dropout_rate,\n",
    "                            norm_epsilon=1e-12,\n",
    "                            inner_activation=self._inner_activation,\n",
    "                            kernel_initializer=\"variance_scaling\",\n",
    "                            name=\"layer_%d\" % i))\n",
    "\n",
    "        self.output_dropout = tf.keras.layers.Dropout(rate=self._dropout_rate)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "                \"vocab_size\":\n",
    "                        self._vocab_size,\n",
    "                \"num_layers\":\n",
    "                        self._num_layers,\n",
    "                \"hidden_size\":\n",
    "                        self._hidden_size,\n",
    "                \"num_attention_heads\":\n",
    "                        self._num_attention_heads,\n",
    "                \"head_size\":\n",
    "                        self._head_size,\n",
    "                \"inner_size\":\n",
    "                        self._inner_size,\n",
    "                \"dropout_rate\":\n",
    "                        self._dropout_rate,\n",
    "                \"attention_dropout_rate\":\n",
    "                        self._attention_dropout_rate,\n",
    "                \"initializer\":\n",
    "                        self._initializer,\n",
    "                \"tie_attention_biases\":\n",
    "                        self._tie_attention_biases,\n",
    "                \"memory_length\":\n",
    "                        self._memory_length,\n",
    "                \"reuse_length\":\n",
    "                        self._reuse_length,\n",
    "                \"inner_activation\":\n",
    "                        self._inner_activation,\n",
    "        }\n",
    "        base_config = super(TransformerXL, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "# content_stream: `Tensor`, the input content stream. This is the standard\n",
    "# input to Transformer XL and is commonly referred to as `h` in XLNet.\n",
    "# relative_position_encoding: Relative positional encoding `Tensor` of shape\n",
    "# `[B, L, dim]`.\n",
    "# state: Optional `Tensor` of shape `[B, M, E]`, where M is the length of\n",
    "# the state or memory. If passed, this is also attended over as in\n",
    "# Transformer XL.\n",
    "# content_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to content attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# query_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to query attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# target_mapping: Optional `Tensor` representing the target mapping when\n",
    "# calculating query attention.\n",
    "# Returns:\n",
    "# A tuple consisting of the attention output and the list of cached memory\n",
    "# states.\n",
    "# The attention output is `content_attention`\n",
    "    def call(self,\n",
    "             content_stream,\n",
    "             relative_position_encoding,\n",
    "             state=None,\n",
    "             content_attention_mask=None,\n",
    "             query_attention_mask=None,\n",
    "             target_mapping=None):\n",
    "        \n",
    "        new_mems = []\n",
    "\n",
    "        if state is None:\n",
    "            state = [None] * self._num_layers\n",
    "        for i in range(self._num_layers):\n",
    "            # cache new mems\n",
    "            new_mems.append(\n",
    "                    _cache_memory(content_stream, state[i],\n",
    "                                                self._memory_length, self._reuse_length))\n",
    "            \n",
    "            tf.print(state[i].shape)\n",
    "            if self._tie_attention_biases:\n",
    "                content_attention_bias = self.content_attention_bias\n",
    "            else:\n",
    "                content_attention_bias = self.content_attention_bias[i]\n",
    "                \n",
    "            if self._tie_attention_biases:\n",
    "                positional_attention_bias = self.positional_attention_bias\n",
    "            else:\n",
    "                positional_attention_bias = self.positional_attention_bias[i]\n",
    "\n",
    "            transformer_xl_layer = self.transformer_xl_layers[i]\n",
    "            \n",
    "            transformer_xl_output = transformer_xl_layer(\n",
    "                    content_stream=content_stream,\n",
    "                    content_attention_bias=content_attention_bias,\n",
    "                    positional_attention_bias=positional_attention_bias,\n",
    "                    relative_position_encoding=relative_position_encoding,\n",
    "                    state=state[i],\n",
    "                    content_attention_mask=content_attention_mask,\n",
    "                    query_attention_mask=query_attention_mask,\n",
    "                    target_mapping=target_mapping)\n",
    "            content_stream = transformer_xl_output[\"content_attention\"]\n",
    "\n",
    "        output_stream = content_stream\n",
    "        return output_stream, new_mems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a127a4b5-15e3-46e3-838b-a5e18e4d8dc8",
   "metadata": {},
   "source": [
    "---\n",
    "# Xl Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "855d43dd-e182-4c2a-961a-ff9781141f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XlModel(keras.Model):\n",
    "    def __init__(self, block_size, seq_len_padded, embed_dim, vocab_size, num_layers, hidden_size, num_attention_heads, maxlen, memory_length, reuse_length, head_size, inner_size, dropout_rate, attention_dropout_rate, initializer):\n",
    "        super(XlModel, self).__init__()\n",
    "        \n",
    "        self.block_size = block_size\n",
    "        self.seq_len_padded = seq_len_padded\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_attention_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.maxlen = maxlen\n",
    "        self.memory_length = memory_length\n",
    "        \n",
    "        self.embedding_layer = MaskedTokenAndPositionEmbedding(maxlen=maxlen, vocab_size=vocab_size, embed_dim=embed_dim)\n",
    "        \n",
    "        self.rel_embeddings = RelativePositionEmbedding(embed_dim)\n",
    "\n",
    "        self.transformer_xl = TransformerXL(\n",
    "                vocab_size=vocab_size,\n",
    "                num_layers=num_layers,\n",
    "                hidden_size=hidden_size,\n",
    "                num_attention_heads=num_attention_heads,\n",
    "                maxlen=maxlen,\n",
    "                embed_dim=embed_dim,\n",
    "                memory_length=memory_length,\n",
    "                reuse_length=reuse_length,\n",
    "                head_size=head_size,\n",
    "                inner_size=inner_size,\n",
    "                dropout_rate=dropout_rate,\n",
    "                attention_dropout_rate=attention_dropout_rate,\n",
    "                initializer=initializer, \n",
    "            )\n",
    "        \n",
    "    \n",
    "    def call(self, x, mems, training=None):        \n",
    "        \n",
    "        embeddings = self.embedding_layer(x)\n",
    "        \n",
    "        \n",
    "        if mems[0] is not None:\n",
    "            rel_embeddings = self.rel_embeddings(tf.concat((embeddings,mems[0]), axis=1))\n",
    "        else:\n",
    "            rel_embeddings = self.rel_embeddings(embeddings)\n",
    "    \n",
    "        for i in range(0, self.seq_len_padded, self.block_size):\n",
    "            block = embeddings[:,i:i+block_size]\n",
    "            rel_block = rel_embeddings[:,i:i+block_size]\n",
    "            encoder, mems = self.transformer_xl(content_stream=block, relative_position_encoding=rel_block, state=mems)\n",
    "        \n",
    "        return encoder, mems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35f479d-e693-4f68-8452-48a88b9869d3",
   "metadata": {},
   "source": [
    "---\n",
    "# Decoder Model Xl Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "fec73818-5f20-49f9-8706-3a78464b364e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderXl(keras.Model):\n",
    "    def __init__(self, block_size, seq_len_padded, seq_len, embed_dim, vocab_size, num_layers, hidden_size, num_attention_heads, maxlen, memory_length, reuse_length, head_size, inner_size, dropout_rate, attention_dropout_rate, initializer):\n",
    "        super(DecoderXl, self).__init__()\n",
    "        \n",
    "        self.block_size = block_size\n",
    "        self.seq_len_padded = seq_len_padded\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_attention_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.maxlen = maxlen\n",
    "        self.memory_length = memory_length\n",
    "        \n",
    "        #Embedding for targets\n",
    "        self.embedding_layer = MaskedTokenAndPositionEmbedding(maxlen=maxlen, vocab_size=vocab_size, embed_dim=embed_dim)\n",
    "        \n",
    "        #Relative embeddings for targets\n",
    "        self.rel_embeddings = RelativePositionEmbedding(embed_dim)\n",
    "\n",
    "        #Transformer xl takes in decoder memories\n",
    "        self.transformer_xl = TransformerXL(\n",
    "                vocab_size=vocab_size,\n",
    "                num_layers=num_layers,\n",
    "                hidden_size=hidden_size,\n",
    "                num_attention_heads=num_attention_heads,\n",
    "                maxlen=maxlen,\n",
    "                embed_dim=embed_dim,\n",
    "                memory_length=memory_length,\n",
    "                reuse_length=reuse_length,\n",
    "                head_size=head_size,\n",
    "                inner_size=inner_size,\n",
    "                dropout_rate=dropout_rate,\n",
    "                attention_dropout_rate=attention_dropout_rate,\n",
    "                initializer=initializer, \n",
    "            )\n",
    "        \n",
    "        #Attention between targets and encoder output\n",
    "        self.x_y_attention_layer = MaskedTransformerBlock(embed_dim=embed_dim, num_heads=num_attention_heads, ff_dim=hidden_size)\n",
    "\n",
    "        self.output_layer = keras.layers.Dense(seq_len)\n",
    "        \n",
    "    \n",
    "    def call(self, encoder, pre_y, post_y, mems, training=None):        \n",
    "        \n",
    "        #Target embeddings\n",
    "        embeddings = self.embedding_layer(pre_y)\n",
    "        \n",
    "        #Position encoding\n",
    "        if mems[0] is not None:\n",
    "            rel_embeddings = self.rel_embeddings(tf.concat((embeddings,mems[0]), axis=1))\n",
    "        else:\n",
    "            rel_embeddings = self.rel_embeddings(embeddings)\n",
    "        \n",
    "        decoder_output = []\n",
    "        \n",
    "        #Loop through target blcoks\n",
    "        for i in range(0, self.seq_len_padded, self.block_size):\n",
    "            block = embeddings[:,i:i+block_size]\n",
    "            rel_block = rel_embeddings[:,i:i+block_size]\n",
    "            \n",
    "            decoder, mems = self.transformer_xl(content_stream=block, relative_position_encoding=rel_block, state=mems)\n",
    "            \n",
    "            self_attention = self.x_y_attention_layer([decoder, encoder])\n",
    "            \n",
    "            if i == 0:\n",
    "                decoder_output = self_attention\n",
    "            else:\n",
    "                decoder_output = tf.concat([decoder_output, self_attention], axis=1)\n",
    "        \n",
    "        output = self.output_layer(decoder_output)    \n",
    "        \n",
    "        return output, mems\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3469de1-250d-4c14-bb79-a4e74f0fe93e",
   "metadata": {},
   "source": [
    "---\n",
    "# Xl Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "2bc3ea7d-d4a1-4a1e-bf2a-7cd1c91430cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xl Parameters \n",
    "embed_dim = 64\n",
    "num_layers = 8\n",
    "hidden_size = 64\n",
    "num_attention_heads = 8\n",
    "memory_length = 20\n",
    "reuse_length = 0\n",
    "head_size = 32\n",
    "inner_size = 32\n",
    "dropout_rate = 0.0\n",
    "attention_dropout_rate = 0.0\n",
    "initializer = keras.initializers.RandomNormal(stddev=0.1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090cf140-6d94-495f-ad77-c6d273acc119",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Decoder Xl Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "dbb3df24-896d-41f4-bc93-3f2f76cfefd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_shape = (block_size, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89649663-7d18-48bc-8678-ce5f093908cc",
   "metadata": {},
   "source": [
    "---\n",
    "# Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "821b0c4a-6d05-4867-a713-474bfdbff881",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():  \n",
    "    encoder = XlModel(block_size, seq_len_padded, embed_dim, vocab_size, num_layers, hidden_size, num_attention_heads, maxlen, memory_length, reuse_length, head_size, inner_size, dropout_rate, attention_dropout_rate, initializer)\n",
    "    encoder.compile(loss = MaskedSparseCategoricalCrossentropy, optimizer = keras.optimizers.Adam())\n",
    "    \n",
    "    decoder =  DecoderXl(block_size, seq_len_padded, seq_len, embed_dim, vocab_size, num_layers, hidden_size, num_attention_heads, maxlen, memory_length, reuse_length, head_size, inner_size, dropout_rate, attention_dropout_rate, initializer)\n",
    "    decoder.compile(loss = MaskedSparseCategoricalCrossentropy, optimizer = keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752fb718-67f6-44ff-aa6d-c889c0254b82",
   "metadata": {},
   "source": [
    "---\n",
    "# Custom Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "3e8a0e13-617c-4836-ac48-6931f1e685b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split encoder segment up inside model\n",
    "#Process one by one\n",
    "#Send last output to decoder\n",
    "#Split target up \n",
    "#Process one by one\n",
    "#Pass in parallel one by one to mha with encoder output\n",
    "#Aggregate\n",
    "#Send gradients back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "e9618dbb-b277-477d-b999-ea916bba5b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, pre_y, post_y = Full_Batch(batch_size, x_flattened, pre_y_flattened, post_y_flattened, num_chars_data, seq_len, rng)\n",
    "encoder_mems = tf.zeros((num_layers, batch_size, memory_length, embed_dim))\n",
    "decoder_mems = tf.zeros((num_layers, batch_size, memory_length, embed_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "faf43b57-f807-4d9e-a951-96930ec912e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorShape([4, 20, 64])\n",
      "TensorShape([4, 5, 8, 32])\n",
      "TensorShape([4, 25, 8, 32])\n",
      "TensorShape([4, 25, 8, 32])\n",
      "TensorShape([4, 8, 5, 25]) TensorShape([4, 8, 5, 5])\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer \"rel_attn\" (type MultiHeadRelativeAttention).\n\nIncompatible shapes: [4,8,5,25] vs. [4,8,5,5] [Op:AddV2]\n\nCall arguments received:\n  • query=tf.Tensor(shape=(4, 5, 64), dtype=float32)\n  • value=tf.Tensor(shape=(4, 5, 64), dtype=float32)\n  • content_attention_bias=<tf.Variable 'content_attention_bias:0' shape=(8, 32) dtype=float32, numpy=\narray([[ 0.03287513,  0.1308676 ,  0.07159242,  0.03937121, -0.17795798,\n        -0.13579169, -0.0221532 , -0.05166721, -0.09662085,  0.15133546,\n        -0.05118976,  0.02578242, -0.04608143,  0.06914274, -0.10207377,\n         0.20638596, -0.0872229 ,  0.15885858, -0.12725139,  0.1462176 ,\n        -0.06859937,  0.09135687, -0.07438351, -0.02813614,  0.07960841,\n        -0.15449849, -0.03970689,  0.20876415,  0.11723583,  0.00644691,\n        -0.0389403 ,  0.04704792],\n       [-0.08524917, -0.2896531 , -0.02580698,  0.01670947,  0.05169223,\n         0.08683338,  0.0491168 , -0.04607744, -0.05214889,  0.02677131,\n        -0.08692616, -0.07404373, -0.14667545,  0.02935993, -0.00817272,\n        -0.02815646,  0.01483426, -0.04266326, -0.10919099,  0.03855965,\n        -0.07222566, -0.17054842,  0.00908754, -0.0622774 ,  0.12986699,\n        -0.03118214,  0.02299939, -0.00548464,  0.04589556, -0.05235977,\n        -0.05871842, -0.08726641],\n       [-0.06108912,  0.0438494 ,  0.04593267,  0.20173314,  0.02447603,\n        -0.08227833,  0.00147641, -0.13687143,  0.2736034 , -0.01947888,\n         0.04949598, -0.16006674, -0.05029876, -0.11988882,  0.16386232,\n         0.02060073, -0.22690168,  0.06145889,  0.02008096, -0.19718926,\n         0.01065039,  0.01022316, -0.26778615,  0.00291108,  0.03542108,\n         0.05421104,  0.03103381, -0.0334777 , -0.06713177,  0.12368794,\n        -0.0106899 ,  0.08349055],\n       [-0.06384157, -0.08431133, -0.00934188,  0.15116042, -0.04571173,\n         0.07931993, -0.04587935, -0.17895998, -0.0548207 , -0.07388108,\n         0.07076109,  0.06776614, -0.10873523, -0.04761241, -0.08560496,\n         0.09679097, -0.030956  , -0.05843683, -0.11706877,  0.14529875,\n        -0.06591242, -0.16234937,  0.04540564,  0.15005367,  0.06710535,\n         0.13785563, -0.0258772 , -0.20246577, -0.06959141,  0.02145141,\n         0.01335473,  0.07557549],\n       [-0.13630204, -0.1717239 ,  0.00492686, -0.18907766,  0.16415209,\n        -0.04626653, -0.03349418,  0.01609157,  0.02821062, -0.10073705,\n        -0.10090559,  0.22355044, -0.04221591,  0.04052363, -0.04928828,\n         0.06290404,  0.15480845,  0.00128631,  0.05774442,  0.0065458 ,\n        -0.08323361, -0.07131942,  0.13009597, -0.15306523, -0.02738934,\n        -0.03204426, -0.10733797,  0.02358152,  0.09766039, -0.0577438 ,\n         0.13668714,  0.03268134],\n       [ 0.06760236,  0.27838883,  0.0417542 , -0.03929676, -0.02233236,\n        -0.04500396, -0.08251137,  0.07393538,  0.03886604, -0.12139211,\n         0.01571097,  0.04667465,  0.08431952,  0.00632558, -0.00446788,\n        -0.19694899, -0.02558201,  0.05203411, -0.10902635, -0.1165527 ,\n         0.12617974, -0.05231718,  0.15782137, -0.16342278, -0.04333888,\n         0.19619061,  0.02044684,  0.01112492, -0.12149236,  0.11609381,\n        -0.02616549,  0.00677783],\n       [ 0.03577612,  0.04805276, -0.06840575, -0.02782409,  0.02700731,\n        -0.03902677, -0.12283748,  0.08405602,  0.11676797,  0.07905778,\n        -0.27158263, -0.06895094,  0.05131955, -0.04788188, -0.11596133,\n        -0.05523408, -0.0336453 ,  0.08770721, -0.03728735,  0.10794931,\n        -0.07981983, -0.11650636, -0.19115786, -0.11263958,  0.09011125,\n         0.26312906,  0.06182327, -0.13847993, -0.06165509, -0.12106355,\n        -0.01166811,  0.00145314],\n       [-0.06837669, -0.22426854, -0.06283196,  0.16505586,  0.12079521,\n        -0.02968594, -0.04697726,  0.07354838, -0.05533506,  0.13174044,\n         0.08842895,  0.19074515,  0.06131871, -0.12680566,  0.10271311,\n        -0.13683051, -0.12032788, -0.09521369, -0.00138116,  0.08564305,\n        -0.02926035, -0.01124342, -0.12745397,  0.05384149,  0.03256594,\n         0.01085354, -0.03050924,  0.09838126, -0.16953214,  0.03227901,\n         0.00547341,  0.07029521]], dtype=float32)>\n  • positional_attention_bias=<tf.Variable 'positional_attention_bias:0' shape=(8, 32) dtype=float32, numpy=\narray([[ 4.96523939e-02, -7.82599598e-02, -6.40058666e-02,\n         3.94228064e-02,  1.64101660e-01,  4.04064963e-03,\n         2.82683503e-02, -2.81508654e-01,  1.64980397e-01,\n        -2.12374598e-01,  4.17430012e-04,  4.64061052e-02,\n         1.93693683e-01,  5.17754853e-02,  1.03411965e-01,\n         2.45158561e-02, -8.06915909e-02, -4.27037254e-02,\n         1.54297771e-02, -1.87940837e-03,  1.75573211e-02,\n        -1.10835984e-01, -5.07753007e-02, -1.87343031e-01,\n        -1.04693763e-01,  3.02336868e-02, -9.48383808e-02,\n         1.44400196e-02, -8.06704238e-02,  1.08647726e-01,\n        -4.45318632e-02,  3.43445013e-03],\n       [-9.68781859e-02,  2.87369620e-02, -2.29990073e-02,\n        -1.56304047e-01,  1.35808038e-02,  5.15457056e-02,\n         1.13785051e-01,  2.99810860e-02, -1.11891523e-01,\n        -2.00788021e-01, -2.92625334e-02,  9.97205302e-02,\n        -2.15612408e-02, -3.90007906e-02, -6.77806363e-02,\n         1.24029927e-01, -9.05738305e-03, -1.15602769e-01,\n         2.85910457e-01, -9.33460072e-02, -9.83844697e-03,\n        -5.31302392e-03, -4.35913019e-02,  1.52017670e-02,\n         9.22025472e-04, -1.77757204e-01,  1.59892723e-01,\n         8.38689134e-02,  5.11010997e-02, -7.60706291e-02,\n        -1.05901696e-01,  1.52387798e-01],\n       [-3.03401481e-02,  1.63786393e-02,  3.07119396e-02,\n         6.49195984e-02,  4.65982035e-02,  2.09330842e-01,\n        -2.26685610e-02,  8.09596106e-02, -9.54415575e-02,\n         5.28485067e-02,  6.98626265e-02,  9.72635224e-02,\n         2.40689754e-01, -1.38374552e-01, -2.37485752e-01,\n        -1.22905090e-01,  4.45015840e-02, -1.14828303e-01,\n        -1.07989363e-01, -1.49183258e-01, -3.28495726e-03,\n        -1.60868894e-02,  1.28169268e-01, -1.14850454e-01,\n        -1.98942255e-02, -1.00085519e-01, -6.86361343e-02,\n         9.49511584e-03,  3.86737548e-02,  1.09858462e-03,\n        -1.18545368e-01, -6.33105170e-04],\n       [-2.04292208e-01,  1.45181283e-01,  3.47661600e-02,\n        -1.76134363e-01, -3.40912752e-02, -7.77934417e-02,\n        -1.50940195e-01,  3.08415224e-03, -1.57851696e-01,\n         2.09094554e-01, -3.77691351e-02, -1.60899565e-01,\n         1.92844623e-03,  1.30802751e-01,  1.46207481e-03,\n        -2.64379811e-02,  5.35440631e-02, -6.41801283e-02,\n         4.40865435e-04, -1.03171922e-01, -2.03811049e-01,\n        -1.53513685e-01, -4.53467667e-02, -4.19649854e-02,\n        -8.44733939e-02, -4.26665284e-02, -9.39067900e-02,\n         2.86218058e-02,  2.83620786e-02, -7.39018545e-02,\n        -3.61480750e-03,  6.75170943e-02],\n       [-1.06783889e-01, -5.10953851e-02, -1.25935942e-01,\n         1.01783790e-01,  3.16843099e-04, -5.06233284e-03,\n         6.18660338e-02, -5.74138165e-02, -7.81808496e-02,\n         1.08715490e-01,  8.02249610e-02,  6.77597299e-02,\n        -4.85224798e-02,  7.01503679e-02, -1.25864029e-01,\n        -1.07915737e-01,  6.20566793e-02, -3.77373062e-02,\n        -9.71285328e-02,  1.66522980e-01, -1.14919759e-01,\n        -1.08922280e-01, -5.04517145e-02, -2.13172272e-01,\n        -3.23094539e-02, -4.75259647e-02, -1.87456802e-01,\n         5.41688465e-02,  7.85206556e-02,  6.60896078e-02,\n        -2.04580612e-02,  5.85508533e-02],\n       [-1.56075791e-01,  4.29254286e-02,  7.06933066e-03,\n         2.20871329e-01, -2.67127994e-02,  3.56614366e-02,\n         4.25571948e-03,  7.11099757e-03,  2.08562482e-02,\n         3.60349901e-02, -1.00487480e-02, -2.62583029e-02,\n        -8.23024809e-02, -3.35723273e-02, -3.11435014e-02,\n        -3.06046698e-02,  8.26091841e-02, -6.63314480e-03,\n        -1.72030613e-01, -1.81203859e-03, -6.29716814e-02,\n        -7.48159438e-02,  7.42930919e-03,  1.08472466e-01,\n         1.36573330e-01, -1.09190121e-01, -6.06230088e-02,\n         1.09687090e-01, -4.26612794e-02,  6.61440566e-02,\n         4.00597826e-02, -7.87590370e-02],\n       [ 1.00730695e-01,  5.78069761e-02, -5.44823371e-02,\n        -5.25628887e-02, -2.59007444e-03,  1.89236123e-02,\n         2.83241309e-02,  1.39710471e-01,  7.35819191e-02,\n         5.13915420e-02, -1.54858528e-04, -1.85611755e-01,\n         8.13360233e-03,  1.11836672e-01,  1.76608726e-01,\n         3.68063189e-02,  3.61284129e-02,  9.41806659e-02,\n         1.64404456e-02,  1.23365417e-01, -1.15396373e-01,\n         1.06553875e-01, -4.70226370e-02,  1.17083928e-02,\n        -5.57335280e-02, -5.87051883e-02,  1.03090800e-01,\n         3.88951302e-02, -5.17024882e-02, -3.77286337e-02,\n         6.15277477e-02,  1.82412997e-01],\n       [-3.74686569e-02, -2.91141301e-01, -2.16458086e-02,\n         3.96759845e-02, -3.56581225e-03, -1.72561388e-02,\n         1.00392103e-01,  1.22536302e-01,  1.98951036e-01,\n         1.07664697e-01,  1.20320998e-01,  2.07861494e-02,\n        -3.57446633e-02, -1.07894177e-02,  9.45679992e-02,\n        -6.83238283e-02, -1.39222160e-01,  7.38982409e-02,\n         1.14277177e-01, -4.54236334e-03,  9.82567146e-02,\n         1.26963302e-01,  4.14014161e-02, -3.59870843e-03,\n         5.65115316e-03,  9.95140225e-02,  2.73675285e-03,\n        -6.41846359e-02,  1.51557520e-01, -1.48837836e-02,\n         1.80239782e-01,  1.01253711e-01]], dtype=float32)>\n  • key=tf.Tensor(shape=(4, 5, 64), dtype=float32)\n  • relative_position_encoding=tf.Tensor(shape=(4, 5, 64), dtype=float32)\n  • state=tf.Tensor(shape=(4, 20, 64), dtype=float32)\n  • attention_mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[0;32mIn [201]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m encoder_output, encoder_mems \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_mems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Input \u001b[0;32mIn [194]\u001b[0m, in \u001b[0;36mXlModel.call\u001b[0;34m(self, x, mems, training)\u001b[0m\n\u001b[1;32m     46\u001b[0m     block \u001b[38;5;241m=\u001b[39m embeddings[:,i:i\u001b[38;5;241m+\u001b[39mblock_size]\n\u001b[1;32m     47\u001b[0m     rel_block \u001b[38;5;241m=\u001b[39m rel_embeddings[:,i:i\u001b[38;5;241m+\u001b[39mblock_size]\n\u001b[0;32m---> 48\u001b[0m     encoder, mems \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_xl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent_stream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelative_position_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_block\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmems\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoder, mems\n",
      "Input \u001b[0;32mIn [193]\u001b[0m, in \u001b[0;36mTransformerXL.call\u001b[0;34m(self, content_stream, relative_position_encoding, state, content_attention_mask, query_attention_mask, target_mapping)\u001b[0m\n\u001b[1;32m    165\u001b[0m         positional_attention_bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_attention_bias[i]\n\u001b[1;32m    167\u001b[0m     transformer_xl_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_xl_layers[i]\n\u001b[0;32m--> 169\u001b[0m     transformer_xl_output \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer_xl_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontent_stream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_stream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontent_attention_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_attention_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpositional_attention_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositional_attention_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrelative_position_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_position_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontent_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mquery_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtarget_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     content_stream \u001b[38;5;241m=\u001b[39m transformer_xl_output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    180\u001b[0m output_stream \u001b[38;5;241m=\u001b[39m content_stream\n",
      "Input \u001b[0;32mIn [192]\u001b[0m, in \u001b[0;36mTransformerXLBlock.call\u001b[0;34m(self, content_stream, content_attention_bias, positional_attention_bias, relative_position_encoding, state, content_attention_mask, query_attention_mask, target_mapping)\u001b[0m\n\u001b[1;32m    171\u001b[0m common_attention_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    172\u001b[0m         content_attention_bias\u001b[38;5;241m=\u001b[39mcontent_attention_bias,\n\u001b[1;32m    173\u001b[0m         relative_position_encoding\u001b[38;5;241m=\u001b[39mrelative_position_encoding,\n\u001b[1;32m    174\u001b[0m         positional_attention_bias\u001b[38;5;241m=\u001b[39mpositional_attention_bias,\n\u001b[1;32m    175\u001b[0m         state\u001b[38;5;241m=\u001b[39mstate)\n\u001b[1;32m    177\u001b[0m attention_kwargs\u001b[38;5;241m.\u001b[39mupdate(common_attention_kwargs)\n\u001b[0;32m--> 178\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attention_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mattention_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m attention_stream \u001b[38;5;241m=\u001b[39m attention_output\n\u001b[1;32m    181\u001b[0m input_stream \u001b[38;5;241m=\u001b[39m content_stream\n",
      "Input \u001b[0;32mIn [188]\u001b[0m, in \u001b[0;36mMultiHeadRelativeAttention.call\u001b[0;34m(self, query, value, content_attention_bias, positional_attention_bias, key, relative_position_encoding, state, attention_mask)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# `position` = [B, L, N, H]\u001b[39;00m\n\u001b[1;32m    166\u001b[0m position \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encoding_dense(relative_position_encoding)\n\u001b[0;32m--> 168\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_attention_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_attention_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpositional_attention_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositional_attention_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# `attention_output` = [B, S, N, H]\u001b[39;00m\n\u001b[1;32m    178\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_dense(attention_output)\n",
      "Input \u001b[0;32mIn [188]\u001b[0m, in \u001b[0;36mMultiHeadRelativeAttention.compute_attention\u001b[0;34m(self, query, key, value, position, content_attention_bias, positional_attention_bias, attention_mask)\u001b[0m\n\u001b[1;32m     94\u001b[0m positional_attention \u001b[38;5;241m=\u001b[39m _rel_shift(positional_attention, klen\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mshape(content_attention)[\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m     96\u001b[0m tf\u001b[38;5;241m.\u001b[39mprint(content_attention\u001b[38;5;241m.\u001b[39mshape, positional_attention\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 98\u001b[0m attention_sum \u001b[38;5;241m=\u001b[39m \u001b[43mcontent_attention\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpositional_attention\u001b[49m\n\u001b[1;32m    100\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmultiply(attention_sum, \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key_dim)))\n\u001b[1;32m    102\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_masked_softmax(attention_scores, attention_mask)\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer \"rel_attn\" (type MultiHeadRelativeAttention).\n\nIncompatible shapes: [4,8,5,25] vs. [4,8,5,5] [Op:AddV2]\n\nCall arguments received:\n  • query=tf.Tensor(shape=(4, 5, 64), dtype=float32)\n  • value=tf.Tensor(shape=(4, 5, 64), dtype=float32)\n  • content_attention_bias=<tf.Variable 'content_attention_bias:0' shape=(8, 32) dtype=float32, numpy=\narray([[ 0.03287513,  0.1308676 ,  0.07159242,  0.03937121, -0.17795798,\n        -0.13579169, -0.0221532 , -0.05166721, -0.09662085,  0.15133546,\n        -0.05118976,  0.02578242, -0.04608143,  0.06914274, -0.10207377,\n         0.20638596, -0.0872229 ,  0.15885858, -0.12725139,  0.1462176 ,\n        -0.06859937,  0.09135687, -0.07438351, -0.02813614,  0.07960841,\n        -0.15449849, -0.03970689,  0.20876415,  0.11723583,  0.00644691,\n        -0.0389403 ,  0.04704792],\n       [-0.08524917, -0.2896531 , -0.02580698,  0.01670947,  0.05169223,\n         0.08683338,  0.0491168 , -0.04607744, -0.05214889,  0.02677131,\n        -0.08692616, -0.07404373, -0.14667545,  0.02935993, -0.00817272,\n        -0.02815646,  0.01483426, -0.04266326, -0.10919099,  0.03855965,\n        -0.07222566, -0.17054842,  0.00908754, -0.0622774 ,  0.12986699,\n        -0.03118214,  0.02299939, -0.00548464,  0.04589556, -0.05235977,\n        -0.05871842, -0.08726641],\n       [-0.06108912,  0.0438494 ,  0.04593267,  0.20173314,  0.02447603,\n        -0.08227833,  0.00147641, -0.13687143,  0.2736034 , -0.01947888,\n         0.04949598, -0.16006674, -0.05029876, -0.11988882,  0.16386232,\n         0.02060073, -0.22690168,  0.06145889,  0.02008096, -0.19718926,\n         0.01065039,  0.01022316, -0.26778615,  0.00291108,  0.03542108,\n         0.05421104,  0.03103381, -0.0334777 , -0.06713177,  0.12368794,\n        -0.0106899 ,  0.08349055],\n       [-0.06384157, -0.08431133, -0.00934188,  0.15116042, -0.04571173,\n         0.07931993, -0.04587935, -0.17895998, -0.0548207 , -0.07388108,\n         0.07076109,  0.06776614, -0.10873523, -0.04761241, -0.08560496,\n         0.09679097, -0.030956  , -0.05843683, -0.11706877,  0.14529875,\n        -0.06591242, -0.16234937,  0.04540564,  0.15005367,  0.06710535,\n         0.13785563, -0.0258772 , -0.20246577, -0.06959141,  0.02145141,\n         0.01335473,  0.07557549],\n       [-0.13630204, -0.1717239 ,  0.00492686, -0.18907766,  0.16415209,\n        -0.04626653, -0.03349418,  0.01609157,  0.02821062, -0.10073705,\n        -0.10090559,  0.22355044, -0.04221591,  0.04052363, -0.04928828,\n         0.06290404,  0.15480845,  0.00128631,  0.05774442,  0.0065458 ,\n        -0.08323361, -0.07131942,  0.13009597, -0.15306523, -0.02738934,\n        -0.03204426, -0.10733797,  0.02358152,  0.09766039, -0.0577438 ,\n         0.13668714,  0.03268134],\n       [ 0.06760236,  0.27838883,  0.0417542 , -0.03929676, -0.02233236,\n        -0.04500396, -0.08251137,  0.07393538,  0.03886604, -0.12139211,\n         0.01571097,  0.04667465,  0.08431952,  0.00632558, -0.00446788,\n        -0.19694899, -0.02558201,  0.05203411, -0.10902635, -0.1165527 ,\n         0.12617974, -0.05231718,  0.15782137, -0.16342278, -0.04333888,\n         0.19619061,  0.02044684,  0.01112492, -0.12149236,  0.11609381,\n        -0.02616549,  0.00677783],\n       [ 0.03577612,  0.04805276, -0.06840575, -0.02782409,  0.02700731,\n        -0.03902677, -0.12283748,  0.08405602,  0.11676797,  0.07905778,\n        -0.27158263, -0.06895094,  0.05131955, -0.04788188, -0.11596133,\n        -0.05523408, -0.0336453 ,  0.08770721, -0.03728735,  0.10794931,\n        -0.07981983, -0.11650636, -0.19115786, -0.11263958,  0.09011125,\n         0.26312906,  0.06182327, -0.13847993, -0.06165509, -0.12106355,\n        -0.01166811,  0.00145314],\n       [-0.06837669, -0.22426854, -0.06283196,  0.16505586,  0.12079521,\n        -0.02968594, -0.04697726,  0.07354838, -0.05533506,  0.13174044,\n         0.08842895,  0.19074515,  0.06131871, -0.12680566,  0.10271311,\n        -0.13683051, -0.12032788, -0.09521369, -0.00138116,  0.08564305,\n        -0.02926035, -0.01124342, -0.12745397,  0.05384149,  0.03256594,\n         0.01085354, -0.03050924,  0.09838126, -0.16953214,  0.03227901,\n         0.00547341,  0.07029521]], dtype=float32)>\n  • positional_attention_bias=<tf.Variable 'positional_attention_bias:0' shape=(8, 32) dtype=float32, numpy=\narray([[ 4.96523939e-02, -7.82599598e-02, -6.40058666e-02,\n         3.94228064e-02,  1.64101660e-01,  4.04064963e-03,\n         2.82683503e-02, -2.81508654e-01,  1.64980397e-01,\n        -2.12374598e-01,  4.17430012e-04,  4.64061052e-02,\n         1.93693683e-01,  5.17754853e-02,  1.03411965e-01,\n         2.45158561e-02, -8.06915909e-02, -4.27037254e-02,\n         1.54297771e-02, -1.87940837e-03,  1.75573211e-02,\n        -1.10835984e-01, -5.07753007e-02, -1.87343031e-01,\n        -1.04693763e-01,  3.02336868e-02, -9.48383808e-02,\n         1.44400196e-02, -8.06704238e-02,  1.08647726e-01,\n        -4.45318632e-02,  3.43445013e-03],\n       [-9.68781859e-02,  2.87369620e-02, -2.29990073e-02,\n        -1.56304047e-01,  1.35808038e-02,  5.15457056e-02,\n         1.13785051e-01,  2.99810860e-02, -1.11891523e-01,\n        -2.00788021e-01, -2.92625334e-02,  9.97205302e-02,\n        -2.15612408e-02, -3.90007906e-02, -6.77806363e-02,\n         1.24029927e-01, -9.05738305e-03, -1.15602769e-01,\n         2.85910457e-01, -9.33460072e-02, -9.83844697e-03,\n        -5.31302392e-03, -4.35913019e-02,  1.52017670e-02,\n         9.22025472e-04, -1.77757204e-01,  1.59892723e-01,\n         8.38689134e-02,  5.11010997e-02, -7.60706291e-02,\n        -1.05901696e-01,  1.52387798e-01],\n       [-3.03401481e-02,  1.63786393e-02,  3.07119396e-02,\n         6.49195984e-02,  4.65982035e-02,  2.09330842e-01,\n        -2.26685610e-02,  8.09596106e-02, -9.54415575e-02,\n         5.28485067e-02,  6.98626265e-02,  9.72635224e-02,\n         2.40689754e-01, -1.38374552e-01, -2.37485752e-01,\n        -1.22905090e-01,  4.45015840e-02, -1.14828303e-01,\n        -1.07989363e-01, -1.49183258e-01, -3.28495726e-03,\n        -1.60868894e-02,  1.28169268e-01, -1.14850454e-01,\n        -1.98942255e-02, -1.00085519e-01, -6.86361343e-02,\n         9.49511584e-03,  3.86737548e-02,  1.09858462e-03,\n        -1.18545368e-01, -6.33105170e-04],\n       [-2.04292208e-01,  1.45181283e-01,  3.47661600e-02,\n        -1.76134363e-01, -3.40912752e-02, -7.77934417e-02,\n        -1.50940195e-01,  3.08415224e-03, -1.57851696e-01,\n         2.09094554e-01, -3.77691351e-02, -1.60899565e-01,\n         1.92844623e-03,  1.30802751e-01,  1.46207481e-03,\n        -2.64379811e-02,  5.35440631e-02, -6.41801283e-02,\n         4.40865435e-04, -1.03171922e-01, -2.03811049e-01,\n        -1.53513685e-01, -4.53467667e-02, -4.19649854e-02,\n        -8.44733939e-02, -4.26665284e-02, -9.39067900e-02,\n         2.86218058e-02,  2.83620786e-02, -7.39018545e-02,\n        -3.61480750e-03,  6.75170943e-02],\n       [-1.06783889e-01, -5.10953851e-02, -1.25935942e-01,\n         1.01783790e-01,  3.16843099e-04, -5.06233284e-03,\n         6.18660338e-02, -5.74138165e-02, -7.81808496e-02,\n         1.08715490e-01,  8.02249610e-02,  6.77597299e-02,\n        -4.85224798e-02,  7.01503679e-02, -1.25864029e-01,\n        -1.07915737e-01,  6.20566793e-02, -3.77373062e-02,\n        -9.71285328e-02,  1.66522980e-01, -1.14919759e-01,\n        -1.08922280e-01, -5.04517145e-02, -2.13172272e-01,\n        -3.23094539e-02, -4.75259647e-02, -1.87456802e-01,\n         5.41688465e-02,  7.85206556e-02,  6.60896078e-02,\n        -2.04580612e-02,  5.85508533e-02],\n       [-1.56075791e-01,  4.29254286e-02,  7.06933066e-03,\n         2.20871329e-01, -2.67127994e-02,  3.56614366e-02,\n         4.25571948e-03,  7.11099757e-03,  2.08562482e-02,\n         3.60349901e-02, -1.00487480e-02, -2.62583029e-02,\n        -8.23024809e-02, -3.35723273e-02, -3.11435014e-02,\n        -3.06046698e-02,  8.26091841e-02, -6.63314480e-03,\n        -1.72030613e-01, -1.81203859e-03, -6.29716814e-02,\n        -7.48159438e-02,  7.42930919e-03,  1.08472466e-01,\n         1.36573330e-01, -1.09190121e-01, -6.06230088e-02,\n         1.09687090e-01, -4.26612794e-02,  6.61440566e-02,\n         4.00597826e-02, -7.87590370e-02],\n       [ 1.00730695e-01,  5.78069761e-02, -5.44823371e-02,\n        -5.25628887e-02, -2.59007444e-03,  1.89236123e-02,\n         2.83241309e-02,  1.39710471e-01,  7.35819191e-02,\n         5.13915420e-02, -1.54858528e-04, -1.85611755e-01,\n         8.13360233e-03,  1.11836672e-01,  1.76608726e-01,\n         3.68063189e-02,  3.61284129e-02,  9.41806659e-02,\n         1.64404456e-02,  1.23365417e-01, -1.15396373e-01,\n         1.06553875e-01, -4.70226370e-02,  1.17083928e-02,\n        -5.57335280e-02, -5.87051883e-02,  1.03090800e-01,\n         3.88951302e-02, -5.17024882e-02, -3.77286337e-02,\n         6.15277477e-02,  1.82412997e-01],\n       [-3.74686569e-02, -2.91141301e-01, -2.16458086e-02,\n         3.96759845e-02, -3.56581225e-03, -1.72561388e-02,\n         1.00392103e-01,  1.22536302e-01,  1.98951036e-01,\n         1.07664697e-01,  1.20320998e-01,  2.07861494e-02,\n        -3.57446633e-02, -1.07894177e-02,  9.45679992e-02,\n        -6.83238283e-02, -1.39222160e-01,  7.38982409e-02,\n         1.14277177e-01, -4.54236334e-03,  9.82567146e-02,\n         1.26963302e-01,  4.14014161e-02, -3.59870843e-03,\n         5.65115316e-03,  9.95140225e-02,  2.73675285e-03,\n        -6.41846359e-02,  1.51557520e-01, -1.48837836e-02,\n         1.80239782e-01,  1.01253711e-01]], dtype=float32)>\n  • key=tf.Tensor(shape=(4, 5, 64), dtype=float32)\n  • relative_position_encoding=tf.Tensor(shape=(4, 5, 64), dtype=float32)\n  • state=tf.Tensor(shape=(4, 20, 64), dtype=float32)\n  • attention_mask=None"
     ]
    }
   ],
   "source": [
    "encoder_output, encoder_mems = encoder(x, encoder_mems, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5f2f2e58-e12c-4653-96dd-a192a6103153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorShape([4, 20, 64])\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer \"rel_attn\" (type MultiHeadRelativeAttention).\n\nIncompatible shapes: [4,8,5,25] vs. [4,8,5,5] [Op:AddV2]\n\nCall arguments received:\n  • query=tf.Tensor(shape=(4, 5, 64), dtype=float32)\n  • value=tf.Tensor(shape=(4, 5, 64), dtype=float32)\n  • content_attention_bias=<tf.Variable 'content_attention_bias:0' shape=(8, 32) dtype=float32, numpy=\narray([[ 1.86760467e-03,  2.96271499e-03,  1.29611909e-01,\n        -5.04253507e-02,  4.23959568e-02, -1.52314007e-01,\n         1.35036483e-01, -2.10590407e-01,  1.24671713e-01,\n         7.45326728e-02,  1.12858191e-01,  1.39476076e-01,\n        -1.61036011e-02, -2.88501326e-02, -1.52743205e-01,\n        -6.64610639e-02,  1.21892050e-01, -8.35317597e-02,\n         4.55178171e-02, -2.65543442e-02, -1.91481814e-01,\n         5.58748422e-03, -4.28662589e-03,  3.10872328e-02,\n        -9.27438810e-02,  1.35184795e-01, -7.94980749e-02,\n         1.30243599e-01,  4.21416871e-02,  1.44851089e-01,\n         1.73747644e-01, -2.81722754e-01],\n       [-2.73013264e-02, -1.53096065e-01, -7.74430707e-02,\n         5.25790714e-02, -9.48117822e-02,  5.05147874e-02,\n         7.37752626e-03, -1.40409455e-01,  1.13601245e-01,\n        -6.11644685e-02, -4.80612653e-04, -6.74714446e-02,\n        -9.22912266e-03,  1.09922372e-01,  4.16588746e-02,\n        -9.95155424e-02,  1.56493768e-01,  6.81702495e-02,\n        -3.32556926e-02,  1.93071097e-01,  5.18371677e-03,\n         2.85548400e-02, -8.14884081e-02, -1.46719843e-01,\n        -7.19312653e-02,  8.49598646e-02, -3.23975682e-02,\n         8.65736045e-03, -1.24359541e-01,  1.63330317e-01,\n        -1.33950397e-01, -1.02232113e-01],\n       [ 5.26217632e-02,  7.45614618e-03, -4.60224599e-02,\n        -5.65767586e-02,  2.50339955e-01, -1.24724559e-01,\n        -2.54936665e-02,  1.07377470e-01,  7.02105463e-03,\n         1.18047468e-01, -1.19208775e-01, -1.38750717e-01,\n        -6.16014283e-03,  4.22470681e-02,  4.88653369e-02,\n        -2.14491598e-02, -2.63942033e-02, -1.50840893e-01,\n        -1.94865223e-02, -5.90865090e-02, -4.03517969e-02,\n         3.60847190e-02, -2.38979645e-02,  1.01032175e-01,\n         3.46795954e-02,  6.15636893e-02, -1.23791099e-01,\n         2.54135821e-02, -5.19496575e-02, -1.70345709e-01,\n        -2.99620539e-01,  5.69550879e-02],\n       [-3.73391621e-02, -1.83664665e-01,  6.63182512e-02,\n         1.62165627e-01,  7.39634261e-02, -1.26264710e-02,\n         1.58359498e-01,  9.41618066e-03,  1.92374989e-01,\n        -1.23934168e-03,  1.60974175e-01, -1.09189423e-02,\n         1.37387891e-03, -5.56089878e-02,  5.80279417e-02,\n        -9.31046307e-02,  2.37557292e-02,  5.82092069e-02,\n        -2.90416088e-02, -4.04951498e-02,  4.74704802e-02,\n        -1.44988760e-01, -4.75542694e-02,  7.80276358e-02,\n        -2.30430439e-02,  4.37893420e-02, -5.01797162e-02,\n        -1.62990429e-02, -9.11499634e-02,  1.36571690e-01,\n        -3.79026495e-03,  3.81513573e-02],\n       [-7.65870512e-02,  2.46438339e-01,  9.98804569e-02,\n        -1.57184564e-04, -1.93004265e-01,  2.04469040e-02,\n         2.38970108e-02, -6.28914461e-02,  2.74005383e-02,\n        -8.29580501e-02, -9.62458085e-03, -2.29094073e-01,\n        -4.36495468e-02, -4.63217385e-02,  9.70634818e-02,\n        -6.02457598e-02,  1.42826855e-01,  9.97336432e-02,\n        -1.05177304e-02,  1.16072511e-02,  8.35213289e-02,\n        -1.26945987e-01, -5.42786792e-02,  5.01059406e-02,\n        -1.06713228e-01,  8.60452652e-03,  1.23227231e-01,\n        -5.01168333e-02,  3.30822542e-02,  6.73990324e-02,\n         9.66971219e-02, -8.87334987e-04],\n       [ 3.39365564e-02, -6.94967136e-02, -8.27240869e-02,\n        -1.07503859e-02, -8.18172172e-02,  1.24142729e-01,\n         4.05189991e-02,  2.26532575e-02, -3.23076919e-02,\n        -9.35842022e-02, -9.45178270e-02, -4.31858905e-04,\n         5.88491149e-02,  1.51996136e-01, -3.67858745e-02,\n        -2.05330759e-01, -2.98511353e-03,  8.02104622e-02,\n         1.19937442e-01,  5.08464165e-02,  3.18324007e-02,\n         7.70572945e-02,  2.25839969e-02,  2.39333585e-02,\n        -9.44461003e-02,  9.39318240e-02,  2.08225194e-02,\n         4.71604615e-02,  1.55822620e-01, -2.61462457e-03,\n        -1.20508827e-01, -1.65657755e-02],\n       [ 1.19185723e-01, -5.71401082e-02, -1.06701897e-02,\n        -1.06115498e-01,  1.04041792e-01,  9.73331407e-02,\n         3.32008265e-02, -7.74608180e-02,  1.40470164e-02,\n         7.62039050e-02, -1.02972806e-01,  1.45190671e-01,\n        -4.27547134e-02, -6.89615831e-02, -1.05460444e-02,\n        -1.38200849e-01,  7.72772506e-02,  1.13719128e-01,\n        -1.48337588e-01, -1.25641912e-01,  1.32094249e-01,\n         2.22888310e-02,  1.44551620e-01,  5.89330271e-02,\n         7.75982961e-02, -5.41831143e-02,  1.14126233e-02,\n         1.09222136e-01, -8.81276280e-02,  3.34281884e-02,\n         1.86875060e-01, -7.80780688e-02],\n       [-1.29826829e-01,  1.28739581e-01, -4.09490764e-02,\n         8.79459735e-03,  3.16222124e-02,  5.56150973e-02,\n        -7.69851431e-02, -1.99419875e-02, -2.40382366e-02,\n        -2.24359669e-02, -4.21087705e-02,  1.87024072e-01,\n         8.39674398e-02,  7.59618282e-02,  1.87056344e-02,\n        -3.72773013e-03,  5.46418242e-02, -1.32175609e-02,\n         7.01750144e-02, -1.30321539e-03, -1.15837671e-01,\n         1.49826100e-02,  6.14379309e-02, -3.80162410e-02,\n        -1.16757369e-02,  7.20193461e-02, -8.92339870e-02,\n         2.72741634e-03, -1.14754504e-02,  2.36584917e-02,\n        -5.05498052e-02,  8.98344293e-02]], dtype=float32)>\n  • positional_attention_bias=<tf.Variable 'positional_attention_bias:0' shape=(8, 32) dtype=float32, numpy=\narray([[ 0.00166921,  0.08862785,  0.00261412,  0.04940394,  0.15308665,\n        -0.09178202, -0.12867403, -0.12105443, -0.13558179,  0.03733106,\n         0.03894128,  0.00513551, -0.0597451 , -0.03831561,  0.15701321,\n        -0.00275353,  0.05419687, -0.00746995,  0.03710644, -0.11294587,\n        -0.05427264, -0.09748205,  0.08716214, -0.01869964, -0.05977954,\n         0.12800835, -0.06509185, -0.00348323,  0.12779748, -0.0825047 ,\n         0.22245029, -0.07295617],\n       [-0.04455743,  0.15197101,  0.10192394,  0.01989247,  0.12194543,\n        -0.0055254 ,  0.02175089, -0.13339211,  0.06057492,  0.06518554,\n        -0.18679395,  0.0886391 ,  0.11348792, -0.1708391 ,  0.03518031,\n         0.02601352, -0.00948043,  0.02613938, -0.12685902, -0.00921613,\n        -0.00895548, -0.07772122, -0.00156366,  0.04918583, -0.01791807,\n        -0.02159614, -0.2046287 ,  0.114048  , -0.04012907, -0.15914385,\n         0.0357406 , -0.0741061 ],\n       [ 0.17192273, -0.13810621, -0.06348004,  0.10002126, -0.00365556,\n         0.03103833, -0.12644625, -0.02407562, -0.08196243, -0.087341  ,\n         0.08095241,  0.13935654,  0.09624406,  0.08142181, -0.13843961,\n        -0.08706938,  0.14830749, -0.04422303,  0.0430083 , -0.11605127,\n         0.15482378,  0.09451232,  0.14155506, -0.00213871,  0.09688843,\n        -0.03114037,  0.0120846 , -0.07991192,  0.07400804, -0.02396027,\n         0.07738224, -0.00668805],\n       [-0.07563897, -0.0226846 ,  0.02575532,  0.01323832,  0.01189098,\n        -0.1053839 , -0.0086355 , -0.15272951, -0.01448914,  0.19166628,\n        -0.00929106,  0.10931209, -0.10042938, -0.18893455, -0.08244561,\n        -0.04516983,  0.04378385,  0.04387952,  0.03288773,  0.09935167,\n         0.12457333,  0.11192733, -0.25518414,  0.12391932, -0.15518789,\n         0.15114337,  0.07295647, -0.07704467, -0.06923246, -0.09313115,\n        -0.16799785,  0.00263528],\n       [-0.06275319, -0.10231341,  0.03300872,  0.12996605,  0.00339183,\n        -0.06861132,  0.11550885, -0.01579003,  0.0266621 ,  0.2416278 ,\n        -0.07957758, -0.08195642,  0.02636874,  0.09275347,  0.0994712 ,\n         0.02369465,  0.08611587, -0.01953929,  0.05290255, -0.02964277,\n        -0.13719282, -0.11211308, -0.07548495,  0.02038859,  0.04670553,\n        -0.1697058 , -0.0180282 , -0.01725457, -0.1324236 , -0.06222239,\n         0.1392187 ,  0.09134556],\n       [-0.0075901 ,  0.14692953, -0.22240269,  0.12738155,  0.13458247,\n        -0.11654633, -0.10945475,  0.15268658, -0.00474202, -0.06650025,\n         0.15150982,  0.09090964,  0.25929436,  0.11870774,  0.20124209,\n        -0.08522146,  0.02976114, -0.06870893, -0.06282172, -0.2330557 ,\n         0.12364554, -0.19563963, -0.10731006,  0.00856675,  0.07725929,\n         0.19253306,  0.06325971,  0.15537083, -0.12416718,  0.10857838,\n         0.0998003 , -0.14662592],\n       [ 0.18371722,  0.02327947, -0.02264924, -0.10910469, -0.06054356,\n         0.05598119,  0.1255347 , -0.07189687, -0.15100671, -0.00449988,\n        -0.10396902, -0.0155946 , -0.13866226, -0.00406492,  0.02970638,\n         0.12426513,  0.03328478, -0.06766485,  0.07111903, -0.01027512,\n        -0.06626134, -0.16738795,  0.05631266, -0.03170551, -0.18940873,\n        -0.10567766,  0.13005953, -0.00648375, -0.06834257, -0.09877597,\n         0.03403797,  0.16298787],\n       [ 0.08628725, -0.06212065, -0.01481287, -0.09631216,  0.18259244,\n        -0.0846512 ,  0.02938559,  0.10101943, -0.05546873, -0.10422307,\n         0.16121781,  0.10890777, -0.03649526, -0.02243537, -0.07430811,\n         0.13201518,  0.13997065, -0.08366717, -0.15034764,  0.01020231,\n         0.03276598,  0.11002145,  0.11600568,  0.09627499, -0.00475578,\n         0.01659898, -0.1295519 , -0.06308933, -0.0601012 ,  0.07169006,\n         0.18448661, -0.02117009]], dtype=float32)>\n  • key=tf.Tensor(shape=(4, 5, 64), dtype=float32)\n  • relative_position_encoding=tf.Tensor(shape=(4, 5, 64), dtype=float32)\n  • state=tf.Tensor(shape=(4, 20, 64), dtype=float32)\n  • attention_mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[0;32mIn [172]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m decoder_output, decoder_mems \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_mems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Input \u001b[0;32mIn [165]\u001b[0m, in \u001b[0;36mDecoderXl.call\u001b[0;34m(self, encoder, pre_y, post_y, mems, training)\u001b[0m\n\u001b[1;32m     59\u001b[0m block \u001b[38;5;241m=\u001b[39m embeddings[:,i:i\u001b[38;5;241m+\u001b[39mblock_size]\n\u001b[1;32m     60\u001b[0m rel_block \u001b[38;5;241m=\u001b[39m rel_embeddings[:,i:i\u001b[38;5;241m+\u001b[39mblock_size]\n\u001b[0;32m---> 62\u001b[0m decoder, mems \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_xl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent_stream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelative_position_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_block\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmems\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m self_attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_y_attention_layer([decoder, encoder])\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Input \u001b[0;32mIn [146]\u001b[0m, in \u001b[0;36mTransformerXL.call\u001b[0;34m(self, content_stream, relative_position_encoding, state, content_attention_mask, query_attention_mask, target_mapping)\u001b[0m\n\u001b[1;32m    165\u001b[0m         positional_attention_bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_attention_bias[i]\n\u001b[1;32m    167\u001b[0m     transformer_xl_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_xl_layers[i]\n\u001b[0;32m--> 169\u001b[0m     transformer_xl_output \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer_xl_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontent_stream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_stream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontent_attention_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_attention_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpositional_attention_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositional_attention_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrelative_position_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_position_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontent_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mquery_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtarget_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     content_stream \u001b[38;5;241m=\u001b[39m transformer_xl_output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    180\u001b[0m output_stream \u001b[38;5;241m=\u001b[39m content_stream\n",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36mTransformerXLBlock.call\u001b[0;34m(self, content_stream, content_attention_bias, positional_attention_bias, relative_position_encoding, state, content_attention_mask, query_attention_mask, target_mapping)\u001b[0m\n\u001b[1;32m    171\u001b[0m common_attention_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    172\u001b[0m         content_attention_bias\u001b[38;5;241m=\u001b[39mcontent_attention_bias,\n\u001b[1;32m    173\u001b[0m         relative_position_encoding\u001b[38;5;241m=\u001b[39mrelative_position_encoding,\n\u001b[1;32m    174\u001b[0m         positional_attention_bias\u001b[38;5;241m=\u001b[39mpositional_attention_bias,\n\u001b[1;32m    175\u001b[0m         state\u001b[38;5;241m=\u001b[39mstate)\n\u001b[1;32m    177\u001b[0m attention_kwargs\u001b[38;5;241m.\u001b[39mupdate(common_attention_kwargs)\n\u001b[0;32m--> 178\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attention_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mattention_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m attention_stream \u001b[38;5;241m=\u001b[39m attention_output\n\u001b[1;32m    181\u001b[0m input_stream \u001b[38;5;241m=\u001b[39m content_stream\n",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36mMultiHeadRelativeAttention.call\u001b[0;34m(self, query, value, content_attention_bias, positional_attention_bias, key, relative_position_encoding, state, attention_mask)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# `position` = [B, L, N, H]\u001b[39;00m\n\u001b[1;32m    160\u001b[0m position \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encoding_dense(relative_position_encoding)\n\u001b[0;32m--> 162\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_attention_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_attention_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpositional_attention_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositional_attention_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# `attention_output` = [B, S, N, H]\u001b[39;00m\n\u001b[1;32m    172\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_dense(attention_output)\n",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36mMultiHeadRelativeAttention.compute_attention\u001b[0;34m(self, query, key, value, position, content_attention_bias, positional_attention_bias, attention_mask)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m#BD\u001b[39;00m\n\u001b[1;32m     90\u001b[0m positional_attention \u001b[38;5;241m=\u001b[39m _rel_shift(positional_attention, klen\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mshape(content_attention)[\u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m---> 92\u001b[0m attention_sum \u001b[38;5;241m=\u001b[39m \u001b[43mcontent_attention\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpositional_attention\u001b[49m\n\u001b[1;32m     94\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmultiply(attention_sum, \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key_dim)))\n\u001b[1;32m     96\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_masked_softmax(attention_scores, attention_mask)\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer \"rel_attn\" (type MultiHeadRelativeAttention).\n\nIncompatible shapes: [4,8,5,25] vs. [4,8,5,5] [Op:AddV2]\n\nCall arguments received:\n  • query=tf.Tensor(shape=(4, 5, 64), dtype=float32)\n  • value=tf.Tensor(shape=(4, 5, 64), dtype=float32)\n  • content_attention_bias=<tf.Variable 'content_attention_bias:0' shape=(8, 32) dtype=float32, numpy=\narray([[ 1.86760467e-03,  2.96271499e-03,  1.29611909e-01,\n        -5.04253507e-02,  4.23959568e-02, -1.52314007e-01,\n         1.35036483e-01, -2.10590407e-01,  1.24671713e-01,\n         7.45326728e-02,  1.12858191e-01,  1.39476076e-01,\n        -1.61036011e-02, -2.88501326e-02, -1.52743205e-01,\n        -6.64610639e-02,  1.21892050e-01, -8.35317597e-02,\n         4.55178171e-02, -2.65543442e-02, -1.91481814e-01,\n         5.58748422e-03, -4.28662589e-03,  3.10872328e-02,\n        -9.27438810e-02,  1.35184795e-01, -7.94980749e-02,\n         1.30243599e-01,  4.21416871e-02,  1.44851089e-01,\n         1.73747644e-01, -2.81722754e-01],\n       [-2.73013264e-02, -1.53096065e-01, -7.74430707e-02,\n         5.25790714e-02, -9.48117822e-02,  5.05147874e-02,\n         7.37752626e-03, -1.40409455e-01,  1.13601245e-01,\n        -6.11644685e-02, -4.80612653e-04, -6.74714446e-02,\n        -9.22912266e-03,  1.09922372e-01,  4.16588746e-02,\n        -9.95155424e-02,  1.56493768e-01,  6.81702495e-02,\n        -3.32556926e-02,  1.93071097e-01,  5.18371677e-03,\n         2.85548400e-02, -8.14884081e-02, -1.46719843e-01,\n        -7.19312653e-02,  8.49598646e-02, -3.23975682e-02,\n         8.65736045e-03, -1.24359541e-01,  1.63330317e-01,\n        -1.33950397e-01, -1.02232113e-01],\n       [ 5.26217632e-02,  7.45614618e-03, -4.60224599e-02,\n        -5.65767586e-02,  2.50339955e-01, -1.24724559e-01,\n        -2.54936665e-02,  1.07377470e-01,  7.02105463e-03,\n         1.18047468e-01, -1.19208775e-01, -1.38750717e-01,\n        -6.16014283e-03,  4.22470681e-02,  4.88653369e-02,\n        -2.14491598e-02, -2.63942033e-02, -1.50840893e-01,\n        -1.94865223e-02, -5.90865090e-02, -4.03517969e-02,\n         3.60847190e-02, -2.38979645e-02,  1.01032175e-01,\n         3.46795954e-02,  6.15636893e-02, -1.23791099e-01,\n         2.54135821e-02, -5.19496575e-02, -1.70345709e-01,\n        -2.99620539e-01,  5.69550879e-02],\n       [-3.73391621e-02, -1.83664665e-01,  6.63182512e-02,\n         1.62165627e-01,  7.39634261e-02, -1.26264710e-02,\n         1.58359498e-01,  9.41618066e-03,  1.92374989e-01,\n        -1.23934168e-03,  1.60974175e-01, -1.09189423e-02,\n         1.37387891e-03, -5.56089878e-02,  5.80279417e-02,\n        -9.31046307e-02,  2.37557292e-02,  5.82092069e-02,\n        -2.90416088e-02, -4.04951498e-02,  4.74704802e-02,\n        -1.44988760e-01, -4.75542694e-02,  7.80276358e-02,\n        -2.30430439e-02,  4.37893420e-02, -5.01797162e-02,\n        -1.62990429e-02, -9.11499634e-02,  1.36571690e-01,\n        -3.79026495e-03,  3.81513573e-02],\n       [-7.65870512e-02,  2.46438339e-01,  9.98804569e-02,\n        -1.57184564e-04, -1.93004265e-01,  2.04469040e-02,\n         2.38970108e-02, -6.28914461e-02,  2.74005383e-02,\n        -8.29580501e-02, -9.62458085e-03, -2.29094073e-01,\n        -4.36495468e-02, -4.63217385e-02,  9.70634818e-02,\n        -6.02457598e-02,  1.42826855e-01,  9.97336432e-02,\n        -1.05177304e-02,  1.16072511e-02,  8.35213289e-02,\n        -1.26945987e-01, -5.42786792e-02,  5.01059406e-02,\n        -1.06713228e-01,  8.60452652e-03,  1.23227231e-01,\n        -5.01168333e-02,  3.30822542e-02,  6.73990324e-02,\n         9.66971219e-02, -8.87334987e-04],\n       [ 3.39365564e-02, -6.94967136e-02, -8.27240869e-02,\n        -1.07503859e-02, -8.18172172e-02,  1.24142729e-01,\n         4.05189991e-02,  2.26532575e-02, -3.23076919e-02,\n        -9.35842022e-02, -9.45178270e-02, -4.31858905e-04,\n         5.88491149e-02,  1.51996136e-01, -3.67858745e-02,\n        -2.05330759e-01, -2.98511353e-03,  8.02104622e-02,\n         1.19937442e-01,  5.08464165e-02,  3.18324007e-02,\n         7.70572945e-02,  2.25839969e-02,  2.39333585e-02,\n        -9.44461003e-02,  9.39318240e-02,  2.08225194e-02,\n         4.71604615e-02,  1.55822620e-01, -2.61462457e-03,\n        -1.20508827e-01, -1.65657755e-02],\n       [ 1.19185723e-01, -5.71401082e-02, -1.06701897e-02,\n        -1.06115498e-01,  1.04041792e-01,  9.73331407e-02,\n         3.32008265e-02, -7.74608180e-02,  1.40470164e-02,\n         7.62039050e-02, -1.02972806e-01,  1.45190671e-01,\n        -4.27547134e-02, -6.89615831e-02, -1.05460444e-02,\n        -1.38200849e-01,  7.72772506e-02,  1.13719128e-01,\n        -1.48337588e-01, -1.25641912e-01,  1.32094249e-01,\n         2.22888310e-02,  1.44551620e-01,  5.89330271e-02,\n         7.75982961e-02, -5.41831143e-02,  1.14126233e-02,\n         1.09222136e-01, -8.81276280e-02,  3.34281884e-02,\n         1.86875060e-01, -7.80780688e-02],\n       [-1.29826829e-01,  1.28739581e-01, -4.09490764e-02,\n         8.79459735e-03,  3.16222124e-02,  5.56150973e-02,\n        -7.69851431e-02, -1.99419875e-02, -2.40382366e-02,\n        -2.24359669e-02, -4.21087705e-02,  1.87024072e-01,\n         8.39674398e-02,  7.59618282e-02,  1.87056344e-02,\n        -3.72773013e-03,  5.46418242e-02, -1.32175609e-02,\n         7.01750144e-02, -1.30321539e-03, -1.15837671e-01,\n         1.49826100e-02,  6.14379309e-02, -3.80162410e-02,\n        -1.16757369e-02,  7.20193461e-02, -8.92339870e-02,\n         2.72741634e-03, -1.14754504e-02,  2.36584917e-02,\n        -5.05498052e-02,  8.98344293e-02]], dtype=float32)>\n  • positional_attention_bias=<tf.Variable 'positional_attention_bias:0' shape=(8, 32) dtype=float32, numpy=\narray([[ 0.00166921,  0.08862785,  0.00261412,  0.04940394,  0.15308665,\n        -0.09178202, -0.12867403, -0.12105443, -0.13558179,  0.03733106,\n         0.03894128,  0.00513551, -0.0597451 , -0.03831561,  0.15701321,\n        -0.00275353,  0.05419687, -0.00746995,  0.03710644, -0.11294587,\n        -0.05427264, -0.09748205,  0.08716214, -0.01869964, -0.05977954,\n         0.12800835, -0.06509185, -0.00348323,  0.12779748, -0.0825047 ,\n         0.22245029, -0.07295617],\n       [-0.04455743,  0.15197101,  0.10192394,  0.01989247,  0.12194543,\n        -0.0055254 ,  0.02175089, -0.13339211,  0.06057492,  0.06518554,\n        -0.18679395,  0.0886391 ,  0.11348792, -0.1708391 ,  0.03518031,\n         0.02601352, -0.00948043,  0.02613938, -0.12685902, -0.00921613,\n        -0.00895548, -0.07772122, -0.00156366,  0.04918583, -0.01791807,\n        -0.02159614, -0.2046287 ,  0.114048  , -0.04012907, -0.15914385,\n         0.0357406 , -0.0741061 ],\n       [ 0.17192273, -0.13810621, -0.06348004,  0.10002126, -0.00365556,\n         0.03103833, -0.12644625, -0.02407562, -0.08196243, -0.087341  ,\n         0.08095241,  0.13935654,  0.09624406,  0.08142181, -0.13843961,\n        -0.08706938,  0.14830749, -0.04422303,  0.0430083 , -0.11605127,\n         0.15482378,  0.09451232,  0.14155506, -0.00213871,  0.09688843,\n        -0.03114037,  0.0120846 , -0.07991192,  0.07400804, -0.02396027,\n         0.07738224, -0.00668805],\n       [-0.07563897, -0.0226846 ,  0.02575532,  0.01323832,  0.01189098,\n        -0.1053839 , -0.0086355 , -0.15272951, -0.01448914,  0.19166628,\n        -0.00929106,  0.10931209, -0.10042938, -0.18893455, -0.08244561,\n        -0.04516983,  0.04378385,  0.04387952,  0.03288773,  0.09935167,\n         0.12457333,  0.11192733, -0.25518414,  0.12391932, -0.15518789,\n         0.15114337,  0.07295647, -0.07704467, -0.06923246, -0.09313115,\n        -0.16799785,  0.00263528],\n       [-0.06275319, -0.10231341,  0.03300872,  0.12996605,  0.00339183,\n        -0.06861132,  0.11550885, -0.01579003,  0.0266621 ,  0.2416278 ,\n        -0.07957758, -0.08195642,  0.02636874,  0.09275347,  0.0994712 ,\n         0.02369465,  0.08611587, -0.01953929,  0.05290255, -0.02964277,\n        -0.13719282, -0.11211308, -0.07548495,  0.02038859,  0.04670553,\n        -0.1697058 , -0.0180282 , -0.01725457, -0.1324236 , -0.06222239,\n         0.1392187 ,  0.09134556],\n       [-0.0075901 ,  0.14692953, -0.22240269,  0.12738155,  0.13458247,\n        -0.11654633, -0.10945475,  0.15268658, -0.00474202, -0.06650025,\n         0.15150982,  0.09090964,  0.25929436,  0.11870774,  0.20124209,\n        -0.08522146,  0.02976114, -0.06870893, -0.06282172, -0.2330557 ,\n         0.12364554, -0.19563963, -0.10731006,  0.00856675,  0.07725929,\n         0.19253306,  0.06325971,  0.15537083, -0.12416718,  0.10857838,\n         0.0998003 , -0.14662592],\n       [ 0.18371722,  0.02327947, -0.02264924, -0.10910469, -0.06054356,\n         0.05598119,  0.1255347 , -0.07189687, -0.15100671, -0.00449988,\n        -0.10396902, -0.0155946 , -0.13866226, -0.00406492,  0.02970638,\n         0.12426513,  0.03328478, -0.06766485,  0.07111903, -0.01027512,\n        -0.06626134, -0.16738795,  0.05631266, -0.03170551, -0.18940873,\n        -0.10567766,  0.13005953, -0.00648375, -0.06834257, -0.09877597,\n         0.03403797,  0.16298787],\n       [ 0.08628725, -0.06212065, -0.01481287, -0.09631216,  0.18259244,\n        -0.0846512 ,  0.02938559,  0.10101943, -0.05546873, -0.10422307,\n         0.16121781,  0.10890777, -0.03649526, -0.02243537, -0.07430811,\n         0.13201518,  0.13997065, -0.08366717, -0.15034764,  0.01020231,\n         0.03276598,  0.11002145,  0.11600568,  0.09627499, -0.00475578,\n         0.01659898, -0.1295519 , -0.06308933, -0.0601012 ,  0.07169006,\n         0.18448661, -0.02117009]], dtype=float32)>\n  • key=tf.Tensor(shape=(4, 5, 64), dtype=float32)\n  • relative_position_encoding=tf.Tensor(shape=(4, 5, 64), dtype=float32)\n  • state=tf.Tensor(shape=(4, 20, 64), dtype=float32)\n  • attention_mask=None"
     ]
    }
   ],
   "source": [
    "decoder_output, decoder_mems = decoder(encoder_output, pre_y, post_y, decoder_mems, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "cf85315a-e013-48fb-8e4f-284d65b7e5e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8, 4, 5, 64), dtype=float32, numpy=\n",
       "array([[[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_mems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c726b5aa-bc90-44a8-bc2e-5bf755a0dcdc",
   "metadata": {},
   "source": [
    "---\n",
    "# Train Step Test Split Inside Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3c9d81f1-57e9-416c-a7ea-bf8b8a3c2881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(batch):\n",
    "    #Create memories\n",
    "    encoder_mems = tf.zeros((num_layers, batch_size, memory_length, embed_dim))\n",
    "    decoder_mems = tf.zeros((num_layers, batch_size, memory_length, embed_dim))\n",
    "\n",
    "    #Get batch\n",
    "    x, pre_y, post_y = batch\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        #Send full sequence into model\n",
    "        encoder_output, encoder_mems = encoder(x, encoder_mems, training=True)\n",
    "        \n",
    "        #Send last output into decoder\n",
    "        decoder_output, decoder_mems = decoder(encoder_output, pre_y, post_y, decoder_mems, training=True)\n",
    "\n",
    "        #Calculate loss and accuraccy\n",
    "        loss = MaskedSparseCategoricalCrossentropy(post_y, decoder_output)\n",
    "    \n",
    "    accuracy = MaskedSparseCategoricalAccuracy(post_y, decoder_output)\n",
    "          \n",
    "    grads = tape.gradient(loss, decoder.trainable_weights)\n",
    "    \n",
    "    encoder.optimizer.apply_gradients(zip(grads, encoder.trainable_weights))\n",
    "    decoder.optimizer.apply_gradients(zip(grads, decoder.trainable_weights))\n",
    "    \n",
    "    return loss, accuracy\n",
    "@tf.function()\n",
    "def dist_train_step(batch):\n",
    "    losses, accuracy = strategy.run(train_step, args=(batch,))\n",
    "    return strategy.reduce(tf.distribute.ReduceOp.SUM, losses, axis=None), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6fafad8d-f020-49b0-8233-bcfe54b88309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def train_step(batch):\n",
    "    \n",
    "#     mems = tf.zeros((num_layers, batch_size, memory_length, embed_dim))\n",
    "#     decoder_mems = tf.zeros((num_layers, batch_size, memory_length, embed_dim))\n",
    "    \n",
    "#     x, y, y_post = batch\n",
    "\n",
    "#     #gradients = [tf.zeros_like(w) for w in model.trainable_weights]\n",
    "    \n",
    "#     for i in range(0, seq_len, block_size):\n",
    "#         result, mems = model(x[:,i:i+block_size], mems, training=True)\n",
    "#         output, decoder_mems = decoder(result, pre_y[:,i:i+block_size], decoder_mems)\n",
    "            \n",
    "            \n",
    "#     with tf.GradientTape() as tape:\n",
    "#         result, mems = model(x[:,i+block_size:], mems, training=True)\n",
    "#         output, decoder_mems = decoder(result, pre_y[:,i:i+block_size], decoder_mems, training=True)\n",
    "        \n",
    "#         loss = MaskedSparseCategoricalCrossentropy(y_post, result)\n",
    "#     accuracy = MaskedSparseCategoricalAccuracy(y_post, result)\n",
    "        \n",
    "#     grads = tape.gradient(loss, model.trainable_weights)\n",
    "        \n",
    "#     encoder.optimizer.apply_gradients(zip(grads, encoder.trainable_weights))\n",
    "#     decoder.optimizer.apply_gradients(zip(grads, decoder.trainable_weights))\n",
    "    \n",
    "#     return loss, accuracy\n",
    "\n",
    "# @tf.function()\n",
    "# def dist_train_step(batch):\n",
    "#     losses, accuracy = strategy.run(train_step, args=(batch,))\n",
    "#     return strategy.reduce(tf.distribute.ReduceOp.SUM, losses, axis=None), accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63c3a2e-02b5-4dbe-9527-a33c23338ce5",
   "metadata": {},
   "source": [
    "---\n",
    "# Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5a6127f4-fb94-4bf2-b3cf-fae83ef0aa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_loss = []\n",
    "history_accuracy = []\n",
    "epochs = 270"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a260118e-c39d-4232-8ee2-11ae86d8c8f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_15257/922320793.py\", line 30, in dist_train_step  *\n        losses, accuracy = strategy.run(train_step, args=(batch,))\n    File \"/tmp/ipykernel_15257/1960824984.py\", line 24, in train_step  *\n        encoder.optimizer.apply_gradients(zip(grads, encoder.trainable_weights))\n    File \"/opt/conda/lib/python3.10/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 639, in apply_gradients  **\n        self._create_all_weights(var_list)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 825, in _create_all_weights\n        self._create_slots(var_list)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/optimizer_v2/adam.py\", line 117, in _create_slots\n        self.add_slot(var, 'm')\n    File \"/opt/conda/lib/python3.10/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 902, in add_slot\n        raise ValueError(\n\n    ValueError: Trying to create optimizer slot variable under the scope for tf.distribute.Strategy (<tensorflow.python.distribute.one_device_strategy.OneDeviceStrategy object at 0x7f89f92a8580>), which is different from the scope used for the original variable (<tf.Variable 'xl_model_6/masked_token_and_position_embedding_12/embedding_24/embeddings:0' shape=(57, 64) dtype=float32, numpy=\n    array([[ 0.00878707, -0.00860304, -0.03164119, ..., -0.01050074,\n            -0.00879501,  0.04384686],\n           [ 0.03190098, -0.0034516 ,  0.04245115, ..., -0.03974744,\n             0.01376127, -0.02328465],\n           [-0.03240434,  0.04820097, -0.02156936, ..., -0.04540182,\n             0.03295   ,  0.02345854],\n           ...,\n           [-0.00915939, -0.03698056, -0.04161053, ..., -0.0370314 ,\n             0.00347018, -0.04946062],\n           [-0.033875  ,  0.03710565,  0.04423403, ...,  0.04675548,\n             0.03480533, -0.034811  ],\n           [-0.02764192, -0.00176588,  0.02258297, ..., -0.03572499,\n            -0.01234657, -0.02274151]], dtype=float32)>). Make sure the slot variables are created under the same strategy scope. This may happen if you're restoring from a checkpoint outside the scope.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [122]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      4\u001b[0m     batch \u001b[38;5;241m=\u001b[39m Full_Batch(batch_size, x_flattened, pre_y_flattened, post_y_flattened, num_chars_data, seq_len, rng)\n\u001b[0;32m----> 6\u001b[0m     loss, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mdist_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     history_loss\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m      9\u001b[0m     history_accuracy\u001b[38;5;241m.\u001b[39mappend(accuracy) \n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m   1148\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_15257/922320793.py\", line 30, in dist_train_step  *\n        losses, accuracy = strategy.run(train_step, args=(batch,))\n    File \"/tmp/ipykernel_15257/1960824984.py\", line 24, in train_step  *\n        encoder.optimizer.apply_gradients(zip(grads, encoder.trainable_weights))\n    File \"/opt/conda/lib/python3.10/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 639, in apply_gradients  **\n        self._create_all_weights(var_list)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 825, in _create_all_weights\n        self._create_slots(var_list)\n    File \"/opt/conda/lib/python3.10/site-packages/keras/optimizer_v2/adam.py\", line 117, in _create_slots\n        self.add_slot(var, 'm')\n    File \"/opt/conda/lib/python3.10/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 902, in add_slot\n        raise ValueError(\n\n    ValueError: Trying to create optimizer slot variable under the scope for tf.distribute.Strategy (<tensorflow.python.distribute.one_device_strategy.OneDeviceStrategy object at 0x7f89f92a8580>), which is different from the scope used for the original variable (<tf.Variable 'xl_model_6/masked_token_and_position_embedding_12/embedding_24/embeddings:0' shape=(57, 64) dtype=float32, numpy=\n    array([[ 0.00878707, -0.00860304, -0.03164119, ..., -0.01050074,\n            -0.00879501,  0.04384686],\n           [ 0.03190098, -0.0034516 ,  0.04245115, ..., -0.03974744,\n             0.01376127, -0.02328465],\n           [-0.03240434,  0.04820097, -0.02156936, ..., -0.04540182,\n             0.03295   ,  0.02345854],\n           ...,\n           [-0.00915939, -0.03698056, -0.04161053, ..., -0.0370314 ,\n             0.00347018, -0.04946062],\n           [-0.033875  ,  0.03710565,  0.04423403, ...,  0.04675548,\n             0.03480533, -0.034811  ],\n           [-0.02764192, -0.00176588,  0.02258297, ..., -0.03572499,\n            -0.01234657, -0.02274151]], dtype=float32)>). Make sure the slot variables are created under the same strategy scope. This may happen if you're restoring from a checkpoint outside the scope.\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        batch = Full_Batch(batch_size, x_flattened, pre_y_flattened, post_y_flattened, num_chars_data, seq_len, rng)\n",
    "        \n",
    "        loss, accuracy = dist_train_step(batch)\n",
    "        \n",
    "        history_loss.append(loss)\n",
    "        history_accuracy.append(accuracy) \n",
    "        \n",
    "        print(f\"\\r{epoch+1}/{epochs} Loss: {loss} Accuracy = {accuracy}\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c45c04-e528-4cf2-865b-8b679b0b95c0",
   "metadata": {},
   "source": [
    "---\n",
    "# Memory vs Accuracy Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724ebe19-73b6-4aeb-b506-f24d6722ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_lens = []\n",
    "final_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc71d008-1f06-4d2b-a8b4-0ed7fb8603d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_lens = np.loadtxt(\"/home/jovyan/TransformerXL/XL/Mem_Lens.txt\", dtype=np.float64)\n",
    "final_accuracies =  np.loadtxt(\"/home/jovyan/TransformerXL/XL/Final_Accuracies.txt\", dtype=np.float64)\n",
    "\n",
    "print(mem_lens)\n",
    "print(final_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5293b4-e6a7-4fbb-a40a-512e965573c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_lens = np.append(mem_lens, memory_length)\n",
    "final_accuracies = np.append(final_accuracies, accuracy)\n",
    "print(mem_lens)\n",
    "print(final_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779262e0-716e-431b-9e95-464bb76b8f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_accuracy = ({mem_len:accuracy for mem_len, accuracy in zip(mem_lens, final_accuracies)})\n",
    "mem_lens_sorted = np.array([l for l in sorted(mem_accuracy)])\n",
    "final_accuracies_sorted = np.array([mem_accuracy[l] for l in mem_lens_sorted])\n",
    "print(mem_lens_sorted)\n",
    "print(final_accuracies_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd866ef-9445-40a7-9d7e-85071baf9b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('/home/jovyan/TransformerXL/XL/Mem_Lens.txt', (mem_lens), fmt='%f', delimiter='\\n')\n",
    "np.savetxt('/home/jovyan/TransformerXL/XL//Final_Accuracies.txt', (final_accuracies), fmt='%f', delimiter='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cba907-7703-4b57-ba10-76a59b332b7b",
   "metadata": {},
   "source": [
    "---\n",
    "# Accuracy vs Loss Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2dabdb-0f8b-4c68-91a0-72d0dd47d056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.plot(history_accuracy)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(history_loss)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "#plt.title(\"Accuracy and Loss Over Time\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e931bbd-a1a9-4098-a4f2-0ef29f7fd89c",
   "metadata": {},
   "source": [
    "---\n",
    "# Memory vs Accuracy Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba942651-37b0-465e-b3c0-5dd22621b61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(2)\n",
    "plt.subplot(222)\n",
    "plt.plot(mem_lens_sorted, final_accuracies_sorted)\n",
    "plt.xlabel('Memory Length')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
