{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dcad43d-67f7-49b9-84ce-6d91a0f468a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e7cb9df-5746-44a3-998a-b671b284957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fda64db1-8058-4e15-b2d4-1a3e64f23fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "import math\n",
    "import string\n",
    "\n",
    "import tf_utils as tfu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dfb3b1a-2db7-4463-a208-b5d29e20f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tfu.strategy.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45d5d65-b728-42de-b529-85643adef96d",
   "metadata": {},
   "source": [
    "# Vanilla Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a199f10-268d-4153-863c-2d430a4d2c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedTransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(MaskedTransformerBlock, self).__init__()\n",
    "        self.att1 = keras.layers.MultiHeadAttention(num_heads=num_heads,\n",
    "        key_dim=embed_dim)\n",
    "        self.att2 = keras.layers.MultiHeadAttention(num_heads=num_heads,\n",
    "        key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "        [keras.layers.Dense(ff_dim, activation=\"gelu\"),\n",
    "        keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "        self.dropout3 = keras.layers.Dropout(rate)\n",
    "    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n",
    "        i = tf.range(n_dest)[:, None]\n",
    "        j = tf.range(n_src)\n",
    "        m = i >= j - n_src + n_dest\n",
    "        mask = tf.cast(m, dtype)\n",
    "        mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "        mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "    def call(self, inputs, training):\n",
    "        input_shape = tf.shape(inputs[0])\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        mask = self.causal_attention_mask(batch_size,\n",
    "        seq_len, seq_len,\n",
    "        tf.bool)\n",
    "        attn_output1 = self.att1(inputs[0], inputs[0],\n",
    "        attention_mask = mask)\n",
    "        attn_output1 = self.dropout1(attn_output1, training=training)\n",
    "        out1 = self.layernorm1(inputs[0] + attn_output1)\n",
    "        attn_output2 = self.att2(out1, inputs[1])\n",
    "        attn_output2 = self.dropout2(attn_output2, training=training)\n",
    "        out2 = self.layernorm1(out1 + attn_output2)\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        return self.layernorm2(out2 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a27a7c63-8d17-4e49-bfd7-4e05d92eb915",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedTokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim, **kwargs):\n",
    "        super(MaskedTokenAndPositionEmbedding, self).__init__(**kwargs)\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size,\n",
    "        output_dim=embed_dim,\n",
    "        mask_zero=True)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen+1,\n",
    "        output_dim=embed_dim,\n",
    "        mask_zero=True)\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=1, limit=maxlen+1, delta=1)\n",
    "        positions = positions * tf.cast(tf.sign(x),tf.int32)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "160d05e4-b851-4348-85df-508526fb154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "def MaskedSparseCategoricalCrossentropy(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "def MaskedSparseCategoricalAccuracy(real, pred):\n",
    "    accuracies = tf.equal(tf.cast(real,tf.int64), tf.argmax(pred, axis=2))\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b261be2-4cbf-4b92-ae0b-de1f6d86226f",
   "metadata": {},
   "source": [
    "---\n",
    "# TF Official Port"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b66c98-f8b5-4b80-9736-66a9769cc966",
   "metadata": {},
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9af4ac04-1eab-4d74-bfc0-716408efeed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_initializer(initializer):\n",
    "    if isinstance(initializer, tf.keras.initializers.Initializer):\n",
    "        return initializer.__class__.from_config(initializer.get_config())\n",
    "    return initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aadfeaaa-08b0-4155-8381-328ac71f4c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape_list(tensor, expected_rank=None, name=None):\n",
    "    if expected_rank is not None:\n",
    "        assert_rank(tensor, expected_rank, name)\n",
    "\n",
    "    shape = tensor.shape.as_list()\n",
    "\n",
    "    non_static_indexes = []\n",
    "    for (index, dim) in enumerate(shape):\n",
    "        if dim is None:\n",
    "            non_static_indexes.append(index)\n",
    "\n",
    "    if not non_static_indexes:\n",
    "        return shape\n",
    "\n",
    "    dyn_shape = tf.shape(tensor)\n",
    "    for index in non_static_indexes:\n",
    "        shape[index] = dyn_shape[index]\n",
    "    return shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835c39dd-9fb5-49bf-8763-14ca694a1a35",
   "metadata": {},
   "source": [
    "---\n",
    "# Cache Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da4cb5e4-2d26-4e40-a00f-3e71f470b273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_state: `Tensor`, the current state.\n",
    "# previous_state: `Tensor`, the previous state.\n",
    "# memory_length: `int`, the number of tokens to cache.\n",
    "# reuse_length: `int`, the number of tokens in the current batch to be cached\n",
    "#     and reused in the future.\n",
    "# Returns:  `Tensor`, representing the cached state with stopped gradients.\n",
    "def _cache_memory(current_state, previous_state, memory_length, reuse_length=0):\n",
    "    if memory_length is None or memory_length == 0:\n",
    "        return None\n",
    "    else:\n",
    "        if reuse_length > 0:\n",
    "            current_state = current_state[:, :reuse_length, :]\n",
    "\n",
    "        if previous_state is None:\n",
    "            new_mem = current_state[:, -memory_length:, :]\n",
    "        else:\n",
    "            new_mem = tf.concat(\n",
    "                    [previous_state, current_state], 1)[:, -memory_length:, :]\n",
    "\n",
    "    return tf.stop_gradient(new_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3126b9-a96e-4d6a-ae65-3c9579514cf1",
   "metadata": {},
   "source": [
    "---\n",
    "# MultiHead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b32faad-fd06-4448-b817-b3a1f5943aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query: Query `Tensor` of shape `[B, T, dim]`.\n",
    "# value: Value `Tensor` of shape `[B, S, dim]`.\n",
    "# content_attention_bias: Bias `Tensor` for content based attention of shape\n",
    "# `[num_heads, dim]`.\n",
    "# positional_attention_bias: Bias `Tensor` for position based attention of\n",
    "# shape `[num_heads, dim]`.\n",
    "# key: Optional key `Tensor` of shape `[B, S, dim]`. If not given, will use\n",
    "# `value` for both `key` and `value`, which is the most common case.\n",
    "# relative_position_encoding: Relative positional encoding `Tensor` of shape\n",
    "# `[B, L, dim]`.\n",
    "# state: Optional `Tensor` of shape `[B, M, E]` where M is the length of the\n",
    "# state or memory. If passed, this is also attended over as in Transformer\n",
    "# XL.\n",
    "# attention_mask: A boolean mask of shape `[B, T, S]` that prevents attention\n",
    "# to certain positions.\n",
    "class MultiHeadRelativeAttention(tf.keras.layers.MultiHeadAttention):\n",
    "    def __init__(self,\n",
    "                 kernel_initializer=\"variance_scaling\",\n",
    "                 **kwargs):\n",
    "        super().__init__(kernel_initializer=kernel_initializer,\n",
    "                                         **kwargs)\n",
    "\n",
    "    def _build_from_signature(self, query, value, key=None):\n",
    "        super(MultiHeadRelativeAttention, self)._build_from_signature(\n",
    "                query=query,\n",
    "                value=value,\n",
    "                key=key)\n",
    "        if hasattr(value, \"shape\"):\n",
    "            value_shape = tf.TensorShape(value.shape)\n",
    "        else:\n",
    "            value_shape = value\n",
    "        if key is None:\n",
    "            key_shape = value_shape\n",
    "        elif hasattr(key, \"shape\"):\n",
    "            key_shape = tf.TensorShape(key.shape)\n",
    "        else:\n",
    "            key_shape = key\n",
    "\n",
    "        common_kwargs = dict(\n",
    "                kernel_initializer=self._kernel_initializer,\n",
    "                bias_initializer=self._bias_initializer,\n",
    "                kernel_regularizer=self._kernel_regularizer,\n",
    "                bias_regularizer=self._bias_regularizer,\n",
    "                activity_regularizer=self._activity_regularizer,\n",
    "                kernel_constraint=self._kernel_constraint,\n",
    "                bias_constraint=self._bias_constraint)\n",
    "\n",
    "        with tf.init_scope():\n",
    "            einsum_equation, _, output_rank = _build_proj_equation(\n",
    "                    key_shape.rank - 1, bound_dims=1, output_dims=2)\n",
    "            self._encoding_dense = tf.keras.layers.experimental.EinsumDense(\n",
    "                    einsum_equation,\n",
    "                    output_shape=_get_output_shape(\n",
    "                        output_rank - 1,\n",
    "                        [self._num_heads, self._key_dim]),\n",
    "                        bias_axes=None,\n",
    "                        name=\"encoding\",\n",
    "                        **common_kwargs)\n",
    "\n",
    "# query: Projected query `Tensor` of shape `[B, T, N, key_dim]`.\n",
    "# key: Projected key `Tensor` of shape `[B, S + M, N, key_dim]`.\n",
    "# value: Projected value `Tensor` of shape `[B, S + M, N, key_dim]`.\n",
    "# position: Projected position `Tensor` of shape `[B, L, N, key_dim]`.\n",
    "# content_attention_bias: Trainable bias parameter added to the query head\n",
    "#     when calculating the content-based attention score.\n",
    "# positional_attention_bias: Trainable bias parameter added to the query\n",
    "#     head when calculating the position-based attention score.\n",
    "# attention_mask: (default None) Optional mask that is added to attention\n",
    "#     logits. If state is not None, the mask source sequence dimension should\n",
    "#     extend M.\n",
    "# Returns:\n",
    "# attention_output: Multi-headed output of attention computation of shape\n",
    "#     `[B, S, N, key_dim]`.\n",
    "    def compute_attention(\n",
    "        self,\n",
    "        query,\n",
    "        key,\n",
    "        value,\n",
    "        position,\n",
    "        content_attention_bias,\n",
    "        positional_attention_bias,\n",
    "        attention_mask=None\n",
    "    ):\n",
    "        #AC\n",
    "        content_attention = tf.einsum(self._dot_product_equation, key, query + content_attention_bias)\n",
    "        \n",
    "        positional_attention = tf.einsum(self._dot_product_equation, position, query + positional_attention_bias)\n",
    "        \n",
    "        #BD\n",
    "        positional_attention = _rel_shift(positional_attention, klen=tf.shape(content_attention)[3])\n",
    "        \n",
    "        attention_sum = content_attention + positional_attention\n",
    "\n",
    "        attention_scores = tf.multiply(attention_sum, 1.0 / math.sqrt(float(self._key_dim)))\n",
    "\n",
    "        attention_scores = self._masked_softmax(attention_scores, attention_mask)\n",
    "\n",
    "        attention_output = self._dropout_layer(attention_scores)\n",
    "\n",
    "        attention_output = tf.einsum(self._combine_equation,\n",
    "                                                                 attention_output,\n",
    "                                                                 value)\n",
    "        return attention_output\n",
    "\n",
    "# * Number of heads (H): the number of attention heads.\n",
    "# * Value size (V): the size of each value embedding per head.\n",
    "# * Key size (K): the size of each key embedding per head. Equally, the size\n",
    "#     of each query embedding per head. Typically K <= V.\n",
    "# * Batch dimensions (B).\n",
    "# * Query (target) attention axes shape (T).\n",
    "# * Value (source) attention axes shape (S), the rank must match the target.\n",
    "# * Encoding length (L): The relative positional encoding length.\n",
    "# query: attention input.\n",
    "# value: attention input.\n",
    "# content_attention_bias: A trainable bias parameter added to the query head\n",
    "#     when calculating the content-based attention score.\n",
    "# positional_attention_bias: A trainable bias parameter added to the query\n",
    "#     head when calculating the position-based attention score.\n",
    "# key: attention input.\n",
    "# relative_position_encoding: relative positional encoding for key and\n",
    "#     value.\n",
    "# state: (default None) optional state. If passed, this is also attended\n",
    "#     over as in TransformerXL.\n",
    "# attention_mask: (default None) Optional mask that is added to attention\n",
    "#     logits. If state is not None, the mask source sequence dimension should\n",
    "#     extend M.\n",
    "# Returns:\n",
    "# attention_output: The result of the computation, of shape [B, T, E],\n",
    "#     where `T` is for target sequence shapes and `E` is the query input last\n",
    "#     dimension if `output_shape` is `None`. Otherwise, the multi-head outputs\n",
    "#     are projected to the shape specified by `output_shape`.\n",
    "    def call(self,\n",
    "             query,\n",
    "             value,\n",
    "             content_attention_bias,\n",
    "             positional_attention_bias,\n",
    "             key=None,\n",
    "             relative_position_encoding=None,\n",
    "             state=None,\n",
    "             attention_mask=None):\n",
    "        \n",
    "        if not self._built_from_signature:\n",
    "            self._build_from_signature(query, value, key=key)\n",
    "        if key is None:\n",
    "            key = value\n",
    "        if state is not None and state.shape.ndims > 1:\n",
    "            value = tf.concat([state, value], 1)\n",
    "            key = tf.concat([state, key], 1)\n",
    "\n",
    "        # `query` = [B, T, N ,H]\n",
    "        query = self._query_dense(query)\n",
    "\n",
    "        # `key` = [B, S + M, N, H]\n",
    "        key = self._key_dense(key)\n",
    "\n",
    "        # `value` = [B, S + M, N, H]\n",
    "        value = self._value_dense(value)\n",
    "\n",
    "        # `position` = [B, L, N, H]\n",
    "        position = self._encoding_dense(relative_position_encoding)\n",
    "\n",
    "        attention_output = self.compute_attention(\n",
    "                query=query,\n",
    "                key=key,\n",
    "                value=value,\n",
    "                position=position,\n",
    "                content_attention_bias=content_attention_bias,\n",
    "                positional_attention_bias=positional_attention_bias,\n",
    "                attention_mask=attention_mask)\n",
    "\n",
    "        # `attention_output` = [B, S, N, H]\n",
    "        attention_output = self._output_dense(attention_output)\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95abdafa-2c98-4c04-b9e8-aae23e53265e",
   "metadata": {},
   "source": [
    "---\n",
    "# Relative Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc8c99a8-3755-48f9-9704-c1dbd04e0ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rel_shift(x, klen=-1):    \n",
    "    x = tf.transpose(x, perm=[2, 3, 0, 1])\n",
    "    x = tf.pad(x, [[0, 0], [1, 0], [0, 0], [0, 0]])\n",
    "    x_size = tf.shape(x)\n",
    "    x = tf.reshape(x, [x_size[1], x_size[0], x_size[2], x_size[3]])\n",
    "    x = tf.slice(x, [1, 0, 0, 0], [-1, -1, -1, -1])\n",
    "    x = tf.reshape(x, [x_size[0], x_size[1] - 1, x_size[2], x_size[3]])\n",
    "    x = tf.transpose(x, perm=[2, 3, 0, 1])\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a026fd-2cbe-45b7-88f7-a3ffd3548806",
   "metadata": {},
   "source": [
    "---\n",
    "# Build Einsum Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5102422c-82af-49d4-a8d5-5081262b21e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_CHR_IDX = string.ascii_lowercase\n",
    "\n",
    "# Builds an einsum equation for projections inside multi-head attention\n",
    "def _build_proj_equation(free_dims, bound_dims, output_dims):\n",
    "    input_str = \"\"\n",
    "    kernel_str = \"\"\n",
    "    output_str = \"\"\n",
    "    bias_axes = \"\"\n",
    "    letter_offset = 0\n",
    "    for i in range(free_dims):\n",
    "        char = _CHR_IDX[i + letter_offset]\n",
    "        input_str += char\n",
    "        output_str += char\n",
    "\n",
    "    letter_offset += free_dims\n",
    "    for i in range(bound_dims):\n",
    "        char = _CHR_IDX[i + letter_offset]\n",
    "        input_str += char\n",
    "        kernel_str += char\n",
    "\n",
    "    letter_offset += bound_dims\n",
    "    for i in range(output_dims):\n",
    "        char = _CHR_IDX[i + letter_offset]\n",
    "        kernel_str += char\n",
    "        output_str += char\n",
    "        bias_axes += char\n",
    "    equation = \"%s,%s->%s\" % (input_str, kernel_str, output_str)\n",
    "\n",
    "    return equation, bias_axes, len(output_str)\n",
    "\n",
    "\n",
    "def _get_output_shape(output_rank, known_last_dims):\n",
    "    return [None] * (output_rank - len(known_last_dims)) + list(known_last_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5343f3e3-8536-46d5-9d1b-28f8c3f7908b",
   "metadata": {},
   "source": [
    "---\n",
    "# Relative Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5f5eeac-4c07-4796-8281-f5c039fe7c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_size: Size of the hidden layer.\n",
    "# min_timescale: Minimum scale that will be applied at each position\n",
    "# max_timescale: Maximum scale that will be applied at each position.\n",
    "class RelativePositionEmbedding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 hidden_size: int,\n",
    "                 min_timescale: float = 1.0,\n",
    "                 max_timescale: float = 1.0e4,\n",
    "                 **kwargs):\n",
    "        \n",
    "        if \"dtype\" not in kwargs:\n",
    "            kwargs[\"dtype\"] = \"float32\"\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self._hidden_size = hidden_size\n",
    "        self._min_timescale = min_timescale\n",
    "        self._max_timescale = max_timescale\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "                \"hidden_size\": self._hidden_size,\n",
    "                \"min_timescale\": self._min_timescale,\n",
    "                \"max_timescale\": self._max_timescale,\n",
    "        }\n",
    "        base_config = super(RelativePositionEmbedding, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "# inputs: An tensor whose second dimension will be used as `length`. If\n",
    "# `None`, the other `length` argument must be specified.\n",
    "# length: An optional integer specifying the number of positions. If both\n",
    "# `inputs` and `length` are spcified, `length` must be equal to the second\n",
    "# dimension of `inputs`.\n",
    "# Returns:\n",
    "# A tensor in shape of `(length, hidden_size)`.\n",
    "    def call(self, inputs, length=None):\n",
    "        if inputs is None and length is None:\n",
    "            raise ValueError(\"If inputs is None, `length` must be set in \"\n",
    "                                             \"RelativePositionEmbedding().\")\n",
    "        if inputs is not None:\n",
    "            input_shape = get_shape_list(inputs)\n",
    "            if length is not None and length != input_shape[1]:\n",
    "                raise ValueError(\n",
    "                        \"If inputs is not None, `length` must equal to input_shape[1].\")\n",
    "            length = input_shape[1]\n",
    "        position = tf.cast(tf.range(length), tf.float32)\n",
    "        num_timescales = self._hidden_size // 2\n",
    "        min_timescale, max_timescale = self._min_timescale, self._max_timescale\n",
    "        log_timescale_increment = (\n",
    "                math.log(float(max_timescale) / float(min_timescale)) /\n",
    "                (tf.cast(num_timescales, tf.float32) - 1))\n",
    "        inv_timescales = min_timescale * tf.exp(\n",
    "                tf.cast(tf.range(num_timescales), tf.float32) *\n",
    "                -log_timescale_increment)\n",
    "        scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(\n",
    "                inv_timescales, 0)\n",
    "        position_embeddings = tf.concat(\n",
    "                [tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n",
    "        position_embeddings = tf.expand_dims(position_embeddings, axis=0)\n",
    "        return tf.tile(position_embeddings, (tf.shape(inputs)[0], 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdbdfc2d-f593-4466-a981-c8e7b3b06e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size: The size of the token vocabulary.\n",
    "# hidden_size: The size of the transformer hidden layers.\n",
    "# num_attention_heads: The number of attention heads.\n",
    "# head_size: The dimension size of each attention head.\n",
    "# inner_size: The inner size for the transformer layers.\n",
    "# dropout_rate: Dropout rate for the output of this layer.\n",
    "# attention_dropout_rate: Dropout rate on attention probabilities.\n",
    "# norm_epsilon: Epsilon value to initialize normalization layers.\n",
    "# inner_activation: The activation to use for the inner\n",
    "# FFN layers.\n",
    "# kernel_initializer: Initializer for dense layer kernels.\n",
    "# inner_dropout: Dropout probability for the inner dropout\n",
    "# layer.\n",
    "class TransformerXLBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 hidden_size,\n",
    "                 num_attention_heads,\n",
    "                 head_size,\n",
    "                 inner_size,\n",
    "                 dropout_rate,\n",
    "                 attention_dropout_rate,\n",
    "                 norm_epsilon=1e-12,\n",
    "                 inner_activation=\"relu\",\n",
    "                 kernel_initializer=\"variance_scaling\",\n",
    "                 inner_dropout=0.0,\n",
    "                 **kwargs):\n",
    "        \"\"\"Initializes TransformerXLBlock layer.\"\"\"\n",
    "\n",
    "        super(TransformerXLBlock, self).__init__(**kwargs)\n",
    "        self._vocab_size = vocab_size\n",
    "        self._num_heads = num_attention_heads\n",
    "        self._head_size = head_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._inner_size = inner_size\n",
    "        self._dropout_rate = dropout_rate\n",
    "        self._attention_dropout_rate = attention_dropout_rate\n",
    "        self._inner_activation = inner_activation\n",
    "        self._norm_epsilon = norm_epsilon\n",
    "        self._kernel_initializer = kernel_initializer\n",
    "        self._inner_dropout = inner_dropout\n",
    "        self._attention_layer_type = MultiHeadRelativeAttention\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_tensor = input_shape[0] if len(input_shape) == 2 else input_shape\n",
    "        input_tensor_shape = tf.TensorShape(input_tensor)\n",
    "        if len(input_tensor_shape.as_list()) != 3:\n",
    "            raise ValueError(\"TransformerLayer expects a three-dimensional input of \"\n",
    "                                             \"shape [batch, sequence, width].\")\n",
    "        batch_size, sequence_length, hidden_size = input_tensor_shape\n",
    "\n",
    "        if len(input_shape) == 2:\n",
    "            mask_tensor_shape = tf.TensorShape(input_shape[1])\n",
    "            expected_mask_tensor_shape = tf.TensorShape(\n",
    "                    [batch_size, sequence_length, sequence_length])\n",
    "            if not expected_mask_tensor_shape.is_compatible_with(mask_tensor_shape):\n",
    "                raise ValueError(\"When passing a mask tensor to TransformerXLBlock, \"\n",
    "                                                 \"the mask tensor must be of shape [batch, \"\n",
    "                                                 \"sequence_length, sequence_length] (here %s). Got a \"\n",
    "                                                 \"mask tensor of shape %s.\" %\n",
    "                                                 (expected_mask_tensor_shape, mask_tensor_shape))\n",
    "        if hidden_size % self._num_heads != 0:\n",
    "            raise ValueError(\n",
    "                    \"The input size (%d) is not a multiple of the number of attention \"\n",
    "                    \"heads (%d)\" % (hidden_size, self._num_heads))\n",
    "        self._attention_layer = self._attention_layer_type(\n",
    "                num_heads=self._num_heads,\n",
    "                key_dim=self._head_size,\n",
    "                value_dim=self._head_size,\n",
    "                dropout=self._attention_dropout_rate,\n",
    "                use_bias=False,\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer),\n",
    "                name=\"rel_attn\")\n",
    "        self._attention_dropout = tf.keras.layers.Dropout(\n",
    "                rate=self._attention_dropout_rate)\n",
    "        self._attention_layer_norm = tf.keras.layers.LayerNormalization(\n",
    "                name=\"self_attention_layer_norm\",\n",
    "                axis=-1,\n",
    "                epsilon=self._norm_epsilon,\n",
    "                dtype=tf.float32)\n",
    "        self._inner_dense = tf.keras.layers.experimental.EinsumDense(\n",
    "                \"abc,cd->abd\",\n",
    "                output_shape=(None, self._inner_size),\n",
    "                bias_axes=\"d\",\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer),\n",
    "                name=\"inner\")\n",
    "\n",
    "        self._inner_activation_layer = tf.keras.layers.Activation(\n",
    "                self._inner_activation)\n",
    "        self._inner_dropout_layer = tf.keras.layers.Dropout(\n",
    "                rate=self._inner_dropout)\n",
    "        self._output_dense = tf.keras.layers.experimental.EinsumDense(\n",
    "                \"abc,cd->abd\",\n",
    "                output_shape=(None, hidden_size),\n",
    "                bias_axes=\"d\",\n",
    "                name=\"output\",\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer))\n",
    "        self._output_dropout = tf.keras.layers.Dropout(rate=self._dropout_rate)\n",
    "        self._output_layer_norm = tf.keras.layers.LayerNormalization(\n",
    "                name=\"output_layer_norm\",\n",
    "                axis=-1,\n",
    "                epsilon=self._norm_epsilon)\n",
    "\n",
    "        super(TransformerXLBlock, self).build(input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "                \"vocab_size\":\n",
    "                        self._vocab_size,\n",
    "                \"hidden_size\":\n",
    "                        self._hidden_size,\n",
    "                \"num_attention_heads\":\n",
    "                        self._num_heads,\n",
    "                \"head_size\":\n",
    "                        self._head_size,\n",
    "                \"inner_size\":\n",
    "                        self._inner_size,\n",
    "                \"dropout_rate\":\n",
    "                        self._dropout_rate,\n",
    "                \"attention_dropout_rate\":\n",
    "                        self._attention_dropout_rate,\n",
    "                \"norm_epsilon\":\n",
    "                        self._norm_epsilon,\n",
    "                \"inner_activation\":\n",
    "                        self._inner_activation,\n",
    "                \"kernel_initializer\":\n",
    "                        self._kernel_initializer,\n",
    "                \"inner_dropout\":\n",
    "                        self._inner_dropout,\n",
    "        }\n",
    "        base_config = super(TransformerXLBlock, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    \n",
    "    # content_stream: `Tensor`, the input content stream. This is the standard\n",
    "    # input to Transformer XL and is commonly referred to as `h` in XLNet.\n",
    "    # content_attention_bias: Bias `Tensor` for content based attention of shape\n",
    "    # `[num_heads, dim]`.\n",
    "    # positional_attention_bias: Bias `Tensor` for position based attention of\n",
    "    # shape `[num_heads, dim]`.\n",
    "    # relative_position_encoding: Relative positional encoding `Tensor` of shape\n",
    "    # `[B, L, dim]`.\n",
    "    # state: Optional `Tensor` of shape `[B, M, E]`, where M is the length of\n",
    "    # the state or memory. If passed, this is also attended over as in\n",
    "    # Transformer XL.\n",
    "    # content_attention_mask: Optional `Tensor` representing the mask that is\n",
    "    # added to content attention logits. If state is not None, the mask source\n",
    "    # sequence dimension should extend M.\n",
    "    # query_attention_mask: Optional `Tensor` representing the mask that is\n",
    "    # added to query attention logits. If state is not None, the mask source\n",
    "    # sequence dimension should extend M.\n",
    "    # target_mapping: Optional `Tensor` representing the target mapping when\n",
    "    # calculating query attention.\n",
    "    # Returns:\n",
    "    # A `dict` object, containing the key value pairs for `content_attention`\n",
    "    def call(self,\n",
    "             content_stream,\n",
    "             content_attention_bias,\n",
    "             positional_attention_bias,\n",
    "             relative_position_encoding=None,\n",
    "             state=None,\n",
    "             content_attention_mask=None,\n",
    "             query_attention_mask=None,\n",
    "             target_mapping=None):\n",
    "\n",
    "        attention_kwargs = dict(\n",
    "                query=content_stream,\n",
    "                value=content_stream,\n",
    "                key=content_stream,\n",
    "                attention_mask=content_attention_mask)\n",
    "\n",
    "        common_attention_kwargs = dict(\n",
    "                content_attention_bias=content_attention_bias,\n",
    "                relative_position_encoding=relative_position_encoding,\n",
    "                positional_attention_bias=positional_attention_bias,\n",
    "                state=state)\n",
    "\n",
    "        attention_kwargs.update(common_attention_kwargs)\n",
    "        attention_output = self._attention_layer(**attention_kwargs)\n",
    "\n",
    "        attention_stream = attention_output\n",
    "        input_stream = content_stream\n",
    "        attention_key = \"content_attention\"\n",
    "        attention_output = {}\n",
    "        \n",
    "        attention_stream = self._attention_dropout(attention_stream)\n",
    "        attention_stream = self._attention_layer_norm(\n",
    "                attention_stream + input_stream)\n",
    "        inner_output = self._inner_dense(attention_stream)\n",
    "        inner_output = self._inner_activation_layer(\n",
    "                inner_output)\n",
    "        inner_output = self._inner_dropout_layer(\n",
    "                inner_output)\n",
    "        layer_output = self._output_dense(inner_output)\n",
    "        layer_output = self._output_dropout(layer_output)\n",
    "        layer_output = self._output_layer_norm(layer_output + attention_stream)\n",
    "        attention_output[attention_key] = layer_output\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c32bb5-4520-4802-a8f0-5f7849905a83",
   "metadata": {},
   "source": [
    "---\n",
    "# XL Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7245b7dd-a3b2-464a-9f5b-7efd4a9d0ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size: The size of the token vocabulary.\n",
    "# hidden_size: The size of the transformer hidden layers.\n",
    "# num_attention_heads: The number of attention heads.\n",
    "# head_size: The dimension size of each attention head.\n",
    "# inner_size: The inner size for the transformer layers.\n",
    "# dropout_rate: Dropout rate for the output of this layer.\n",
    "# attention_dropout_rate: Dropout rate on attention probabilities.\n",
    "# norm_epsilon: Epsilon value to initialize normalization layers.\n",
    "# inner_activation: The activation to use for the inner\n",
    "#     FFN layers.\n",
    "# kernel_initializer: Initializer for dense layer kernels.\n",
    "# inner_dropout: Dropout probability for the inner dropout\n",
    "#     layer.\n",
    "class TransformerXLBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 hidden_size,\n",
    "                 num_attention_heads,\n",
    "                 head_size,\n",
    "                 inner_size,\n",
    "                 dropout_rate,\n",
    "                 attention_dropout_rate,\n",
    "                 norm_epsilon=1e-12,\n",
    "                 inner_activation=\"relu\",\n",
    "                 kernel_initializer=\"variance_scaling\",\n",
    "                 inner_dropout=0.0,\n",
    "                 **kwargs):\n",
    "        \"\"\"Initializes TransformerXLBlock layer.\"\"\"\n",
    "\n",
    "        super(TransformerXLBlock, self).__init__(**kwargs)\n",
    "        self._vocab_size = vocab_size\n",
    "        self._num_heads = num_attention_heads\n",
    "        self._head_size = head_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._inner_size = inner_size\n",
    "        self._dropout_rate = dropout_rate\n",
    "        self._attention_dropout_rate = attention_dropout_rate\n",
    "        self._inner_activation = inner_activation\n",
    "        self._norm_epsilon = norm_epsilon\n",
    "        self._kernel_initializer = kernel_initializer\n",
    "        self._inner_dropout = inner_dropout\n",
    "        self._attention_layer_type = MultiHeadRelativeAttention\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_tensor = input_shape[0] if len(input_shape) == 2 else input_shape\n",
    "        input_tensor_shape = tf.TensorShape(input_tensor)\n",
    "        if len(input_tensor_shape.as_list()) != 3:\n",
    "            raise ValueError(\"TransformerLayer expects a three-dimensional input of \"\n",
    "                                             \"shape [batch, sequence, width].\")\n",
    "        batch_size, sequence_length, hidden_size = input_tensor_shape\n",
    "\n",
    "        if len(input_shape) == 2:\n",
    "            mask_tensor_shape = tf.TensorShape(input_shape[1])\n",
    "            expected_mask_tensor_shape = tf.TensorShape(\n",
    "                    [batch_size, sequence_length, sequence_length])\n",
    "            if not expected_mask_tensor_shape.is_compatible_with(mask_tensor_shape):\n",
    "                raise ValueError(\"When passing a mask tensor to TransformerXLBlock, \"\n",
    "                                                 \"the mask tensor must be of shape [batch, \"\n",
    "                                                 \"sequence_length, sequence_length] (here %s). Got a \"\n",
    "                                                 \"mask tensor of shape %s.\" %\n",
    "                                                 (expected_mask_tensor_shape, mask_tensor_shape))\n",
    "        if hidden_size % self._num_heads != 0:\n",
    "            raise ValueError(\n",
    "                    \"The input size (%d) is not a multiple of the number of attention \"\n",
    "                    \"heads (%d)\" % (hidden_size, self._num_heads))\n",
    "        self._attention_layer = self._attention_layer_type(\n",
    "                num_heads=self._num_heads,\n",
    "                key_dim=self._head_size,\n",
    "                value_dim=self._head_size,\n",
    "                dropout=self._attention_dropout_rate,\n",
    "                use_bias=False,\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer),\n",
    "                name=\"rel_attn\")\n",
    "        self._attention_dropout = tf.keras.layers.Dropout(\n",
    "                rate=self._attention_dropout_rate)\n",
    "        self._attention_layer_norm = tf.keras.layers.LayerNormalization(\n",
    "                name=\"self_attention_layer_norm\",\n",
    "                axis=-1,\n",
    "                epsilon=self._norm_epsilon,\n",
    "                dtype=tf.float32)\n",
    "        self._inner_dense = tf.keras.layers.experimental.EinsumDense(\n",
    "                \"abc,cd->abd\",\n",
    "                output_shape=(None, self._inner_size),\n",
    "                bias_axes=\"d\",\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer),\n",
    "                name=\"inner\")\n",
    "\n",
    "        self._inner_activation_layer = tf.keras.layers.Activation(\n",
    "                self._inner_activation)\n",
    "        self._inner_dropout_layer = tf.keras.layers.Dropout(\n",
    "                rate=self._inner_dropout)\n",
    "        self._output_dense = tf.keras.layers.experimental.EinsumDense(\n",
    "                \"abc,cd->abd\",\n",
    "                output_shape=(None, hidden_size),\n",
    "                bias_axes=\"d\",\n",
    "                name=\"output\",\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer))\n",
    "        self._output_dropout = tf.keras.layers.Dropout(rate=self._dropout_rate)\n",
    "        self._output_layer_norm = tf.keras.layers.LayerNormalization(\n",
    "                name=\"output_layer_norm\",\n",
    "                axis=-1,\n",
    "                epsilon=self._norm_epsilon)\n",
    "\n",
    "        super(TransformerXLBlock, self).build(input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "                \"vocab_size\":\n",
    "                        self._vocab_size,\n",
    "                \"hidden_size\":\n",
    "                        self._hidden_size,\n",
    "                \"num_attention_heads\":\n",
    "                        self._num_heads,\n",
    "                \"head_size\":\n",
    "                        self._head_size,\n",
    "                \"inner_size\":\n",
    "                        self._inner_size,\n",
    "                \"dropout_rate\":\n",
    "                        self._dropout_rate,\n",
    "                \"attention_dropout_rate\":\n",
    "                        self._attention_dropout_rate,\n",
    "                \"norm_epsilon\":\n",
    "                        self._norm_epsilon,\n",
    "                \"inner_activation\":\n",
    "                        self._inner_activation,\n",
    "                \"kernel_initializer\":\n",
    "                        self._kernel_initializer,\n",
    "                \"inner_dropout\":\n",
    "                        self._inner_dropout,\n",
    "        }\n",
    "        base_config = super(TransformerXLBlock, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "# content_stream: `Tensor`, the input content stream. This is the standard\n",
    "# input to Transformer XL and is commonly referred to as `h` in XLNet.\n",
    "# content_attention_bias: Bias `Tensor` for content based attention of shape\n",
    "# `[num_heads, dim]`.\n",
    "# positional_attention_bias: Bias `Tensor` for position based attention of\n",
    "# shape `[num_heads, dim]`.\n",
    "# relative_position_encoding: Relative positional encoding `Tensor` of shape\n",
    "# `[B, L, dim]`.\n",
    "# state: Optional `Tensor` of shape `[B, M, E]`, where M is the length of\n",
    "# the state or memory. If passed, this is also attended over as in\n",
    "# Transformer XL.\n",
    "# content_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to content attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# query_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to query attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# target_mapping: Optional `Tensor` representing the target mapping when\n",
    "# calculating query attention.\n",
    "# Returns:\n",
    "# A `dict` object, containing the key value pairs for `content_attention`\n",
    "    def call(self,\n",
    "             content_stream,\n",
    "             content_attention_bias,\n",
    "             positional_attention_bias,\n",
    "             relative_position_encoding=None,\n",
    "             state=None,\n",
    "             content_attention_mask=None,\n",
    "             query_attention_mask=None,\n",
    "             target_mapping=None):\n",
    "        \n",
    "        attention_kwargs = dict(\n",
    "                query=content_stream,\n",
    "                value=content_stream,\n",
    "                key=content_stream,\n",
    "                attention_mask=content_attention_mask)\n",
    "\n",
    "        common_attention_kwargs = dict(\n",
    "                content_attention_bias=content_attention_bias,\n",
    "                relative_position_encoding=relative_position_encoding,\n",
    "                positional_attention_bias=positional_attention_bias,\n",
    "                state=state)\n",
    "\n",
    "        attention_kwargs.update(common_attention_kwargs)\n",
    "        attention_output = self._attention_layer(**attention_kwargs)\n",
    "\n",
    "        attention_stream = attention_output\n",
    "        input_stream = content_stream\n",
    "        attention_key = \"content_attention\"\n",
    "        attention_output = {}\n",
    "        \n",
    "        attention_stream = self._attention_dropout(attention_stream)\n",
    "        attention_stream = self._attention_layer_norm(attention_stream + input_stream)\n",
    "        inner_output = self._inner_dense(attention_stream)\n",
    "        inner_output = self._inner_activation_layer(\n",
    "                inner_output)\n",
    "        inner_output = self._inner_dropout_layer(\n",
    "                inner_output)\n",
    "        layer_output = self._output_dense(inner_output)\n",
    "        layer_output = self._output_dropout(layer_output)\n",
    "        layer_output = self._output_layer_norm(layer_output + attention_stream)\n",
    "        attention_output[attention_key] = layer_output\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8749ec30-1281-4f9d-8baa-54b890fb7949",
   "metadata": {},
   "source": [
    "---\n",
    "# XL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "019ca25d-e899-4e17-8f41-dfdc3d9bdee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_layers: The number of layers.\n",
    "# hidden_size: The hidden size.\n",
    "# num_attention_heads: The number of attention heads.\n",
    "# head_size: The dimension size of each attention head.\n",
    "# inner_size: The hidden size in feed-forward layers.\n",
    "# dropout_rate: Dropout rate used in each Transformer XL block.\n",
    "# attention_dropout_rate: Dropout rate on attention probabilities.\n",
    "# initializer: The initializer to use for attention biases.\n",
    "# tie_attention_biases: Whether or not to tie biases together. If `True`, then\n",
    "# each Transformer XL block shares the same trainable attention bias. If\n",
    "# `False`, then each block has its own attention bias. This is usually set\n",
    "# to `True`.\n",
    "# memory_length: The number of tokens to cache.\n",
    "# reuse_length: The number of tokens in the current batch to be cached\n",
    "# and reused in the future.\n",
    "# inner_activation: The activation to use in the inner layers\n",
    "# for Transformer XL blocks. Typically \"relu\" or \"gelu\".\n",
    "class TransformerXL(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 num_layers,\n",
    "                 hidden_size,\n",
    "                 maxlen,\n",
    "                 embed_dim,\n",
    "                 num_attention_heads,\n",
    "                 head_size,\n",
    "                 inner_size,\n",
    "                 dropout_rate,\n",
    "                 attention_dropout_rate,\n",
    "                 initializer,\n",
    "                 tie_attention_biases=True,\n",
    "                 memory_length=None,\n",
    "                 reuse_length=None,\n",
    "                 inner_activation=\"relu\",\n",
    "                 **kwargs):\n",
    "        super(TransformerXL, self).__init__(**kwargs)\n",
    "\n",
    "        self._vocab_size = vocab_size\n",
    "        self._initializer = initializer\n",
    "        self._num_layers = num_layers\n",
    "        self._hidden_size = hidden_size\n",
    "        self._num_attention_heads = num_attention_heads\n",
    "        self._head_size = head_size\n",
    "        self._inner_size = inner_size\n",
    "        self._inner_activation = inner_activation\n",
    "        self._dropout_rate = dropout_rate\n",
    "        self._attention_dropout_rate = attention_dropout_rate\n",
    "        self._tie_attention_biases = tie_attention_biases\n",
    "\n",
    "        self._memory_length = memory_length\n",
    "        self._reuse_length = reuse_length\n",
    "\n",
    "        if self._tie_attention_biases:\n",
    "            attention_bias_shape = [self._num_attention_heads, self._head_size]\n",
    "        else:\n",
    "            attention_bias_shape = [self._num_layers, self._num_attention_heads, self._head_size]\n",
    "\n",
    "        self.content_attention_bias = self.add_weight(\n",
    "                \"content_attention_bias\",\n",
    "                shape=attention_bias_shape,\n",
    "                dtype=tf.float32,\n",
    "                initializer=clone_initializer(self._initializer))\n",
    "        self.positional_attention_bias = self.add_weight(\n",
    "                \"positional_attention_bias\",\n",
    "                shape=attention_bias_shape,\n",
    "                dtype=tf.float32,\n",
    "                initializer=clone_initializer(self._initializer))\n",
    "\n",
    "        self.transformer_xl_layers = []\n",
    "        for i in range(self._num_layers):\n",
    "            self.transformer_xl_layers.append(\n",
    "                    TransformerXLBlock(\n",
    "                            vocab_size=self._vocab_size,\n",
    "                            hidden_size=self._head_size * self._num_attention_heads,\n",
    "                            num_attention_heads=self._num_attention_heads,\n",
    "                            head_size=self._head_size,\n",
    "                            inner_size=self._inner_size,\n",
    "                            dropout_rate=self._dropout_rate,\n",
    "                            attention_dropout_rate=self._attention_dropout_rate,\n",
    "                            norm_epsilon=1e-12,\n",
    "                            inner_activation=self._inner_activation,\n",
    "                            kernel_initializer=\"variance_scaling\",\n",
    "                            name=\"layer_%d\" % i))\n",
    "\n",
    "        self.output_dropout = tf.keras.layers.Dropout(rate=self._dropout_rate)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "                \"vocab_size\":\n",
    "                        self._vocab_size,\n",
    "                \"num_layers\":\n",
    "                        self._num_layers,\n",
    "                \"hidden_size\":\n",
    "                        self._hidden_size,\n",
    "                \"num_attention_heads\":\n",
    "                        self._num_attention_heads,\n",
    "                \"head_size\":\n",
    "                        self._head_size,\n",
    "                \"inner_size\":\n",
    "                        self._inner_size,\n",
    "                \"dropout_rate\":\n",
    "                        self._dropout_rate,\n",
    "                \"attention_dropout_rate\":\n",
    "                        self._attention_dropout_rate,\n",
    "                \"initializer\":\n",
    "                        self._initializer,\n",
    "                \"tie_attention_biases\":\n",
    "                        self._tie_attention_biases,\n",
    "                \"memory_length\":\n",
    "                        self._memory_length,\n",
    "                \"reuse_length\":\n",
    "                        self._reuse_length,\n",
    "                \"inner_activation\":\n",
    "                        self._inner_activation,\n",
    "        }\n",
    "        base_config = super(TransformerXL, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "# content_stream: `Tensor`, the input content stream. This is the standard\n",
    "# input to Transformer XL and is commonly referred to as `h` in XLNet.\n",
    "# relative_position_encoding: Relative positional encoding `Tensor` of shape\n",
    "# `[B, L, dim]`.\n",
    "# state: Optional `Tensor` of shape `[B, M, E]`, where M is the length of\n",
    "# the state or memory. If passed, this is also attended over as in\n",
    "# Transformer XL.\n",
    "# content_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to content attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# query_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to query attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# target_mapping: Optional `Tensor` representing the target mapping when\n",
    "# calculating query attention.\n",
    "# Returns:\n",
    "# A tuple consisting of the attention output and the list of cached memory\n",
    "# states.\n",
    "# The attention output is `content_attention`\n",
    "    def call(self,\n",
    "             content_stream,\n",
    "             relative_position_encoding,\n",
    "             state=None,\n",
    "             content_attention_mask=None,\n",
    "             query_attention_mask=None,\n",
    "             target_mapping=None):\n",
    "        \n",
    "        new_mems = []\n",
    "\n",
    "        if state is None:\n",
    "            state = [None] * self._num_layers\n",
    "        for i in range(self._num_layers):\n",
    "            # cache new mems\n",
    "            new_mems.append(\n",
    "                    _cache_memory(content_stream, state[i],\n",
    "                                                self._memory_length, self._reuse_length))\n",
    "            \n",
    "            if self._tie_attention_biases:\n",
    "                content_attention_bias = self.content_attention_bias\n",
    "            else:\n",
    "                content_attention_bias = self.content_attention_bias[i]\n",
    "                \n",
    "            if self._tie_attention_biases:\n",
    "                positional_attention_bias = self.positional_attention_bias\n",
    "            else:\n",
    "                positional_attention_bias = self.positional_attention_bias[i]\n",
    "\n",
    "            transformer_xl_layer = self.transformer_xl_layers[i]\n",
    "            \n",
    "            transformer_xl_output = transformer_xl_layer(\n",
    "                    content_stream=content_stream,\n",
    "                    content_attention_bias=content_attention_bias,\n",
    "                    positional_attention_bias=positional_attention_bias,\n",
    "                    relative_position_encoding=relative_position_encoding,\n",
    "                    state=state[i],\n",
    "                    content_attention_mask=content_attention_mask,\n",
    "                    query_attention_mask=query_attention_mask,\n",
    "                    target_mapping=target_mapping)\n",
    "            content_stream = transformer_xl_output[\"content_attention\"]\n",
    "\n",
    "        output_stream = content_stream\n",
    "        return output_stream, new_mems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "855d43dd-e182-4c2a-961a-ff9781141f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XlModel(keras.Model):\n",
    "    def __init__(self,  embed_dim, encoder_shape, target_shape, vocab_size, num_layers, hidden_size, num_attention_heads, maxlen, memory_length, reuse_length, head_size, inner_size, dropout_rate, attention_dropout_rate, initializer):\n",
    "        super(XlModel, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_attention_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.maxlen = maxlen\n",
    "        self.memory_length = memory_length\n",
    "        \n",
    "        self.embedding_layer = MaskedTokenAndPositionEmbedding(maxlen=maxlen, vocab_size=vocab_size, embed_dim=embed_dim)\n",
    "        \n",
    "        self.rel_embeddings = RelativePositionEmbedding(embed_dim)\n",
    "        \n",
    "        self.decoder_embeddings = MaskedTokenAndPositionEmbedding(maxlen=pre_y_train.shape[1], vocab_size=vocab_size, embed_dim=self.embed_dim)\n",
    "\n",
    "        self.transformer_xl = TransformerXL(\n",
    "                vocab_size=vocab_size,\n",
    "                num_layers=num_layers,\n",
    "                hidden_size=hidden_size,\n",
    "                num_attention_heads=num_attention_heads,\n",
    "                maxlen=maxlen,\n",
    "                embed_dim=embed_dim,\n",
    "                memory_length=memory_length,\n",
    "                reuse_length=reuse_length,\n",
    "                head_size=head_size,\n",
    "                inner_size=inner_size,\n",
    "                dropout_rate=dropout_rate,\n",
    "                attention_dropout_rate=attention_dropout_rate,\n",
    "                initializer=initializer, \n",
    "            )\n",
    "        \n",
    "        self.decoder = self.create_decoder(encoder_shape, target_shape)\n",
    "        \n",
    "    def create_decoder(self, encoder_shape, target_shape):\n",
    "        y = x = keras.layers.Input(shape=target_shape)\n",
    "        y = self.decoder_embeddings(y)\n",
    "        \n",
    "        c = keras.layers.Input(shape=encoder_shape)\n",
    "    \n",
    "        for i in range(4):\n",
    "            y = MaskedTransformerBlock(embed_dim=self.embed_dim, num_heads=self.num_heads, ff_dim=self.hidden_size)([y,c])\n",
    "\n",
    "        y = keras.layers.Dense(len(i_to_c_pandp), name=\"hi\")(y)    \n",
    "        decoder = keras.Model([x, c],y,name=\"Decoder\")\n",
    "\n",
    "        return decoder    \n",
    "    \n",
    "    def call(self, x, mems, pre_y_train, training=None):        \n",
    "        embeddings = self.embedding_layer(x)\n",
    "        \n",
    "        \n",
    "        if mems[0] is not None:\n",
    "            rel_embeddings = self.rel_embeddings(tf.concat((embeddings,mems[0]), axis=1))\n",
    "        else:\n",
    "            rel_embeddings = self.rel_embeddings(embeddings)\n",
    "        \n",
    "        encoder, mems = self.transformer_xl(content_stream=embeddings, relative_position_encoding=rel_embeddings, state=mems)\n",
    "                    \n",
    "        model_output = self.decoder(([pre_y_train, encoder]), training=training)\n",
    "        \n",
    "        return model_output, mems\n",
    "    \n",
    "    def predictNTF(self, j, x, pre_y_train, post_y_train):\n",
    "        embeddings = self.embedding_layer(x)\n",
    "        mems = [*tf.zeros((self.num_layers, x.shape[0], self.memory_length, self.embed_dim))]\n",
    "        rel_embeddings = self.rel_embeddings(tf.concat((embeddings,mems[0]), axis=1))\n",
    "        context, mems = self.transformer_xl(content_stream=embeddings, relative_position_encoding=rel_embeddings, state=mems)\n",
    "        \n",
    "        token = np.zeros((x.shape[0], self.maxlen-1))\n",
    "        token[:,0] = 1 \n",
    "        \n",
    "        for i in range(post_y_train.shape[1]-1):\n",
    "            result = self.decoder.predict([token,context[:x.shape[0]]]).argmax(-1)\n",
    "            if np.all(result[:,i] == 2):\n",
    "                break \n",
    "                token[:,i+1] = result[:,i]\n",
    "                \n",
    "        result = self.decoder.predict([token,context])\n",
    "        result = result.argmax(-1)\n",
    "        \n",
    "        decoded = decode_seq(result[j],i_to_c_pandp)\n",
    "        target = decode_seq(pre_y_train[j],i_to_c_pandp)\n",
    "        \n",
    "        #accuracy = MaskedSparseCategoricalAccuracy(post_y_train, result).numpy()*100.0\n",
    "        accuracy = 5\n",
    "        \n",
    "        return decoded, target, accuracy,  mems\n",
    "    \n",
    "    \n",
    "    def predictTF(self, j, x, pre_y_train):\n",
    "        embeddings = self.embedding_layer(x)\n",
    "        mems = [*tf.zeros((self.num_layers, x.shape[0], self.memory_length, self.embed_dim))]\n",
    "        rel_embeddings = self.rel_embeddings(tf.concat((embeddings,mems[0]), axis=1))\n",
    "        context, mems = self.transformer_xl(content_stream=embeddings, relative_position_encoding=rel_embeddings, state=mems)\n",
    "        \n",
    "        result = self.decoder.predict([pre_y_train, context[:x.shape[0]]]).argmax(-1)\n",
    "        \n",
    "        decoded = decode_seq(result[j],i_to_c_pandp)\n",
    "        target = decode_seq(post_y_train[j],i_to_c_pandp)\n",
    "        \n",
    "        #accuracy = MaskedSparseCategoricalAccuracy(post_y_train[:0], result[:0]).numpy()*100.0\n",
    "        accuracy = 5\n",
    "        \n",
    "        return decoded, target, accuracy, mems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2bc3ea7d-d4a1-4a1e-bf2a-7cd1c91430cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters \n",
    "embed_dim = 64\n",
    "encoder_shape = (x_train.shape[1], embed_dim)\n",
    "target_shape = pre_y_train.shape[1:]\n",
    "vocab_size = len(i_to_c_pandp)\n",
    "num_layers = 8\n",
    "hidden_size = 64\n",
    "num_attention_heads = 8\n",
    "maxlen = x_train.shape[1]\n",
    "memory_length = 200\n",
    "reuse_length = 0\n",
    "head_size = 32\n",
    "inner_size = 32\n",
    "dropout_rate = 0.0\n",
    "attention_dropout_rate = 0.0\n",
    "initializer = keras.initializers.RandomNormal(stddev=0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "821b0c4a-6d05-4867-a713-474bfdbff881",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():  \n",
    "    model = XlModel(embed_dim, encoder_shape, target_shape, vocab_size, num_layers, hidden_size, num_attention_heads, maxlen, memory_length, reuse_length, head_size, inner_size, dropout_rate, attention_dropout_rate, initializer)\n",
    "    model.compile(loss = MaskedSparseCategoricalCrossentropy, optimizer = keras.optimizers.Adam())\n",
    "    batch = x_train\n",
    "    batch_size = x_train.shape[0]\n",
    "    mems = [*tf.zeros((num_layers, batch_size, memory_length, embed_dim))]\n",
    "    output, mems = model(x_train, mems, pre_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "92b02390-c88e-40e1-9686-cb7fc2581e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"xl_model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " masked_token_and_position_e  multiple                 8448      \n",
      " mbedding_4 (MaskedTokenAndP                                     \n",
      " ositionEmbedding)                                               \n",
      "                                                                 \n",
      " relative_position_embedding  multiple                 0         \n",
      " _2 (RelativePositionEmbeddi                                     \n",
      " ng)                                                             \n",
      "                                                                 \n",
      " masked_token_and_position_e  (None, 73, 64)           8384      \n",
      " mbedding_5 (MaskedTokenAndP                                     \n",
      " ositionEmbedding)                                               \n",
      "                                                                 \n",
      " transformer_xl_2 (Transform  multiple                 691456    \n",
      " erXL)                                                           \n",
      "                                                                 \n",
      " Decoder (Functional)        (None, 73, 57)            1107769   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,807,673\n",
      "Trainable params: 1,807,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752fb718-67f6-44ff-aa6d-c889c0254b82",
   "metadata": {},
   "source": [
    "---\n",
    "# Custom Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6fafad8d-f020-49b0-8233-bcfe54b88309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_step(batch, mems):\n",
    "    x, y, y_post = batch\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        result, new_mems = model(x, mems, y, training=True)\n",
    "        loss = MaskedSparseCategoricalCrossentropy(y_post, result)\n",
    "        \n",
    "    accuracy = MaskedSparseCategoricalAccuracy(y_post, result)\n",
    "        \n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    return loss, accuracy, new_mems\n",
    "\n",
    "@tf.function()\n",
    "def dist_train_step(batch, mems):\n",
    "    losses, accuracy, new_mems = strategy.run(train_step, args=(batch, mems))\n",
    "    return strategy.reduce(tf.distribute.ReduceOp.SUM, losses, axis=None), accuracy, new_mems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5a6127f4-fb94-4bf2-b3cf-fae83ef0aa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "history_loss = []\n",
    "history_accuracy = []\n",
    "epochs = 270"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a260118e-c39d-4232-8ee2-11ae86d8c8f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 Loss: 0.13592471182346344 Accuracy = 0.9672094583511353"
     ]
    }
   ],
   "source": [
    "with strategy.scope():    \n",
    "    for epoch in range(epochs):\n",
    "        batch = (x_train, pre_y_train, post_y_train)\n",
    "\n",
    "        loss, accuracy, mems = dist_train_step(batch, mems)\n",
    "        \n",
    "        history_loss.append(loss)\n",
    "        history_accuracy.append(accuracy) \n",
    "        \n",
    "        print(f\"\\r{epoch+1}/{epochs} Loss: {loss} Accuracy = {accuracy}\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "724ebe19-73b6-4aeb-b506-f24d6722ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_lens = []\n",
    "final_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cc71d008-1f06-4d2b-a8b4-0ed7fb8603d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 30.   0. 100. 150.]\n",
      "[0.965912 0.968771 0.969217 0.969663]\n"
     ]
    }
   ],
   "source": [
    "mem_lens = np.loadtxt(\"/home/jovyan/TransformerXL/XL/Mem_Lens.txt\", dtype=np.float64)\n",
    "final_accuracies =  np.loadtxt(\"/home/jovyan/TransformerXL/XL/Final_Accuracies.txt\", dtype=np.float64)\n",
    "\n",
    "print(mem_lens)\n",
    "print(final_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6b5293b4-e6a7-4fbb-a40a-512e965573c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 30.   0. 100. 150. 200.]\n",
      "[0.965912   0.968771   0.969217   0.969663   0.96720946]\n"
     ]
    }
   ],
   "source": [
    "mem_lens = np.append(mem_lens, memory_length)\n",
    "final_accuracies = np.append(final_accuracies, accuracy)\n",
    "print(mem_lens)\n",
    "print(final_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "779262e0-716e-431b-9e95-464bb76b8f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.  30. 100. 150. 200.]\n",
      "[0.968771   0.965912   0.969217   0.969663   0.96720946]\n"
     ]
    }
   ],
   "source": [
    "mem_accuracy = ({mem_len:accuracy for mem_len, accuracy in zip(mem_lens, final_accuracies)})\n",
    "mem_lens_sorted = np.array([l for l in sorted(mem_accuracy)])\n",
    "final_accuracies_sorted = np.array([mem_accuracy[l] for l in mem_lens_sorted])\n",
    "print(mem_lens_sorted)\n",
    "print(final_accuracies_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2fd866ef-9445-40a7-9d7e-85071baf9b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('/home/jovyan/TransformerXL/XL/Mem_Lens.txt', (mem_lens), fmt='%f', delimiter='\\n')\n",
    "np.savetxt('/home/jovyan/TransformerXL/XL//Final_Accuracies.txt', (final_accuracies), fmt='%f', delimiter='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3e2dabdb-0f8b-4c68-91a0-72d0dd47d056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.plot(history_accuracy)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(history_loss)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "#plt.title(\"Accuracy and Loss Over Time\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ba942651-37b0-465e-b3c0-5dd22621b61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(2)\n",
    "plt.subplot(222)\n",
    "plt.plot(mem_lens_sorted, final_accuracies_sorted)\n",
    "plt.xlabel('Memory Length')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "27464ed8-16ad-423c-acc5-d379bb4db2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ouickness than her sisters.\" \n",
      " quickness than her sisters.\" \n",
      " 5\n"
     ]
    }
   ],
   "source": [
    "predictionTF, target, accuracyTF, mems = model.predictTF(0, x_train, pre_y_train)\n",
    "print(predictionTF, '\\n', target, '\\n', accuracyTF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "23dd8aee-994c-4f4c-8a18-69356b3e1321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \"                                                                         \n",
      " \"I do not cough for my own amusement,\" replied Kitty fretfully. \"When is \n",
      " 5\n"
     ]
    }
   ],
   "source": [
    "predictionNTF, target, accuracyNTF, mems = model.predictNTF(1, x_train[:20], pre_y_train, post_y_train)\n",
    "print('\\n', predictionNTF, '\\n', target, '\\n', accuracyNTF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae81ff1b-a864-468f-b5a2-9a9036102cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-07-07 17:18:46--  https://www.cs.mtsu.edu/~jphillips/courses/CSCI4850-5850/public/PandP_Jane_Austen.txt\n",
      "Resolving www.cs.mtsu.edu (www.cs.mtsu.edu)... 161.45.162.100\n",
      "Connecting to www.cs.mtsu.edu (www.cs.mtsu.edu)|161.45.162.100|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 683838 (668K) [text/plain]\n",
      "Saving to: ‘PandP_Jane_Austen.txt.1’\n",
      "\n",
      "PandP_Jane_Austen.t 100%[===================>] 667.81K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2022-07-07 17:18:46 (11.5 MB/s) - ‘PandP_Jane_Austen.txt.1’ saved [683838/683838]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10657, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!wget https://www.cs.mtsu.edu/~jphillips/courses/CSCI4850-5850/public/PandP_Jane_Austen.txt\n",
    "with open('PandP_Jane_Austen.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "# Paragraphs are separated by blank\n",
    "# lines -> just drop those lines...\n",
    "text = []\n",
    "for i in range(len(lines)):\n",
    "     if lines[i] != '':\n",
    "        text = text + [lines[i]]\n",
    "data = np.vstack([[text[0:-1]],[text[1:]]]).T\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22c1911c-092e-4a19-be17-008e0979c3fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq = 100\n",
    "split_point = 80\n",
    "data = data[0:n_seq]\n",
    "np.random.shuffle(data) # In-place modification\n",
    "max_length = np.max([len(i) for i in data.flatten()]) + 2 # Add start/stop\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf21399d-41c5-4dcd-9650-916337917cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_seq(x,mapping,max_length=0):\n",
    "    # String to integer\n",
    "    return [mapping['<START>']] + \\\n",
    "    [mapping[i] for i in list(x)] + \\\n",
    "    [mapping['<STOP>']] + \\\n",
    "    [0]*(max_length-len(list(x))-2)\n",
    "def decode_seq(x,mapping):\n",
    "    # Integer-to-string\n",
    "    try:\n",
    "        idx = list(x).index(2) # Stop token?\n",
    "    except:\n",
    "        idx = len(list(x)) # No stop token found\n",
    "    return ''.join([mapping[i] for i in list(x)[0:idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2289815-d2d7-4c5c-ba6f-ed36987d8a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_to_c_pandp = ['','<START>','<STOP>'] + list({char for word in data[:,0] for char in word})\n",
    "c_to_i_pandp = {i_to_c_pandp[i]:i for i in range(len(i_to_c_pandp))}\n",
    "i_to_c_pandp[1] = i_to_c_pandp[2] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9134d636-3ece-4bc9-b7bc-bb3816f249e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack([encode_seq(x,c_to_i_pandp,max_length) for x in data[:,0]])\n",
    "Y = np.vstack([encode_seq(x,c_to_i_pandp,max_length) for x in data[:,1]])\n",
    "x_train = X[:split_point]\n",
    "x_test = X[split_point:]\n",
    "pre_y_train = Y[:,0:-1][:split_point]\n",
    "pre_y_test = Y[:,0:-1][split_point:]\n",
    "post_y_train = Y[:,1:][:split_point]\n",
    "post_y_test = Y[:,1:][split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd72860-e163-4462-835e-2183147f6dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
