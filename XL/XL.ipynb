{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ccea5cd-89de-452b-afa6-703df9e9a604",
   "metadata": {},
   "source": [
    "---\n",
    "# Transformer XL\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2c14f8-8dd8-459f-81fe-a6708e6a8355",
   "metadata": {},
   "source": [
    "---\n",
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dcad43d-67f7-49b9-84ce-6d91a0f468a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e7cb9df-5746-44a3-998a-b671b284957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fda64db1-8058-4e15-b2d4-1a3e64f23fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "import math\n",
    "import string\n",
    "\n",
    "import tf_utils as tfu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dfb3b1a-2db7-4463-a208-b5d29e20f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tfu.devices.select_gpu(0, use_dynamic_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54593a0f-b5b4-401e-91e2-8c1a1598432e",
   "metadata": {},
   "source": [
    "---\n",
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddb61447-8228-4b92-8f3f-0eddb8e33574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10657, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('Dataset/PandP_Jane_Austen.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "# Paragraphs are separated by blank\n",
    "# lines -> just drop those lines...\n",
    "text = []\n",
    "for i in range(len(lines)):\n",
    "     if lines[i] != '':\n",
    "        text = text + [lines[i]]\n",
    "data = np.vstack([[text[0:-1]],[text[1:]]]).T\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5f65884-85ad-4ebd-ba51-29430504b0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq = 100\n",
    "split_point = 80\n",
    "data = data[0:n_seq]\n",
    "np.random.shuffle(data) # In-place modification\n",
    "max_length = np.max([len(i) for i in data.flatten()]) + 2 # Add start/stop\n",
    "max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689bbd23-6cbf-472f-bd6f-dc4a008b9072",
   "metadata": {},
   "source": [
    "---\n",
    "# Encode Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09c4c6ef-93df-4294-af8f-2fbaf01ca9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_seq(x,mapping,max_length=0):\n",
    "    # String to integer\n",
    "    return [mapping['<START>']] + \\\n",
    "    [mapping[i] for i in list(x)] + \\\n",
    "    [mapping['<STOP>']] + \\\n",
    "    [0]*(max_length-len(list(x))-2)\n",
    "def decode_seq(x,mapping):\n",
    "    # Integer-to-string\n",
    "    try:\n",
    "        idx = list(x).index(2) # Stop token?\n",
    "    except:\n",
    "        idx = len(list(x)) # No stop token found\n",
    "    return ''.join([mapping[i] for i in list(x)[0:idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76b7c608-fe15-4420-a4fb-8356ebdcefc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_to_c_pandp = ['','<START>','<STOP>'] + list({char for word in data[:,0] for char in word})\n",
    "c_to_i_pandp = {i_to_c_pandp[i]:i for i in range(len(i_to_c_pandp))}\n",
    "i_to_c_pandp[1] = i_to_c_pandp[2] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6889059-fdd9-43e2-b0bc-199ff07c4199",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack([encode_seq(x,c_to_i_pandp,max_length) for x in data[:,0]])\n",
    "Y = np.vstack([encode_seq(x,c_to_i_pandp,max_length) for x in data[:,1]])\n",
    "x_train = X[:split_point]\n",
    "x_test = X[split_point:]\n",
    "pre_y_train = Y[:,0:-1][:split_point]\n",
    "pre_y_test = Y[:,0:-1][split_point:]\n",
    "post_y_train = Y[:,1:][:split_point]\n",
    "post_y_test = Y[:,1:][split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca664bb9-544b-4264-8da4-34acaca2d379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 74)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f725c81-51b2-4497-981a-a15981f9dba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 74)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6a03188-6146-4487-8483-b0a3296d4426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  4, 45, 37, 45,  8, 52, 53, 48, 24, 24, 23, 33, 18, 34, 37, 56,\n",
       "        8, 27, 45, 18, 12, 45, 18, 25,  4, 45, 18,  8, 36, 52, 18, 52, 36,\n",
       "       18,  3, 56, 37, 56, 52, 17, 34,  2,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "154bc9ef-0e64-4ad5-96d4-13012994718f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4, 45, 37, 45,  8, 52, 53, 48, 24, 24, 23, 33, 18, 34, 37, 56,  8,\n",
       "       27, 45, 18, 12, 45, 18, 25,  4, 45, 18,  8, 36, 52, 18, 52, 36, 18,\n",
       "        3, 56, 37, 56, 52, 17, 34,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2e33b84-c781-43ae-a1f6-5c721f8a5697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1, 34, 20, ...,  4,  2,  0],\n",
       "       [ 1, 51, 23, ..., 45,  2,  0],\n",
       "       [ 1, 34, 14, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 1, 52, 22, ...,  0,  0,  0],\n",
       "       [ 1, 25,  4, ...,  0,  0,  0],\n",
       "       [ 1, 37, 36, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd768ff5-2158-4e46-9852-2551bf73d350",
   "metadata": {},
   "source": [
    "---\n",
    "# Flatten Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be0ed294-df8f-47e7-b127-4d5d33a862ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Flatten_Data(x):\n",
    "    x = x.flatten()\n",
    "    \n",
    "    x = x[x != 0]\n",
    "    x = x[x != 1]\n",
    "    x = x[x != 2]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd7b115-1756-4eab-8494-a2c3cd6cfc77",
   "metadata": {},
   "source": [
    "---\n",
    "# Create Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93d7530f-e127-4ab2-b006-d8f3711ca453",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_flattened_train = Flatten_Data(x_train)\n",
    "pre_y_flattened_train = Flatten_Data(pre_y_train)\n",
    "post_y_flattened_train = Flatten_Data(post_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f33669-803a-45b3-8856-9f01c2ec70d4",
   "metadata": {},
   "source": [
    "---\n",
    "# Create Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f48728bc-d9fb-4f57-88b4-fac9bf7344b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_flattened_test = Flatten_Data(x_test)\n",
    "pre_y_flattened_test = Flatten_Data(pre_y_test)\n",
    "post_y_flattened_test = Flatten_Data(post_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1293d03c-68bd-496d-b9fb-91ae84688fe0",
   "metadata": {},
   "source": [
    "--- \n",
    "# Batch Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e23e34f9-3066-4756-ad0b-ccc1a3194438",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "rng = np.random.default_rng(seed)\n",
    "batch_size = 20\n",
    "block_size = 2\n",
    "seq_len = 10\n",
    "seq_len_padded = seq_len + 2\n",
    "maxlen = seq_len + 2 #Add start/stop tokens\n",
    "vocab_size = len(i_to_c_pandp)\n",
    "num_chars_data_train = x_flattened_train.shape[0]\n",
    "num_chars_data_test = x_flattened_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e768970c-7af4-4626-9d6e-d712ab7de959",
   "metadata": {},
   "outputs": [],
   "source": [
    "if block_size-2 > seq_len:\n",
    "    raise ValueError(\"Block size should not be bigger than sequence length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dce0777e-c8af-4835-833d-f1717dbf2501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "57\n",
      "4410\n",
      "1180\n"
     ]
    }
   ],
   "source": [
    "print(maxlen)\n",
    "print(vocab_size)\n",
    "print(num_chars_data_train)\n",
    "print(num_chars_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56decdad-3c1d-496f-8f04-d783de100a1c",
   "metadata": {},
   "source": [
    "---\n",
    "# Generate Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc198d86-abfe-4a39-86fb-d8722200c228",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "def Partial_Batch(dataset, pre_y, post_y, num_chars, seq_len, rng):\n",
    "    rints = rng.integers(low=0, high=num_chars-seq_len, size=1)[0]\n",
    "    \n",
    "    end_x = rints + seq_len\n",
    "    end_y = end_x + seq_len\n",
    "    \n",
    "    x = dataset[rints:end_x]\n",
    "    x = np.insert(x, 0, 1)\n",
    "    x = np.insert(x, x.shape[0], 2)\n",
    "    \n",
    "    pre_y = pre_y[end_x:end_y]\n",
    "    pre_y = np.insert(pre_y, 0, 1)\n",
    "    pre_y[-1] = 2\n",
    "    \n",
    "    post_y = post_y[end_x:end_y]\n",
    "    post_y = np.insert(post_y, post_y.shape[0], 2)\n",
    "    \n",
    "    batch = [x, pre_y, post_y]    \n",
    "    \n",
    "    batch = tf.keras.preprocessing.sequence.pad_sequences(batch, maxlen=maxlen, padding='post', value=0)\n",
    "\n",
    "    padding = maxlen + (block_size-(maxlen%block_size))\n",
    "                        \n",
    "    if (batch.shape[1] % block_size) != 0:\n",
    "        batch = tf.keras.preprocessing.sequence.pad_sequences(batch, maxlen=padding, padding='post', value=0)\n",
    "   \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ccc910a-fc5a-47c0-bf90-83587350fb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Full_Batch(batch_size, x, pre_y, post_y, num_chars, seq_len, rng):\n",
    "    x0 = []\n",
    "    y1 = []\n",
    "    y2 = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        a, b, c, = Partial_Batch(x, pre_y, post_y, num_chars, seq_len, rng)\n",
    "    \n",
    "        x0.append(a)\n",
    "        y1.append(b)\n",
    "        y2.append(c) \n",
    "    \n",
    "    return np.asarray(x0), np.asarray(y1), np.asarray(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33326847-2235-4ec9-ae39-a808a34050c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_train = Full_Batch(batch_size, x_flattened_train, pre_y_flattened_train, post_y_flattened_train, num_chars_data_train, seq_len, rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74123bfb-38c6-48cf-b830-39c7266398d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_test = Full_Batch(batch_size, x_flattened_test, pre_y_flattened_test, post_y_flattened_test, num_chars_data_test, seq_len, rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45d5d65-b728-42de-b529-85643adef96d",
   "metadata": {},
   "source": [
    "---\n",
    "# Masked Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a199f10-268d-4153-863c-2d430a4d2c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedTransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(MaskedTransformerBlock, self).__init__()\n",
    "        self.att1 = keras.layers.MultiHeadAttention(num_heads=num_heads,\n",
    "        key_dim=embed_dim)\n",
    "        self.att2 = keras.layers.MultiHeadAttention(num_heads=num_heads,\n",
    "        key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "        [keras.layers.Dense(ff_dim, activation=\"gelu\"),\n",
    "        keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "        self.dropout3 = keras.layers.Dropout(rate)\n",
    "    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n",
    "        i = tf.range(n_dest)[:, None]\n",
    "        j = tf.range(n_src)\n",
    "        m = i >= j - n_src + n_dest\n",
    "        mask = tf.cast(m, dtype)\n",
    "        mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "        mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "    def call(self, inputs, training):\n",
    "        input_shape = tf.shape(inputs[0])\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        mask = self.causal_attention_mask(batch_size,\n",
    "        seq_len, seq_len,\n",
    "        tf.bool)\n",
    "        attn_output1 = self.att1(inputs[0], inputs[0],\n",
    "        attention_mask = mask)\n",
    "        attn_output1 = self.dropout1(attn_output1, training=training)\n",
    "        out1 = self.layernorm1(inputs[0] + attn_output1)\n",
    "        attn_output2 = self.att2(out1, inputs[1])\n",
    "        attn_output2 = self.dropout2(attn_output2, training=training)\n",
    "        out2 = self.layernorm1(out1 + attn_output2)\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        return self.layernorm2(out2 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befee659-3a43-4372-ae56-860a836497e1",
   "metadata": {},
   "source": [
    "---\n",
    "# Masked Token and Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a27a7c63-8d17-4e49-bfd7-4e05d92eb915",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedTokenAndPositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim, **kwargs):\n",
    "        super(MaskedTokenAndPositionEmbedding, self).__init__(**kwargs)\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size,\n",
    "        output_dim=embed_dim,\n",
    "        mask_zero=True)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen+1,\n",
    "        output_dim=embed_dim,\n",
    "        mask_zero=True)\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=1, limit=maxlen+1, delta=1)\n",
    "        positions = positions * tf.cast(tf.sign(x),tf.int32)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348583b3-a8cf-4d03-bc57-cbd8c106b890",
   "metadata": {},
   "source": [
    "---\n",
    "# Custom Loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "160d05e4-b851-4348-85df-508526fb154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "def MaskedSparseCategoricalCrossentropy(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "def MaskedSparseCategoricalAccuracy(real, pred):\n",
    "    accuracies = tf.equal(tf.cast(real,tf.int64), tf.argmax(pred, axis=2))\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b66c98-f8b5-4b80-9736-66a9769cc966",
   "metadata": {},
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9af4ac04-1eab-4d74-bfc0-716408efeed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_initializer(initializer):\n",
    "    if isinstance(initializer, tf.keras.initializers.Initializer):\n",
    "        return initializer.__class__.from_config(initializer.get_config())\n",
    "    return initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aadfeaaa-08b0-4155-8381-328ac71f4c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape_list(tensor, expected_rank=None, name=None):\n",
    "    if expected_rank is not None:\n",
    "        assert_rank(tensor, expected_rank, name)\n",
    "\n",
    "    shape = tensor.shape.as_list()\n",
    "\n",
    "    non_static_indexes = []\n",
    "    for (index, dim) in enumerate(shape):\n",
    "        if dim is None:\n",
    "            non_static_indexes.append(index)\n",
    "\n",
    "    if not non_static_indexes:\n",
    "        return shape\n",
    "\n",
    "    dyn_shape = tf.shape(tensor)\n",
    "    for index in non_static_indexes:\n",
    "        shape[index] = dyn_shape[index]\n",
    "    return shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835c39dd-9fb5-49bf-8763-14ca694a1a35",
   "metadata": {},
   "source": [
    "---\n",
    "# Cache Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da4cb5e4-2d26-4e40-a00f-3e71f470b273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_state: `Tensor`, the current state.\n",
    "# previous_state: `Tensor`, the previous state.\n",
    "# memory_length: `int`, the number of tokens to cache.\n",
    "# reuse_length: `int`, the number of tokens in the current batch to be cached\n",
    "#     and reused in the future.\n",
    "# Returns:  `Tensor`, representing the cached state with stopped gradients.\n",
    "def _cache_memory(current_state, previous_state, memory_length, reuse_length=0):\n",
    "    if memory_length is None or memory_length == 0:\n",
    "        return None\n",
    "    else:\n",
    "        if reuse_length > 0:\n",
    "            current_state = current_state[:, :reuse_length, :]\n",
    "\n",
    "        if previous_state is None:\n",
    "            new_mem = current_state[:, -memory_length:, :]\n",
    "        else:\n",
    "            new_mem = tf.concat(\n",
    "                    [previous_state, current_state], 1)[:, -memory_length:, :]\n",
    "\n",
    "    return tf.stop_gradient(new_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3126b9-a96e-4d6a-ae65-3c9579514cf1",
   "metadata": {},
   "source": [
    "---\n",
    "# MultiHead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b32faad-fd06-4448-b817-b3a1f5943aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query: Query `Tensor` of shape `[B, T, dim]`.\n",
    "# value: Value `Tensor` of shape `[B, S, dim]`.\n",
    "# content_attention_bias: Bias `Tensor` for content based attention of shape\n",
    "# `[num_heads, dim]`.\n",
    "# positional_attention_bias: Bias `Tensor` for position based attention of\n",
    "# shape `[num_heads, dim]`.\n",
    "# key: Optional key `Tensor` of shape `[B, S, dim]`. If not given, will use\n",
    "# `value` for both `key` and `value`, which is the most common case.\n",
    "# relative_position_encoding: Relative positional encoding `Tensor` of shape\n",
    "# `[B, L, dim]`.\n",
    "# state: Optional `Tensor` of shape `[B, M, E]` where M is the length of the\n",
    "# state or memory. If passed, this is also attended over as in Transformer\n",
    "# XL.\n",
    "# attention_mask: A boolean mask of shape `[B, T, S]` that prevents attention\n",
    "# to certain positions.\n",
    "class MultiHeadRelativeAttention(tf.keras.layers.MultiHeadAttention):\n",
    "    def __init__(self,\n",
    "                 kernel_initializer=\"variance_scaling\",\n",
    "                 **kwargs):\n",
    "        super().__init__(kernel_initializer=kernel_initializer,\n",
    "                                         **kwargs)\n",
    "\n",
    "    def _build_from_signature(self, query, value, key=None):\n",
    "        super(MultiHeadRelativeAttention, self)._build_from_signature(\n",
    "                query=query,\n",
    "                value=value,\n",
    "                key=key)\n",
    "        if hasattr(value, \"shape\"):\n",
    "            value_shape = tf.TensorShape(value.shape)\n",
    "        else:\n",
    "            value_shape = value\n",
    "        if key is None:\n",
    "            key_shape = value_shape\n",
    "        elif hasattr(key, \"shape\"):\n",
    "            key_shape = tf.TensorShape(key.shape)\n",
    "        else:\n",
    "            key_shape = key\n",
    "\n",
    "        common_kwargs = dict(\n",
    "                kernel_initializer=self._kernel_initializer,\n",
    "                bias_initializer=self._bias_initializer,\n",
    "                kernel_regularizer=self._kernel_regularizer,\n",
    "                bias_regularizer=self._bias_regularizer,\n",
    "                activity_regularizer=self._activity_regularizer,\n",
    "                kernel_constraint=self._kernel_constraint,\n",
    "                bias_constraint=self._bias_constraint)\n",
    "\n",
    "        with tf.init_scope():\n",
    "            einsum_equation, _, output_rank = _build_proj_equation(\n",
    "                    key_shape.rank - 1, bound_dims=1, output_dims=2)\n",
    "            self._encoding_dense = tf.keras.layers.experimental.EinsumDense(\n",
    "                    einsum_equation,\n",
    "                    output_shape=_get_output_shape(\n",
    "                        output_rank - 1,\n",
    "                        [self._num_heads, self._key_dim]),\n",
    "                        bias_axes=None,\n",
    "                        name=\"encoding\",\n",
    "                        **common_kwargs)\n",
    "\n",
    "# query: Projected query `Tensor` of shape `[B, T, N, key_dim]`.\n",
    "# key: Projected key `Tensor` of shape `[B, S + M, N, key_dim]`.\n",
    "# value: Projected value `Tensor` of shape `[B, S + M, N, key_dim]`.\n",
    "# position: Projected position `Tensor` of shape `[B, L, N, key_dim]`.\n",
    "# content_attention_bias: Trainable bias parameter added to the query head\n",
    "#     when calculating the content-based attention score.\n",
    "# positional_attention_bias: Trainable bias parameter added to the query\n",
    "#     head when calculating the position-based attention score.\n",
    "# attention_mask: (default None) Optional mask that is added to attention\n",
    "#     logits. If state is not None, the mask source sequence dimension should\n",
    "#     extend M.\n",
    "# Returns:\n",
    "# attention_output: Multi-headed output of attention computation of shape\n",
    "#     `[B, S, N, key_dim]`.\n",
    "    def compute_attention(\n",
    "        self,\n",
    "        query,\n",
    "        key,\n",
    "        value,\n",
    "        position,\n",
    "        content_attention_bias,\n",
    "        positional_attention_bias,\n",
    "        attention_mask=None\n",
    "    ):\n",
    "        \n",
    "        #AC\n",
    "        content_attention = tf.einsum(self._dot_product_equation, key, query + content_attention_bias)\n",
    "        \n",
    "        positional_attention = tf.einsum(self._dot_product_equation, position, query + positional_attention_bias)\n",
    "        \n",
    "        #BD\n",
    "        positional_attention = _rel_shift(positional_attention, klen=tf.shape(content_attention)[3])\n",
    "        \n",
    "        attention_sum = content_attention + positional_attention\n",
    "\n",
    "        attention_scores = tf.multiply(attention_sum, 1.0 / math.sqrt(float(self._key_dim)))\n",
    "\n",
    "        attention_scores = self._masked_softmax(attention_scores, attention_mask)\n",
    "\n",
    "        attention_output = self._dropout_layer(attention_scores)\n",
    "\n",
    "        attention_output = tf.einsum(self._combine_equation,\n",
    "                                                                 attention_output,\n",
    "                                                                 value)\n",
    "        return attention_output\n",
    "\n",
    "# * Number of heads (H): the number of attention heads.\n",
    "# * Value size (V): the size of each value embedding per head.\n",
    "# * Key size (K): the size of each key embedding per head. Equally, the size\n",
    "#     of each query embedding per head. Typically K <= V.\n",
    "# * Batch dimensions (B).\n",
    "# * Query (target) attention axes shape (T).\n",
    "# * Value (source) attention axes shape (S), the rank must match the target.\n",
    "# * Encoding length (L): The relative positional encoding length.\n",
    "# query: attention input.\n",
    "# value: attention input.\n",
    "# content_attention_bias: A trainable bias parameter added to the query head\n",
    "#     when calculating the content-based attention score.\n",
    "# positional_attention_bias: A trainable bias parameter added to the query\n",
    "#     head when calculating the position-based attention score.\n",
    "# key: attention input.\n",
    "# relative_position_encoding: relative positional encoding for key and\n",
    "#     value.\n",
    "# state: (default None) optional state. If passed, this is also attended\n",
    "#     over as in TransformerXL.\n",
    "# attention_mask: (default None) Optional mask that is added to attention\n",
    "#     logits. If state is not None, the mask source sequence dimension should\n",
    "#     extend M.\n",
    "# Returns:\n",
    "# attention_output: The result of the computation, of shape [B, T, E],\n",
    "#     where `T` is for target sequence shapes and `E` is the query input last\n",
    "#     dimension if `output_shape` is `None`. Otherwise, the multi-head outputs\n",
    "#     are projected to the shape specified by `output_shape`.\n",
    "    def call(self,\n",
    "             query,\n",
    "             value,\n",
    "             content_attention_bias,\n",
    "             positional_attention_bias,\n",
    "             key=None,\n",
    "             relative_position_encoding=None,\n",
    "             state=None,\n",
    "             attention_mask=None):\n",
    "        \n",
    "        if not self._built_from_signature:\n",
    "            self._build_from_signature(query, value, key=key)\n",
    "        if key is None:\n",
    "            key = value\n",
    "        if state is not None and state.shape.ndims > 1:\n",
    "            value = tf.concat([state, value], 1)\n",
    "            key = tf.concat([state, key], 1)\n",
    "\n",
    "        # `query` = [B, T, N ,H]\n",
    "        query = self._query_dense(query)\n",
    "\n",
    "        # `key` = [B, S + M, N, H]\n",
    "        key = self._key_dense(key)\n",
    "\n",
    "        # `value` = [B, S + M, N, H]\n",
    "        value = self._value_dense(value)\n",
    "\n",
    "        # `position` = [B, L, N, H]\n",
    "        position = self._encoding_dense(relative_position_encoding)\n",
    "\n",
    "        attention_output = self.compute_attention(\n",
    "                query=query,\n",
    "                key=key,\n",
    "                value=value,\n",
    "                position=position,\n",
    "                content_attention_bias=content_attention_bias,\n",
    "                positional_attention_bias=positional_attention_bias,\n",
    "                attention_mask=attention_mask)\n",
    "\n",
    "        # `attention_output` = [B, S, N, H]\n",
    "        attention_output = self._output_dense(attention_output)\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95abdafa-2c98-4c04-b9e8-aae23e53265e",
   "metadata": {},
   "source": [
    "---\n",
    "# Relative Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc8c99a8-3755-48f9-9704-c1dbd04e0ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rel_shift(x, klen=-1):    \n",
    "    x = tf.transpose(x, perm=[2, 3, 0, 1])\n",
    "    x = tf.pad(x, [[0, 0], [1, 0], [0, 0], [0, 0]])\n",
    "    x_size = tf.shape(x)\n",
    "    x = tf.reshape(x, [x_size[1], x_size[0], x_size[2], x_size[3]])\n",
    "    x = tf.slice(x, [1, 0, 0, 0], [-1, -1, -1, -1])\n",
    "    x = tf.reshape(x, [x_size[0], x_size[1] - 1, x_size[2], x_size[3]])\n",
    "    x = tf.transpose(x, perm=[2, 3, 0, 1])\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a026fd-2cbe-45b7-88f7-a3ffd3548806",
   "metadata": {},
   "source": [
    "---\n",
    "# Build Einsum Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5102422c-82af-49d4-a8d5-5081262b21e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_CHR_IDX = string.ascii_lowercase\n",
    "\n",
    "# Builds an einsum equation for projections inside multi-head attention\n",
    "def _build_proj_equation(free_dims, bound_dims, output_dims):\n",
    "    input_str = \"\"\n",
    "    kernel_str = \"\"\n",
    "    output_str = \"\"\n",
    "    bias_axes = \"\"\n",
    "    letter_offset = 0\n",
    "    for i in range(free_dims):\n",
    "        char = _CHR_IDX[i + letter_offset]\n",
    "        input_str += char\n",
    "        output_str += char\n",
    "\n",
    "    letter_offset += free_dims\n",
    "    for i in range(bound_dims):\n",
    "        char = _CHR_IDX[i + letter_offset]\n",
    "        input_str += char\n",
    "        kernel_str += char\n",
    "\n",
    "    letter_offset += bound_dims\n",
    "    for i in range(output_dims):\n",
    "        char = _CHR_IDX[i + letter_offset]\n",
    "        kernel_str += char\n",
    "        output_str += char\n",
    "        bias_axes += char\n",
    "    equation = \"%s,%s->%s\" % (input_str, kernel_str, output_str)\n",
    "\n",
    "    return equation, bias_axes, len(output_str)\n",
    "\n",
    "\n",
    "def _get_output_shape(output_rank, known_last_dims):\n",
    "    return [None] * (output_rank - len(known_last_dims)) + list(known_last_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5343f3e3-8536-46d5-9d1b-28f8c3f7908b",
   "metadata": {},
   "source": [
    "---\n",
    "# Relative Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a5f5eeac-4c07-4796-8281-f5c039fe7c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_size: Size of the hidden layer.\n",
    "# min_timescale: Minimum scale that will be applied at each position\n",
    "# max_timescale: Maximum scale that will be applied at each position.\n",
    "class RelativePositionEmbedding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 hidden_size: int,\n",
    "                 min_timescale: float = 1.0,\n",
    "                 max_timescale: float = 1.0e4,\n",
    "                 **kwargs):\n",
    "        \n",
    "        if \"dtype\" not in kwargs:\n",
    "            kwargs[\"dtype\"] = \"float32\"\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self._hidden_size = hidden_size\n",
    "        self._min_timescale = min_timescale\n",
    "        self._max_timescale = max_timescale\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "                \"hidden_size\": self._hidden_size,\n",
    "                \"min_timescale\": self._min_timescale,\n",
    "                \"max_timescale\": self._max_timescale,\n",
    "        }\n",
    "        base_config = super(RelativePositionEmbedding, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "# inputs: An tensor whose second dimension will be used as `length`. If\n",
    "# `None`, the other `length` argument must be specified.\n",
    "# length: An optional integer specifying the number of positions. If both\n",
    "# `inputs` and `length` are spcified, `length` must be equal to the second\n",
    "# dimension of `inputs`.\n",
    "# Returns:\n",
    "# A tensor in shape of `(length, hidden_size)`.\n",
    "    def call(self, inputs, length=None):\n",
    "        if inputs is None and length is None:\n",
    "            raise ValueError(\"If inputs is None, `length` must be set in \"\n",
    "                                             \"RelativePositionEmbedding().\")\n",
    "        if inputs is not None:\n",
    "            input_shape = get_shape_list(inputs)\n",
    "            if length is not None and length != input_shape[1]:\n",
    "                raise ValueError(\n",
    "                        \"If inputs is not None, `length` must equal to input_shape[1].\")\n",
    "            length = input_shape[1]\n",
    "        position = tf.cast(tf.range(length), tf.float32)\n",
    "        num_timescales = self._hidden_size // 2\n",
    "        min_timescale, max_timescale = self._min_timescale, self._max_timescale\n",
    "        log_timescale_increment = (\n",
    "                math.log(float(max_timescale) / float(min_timescale)) /\n",
    "                (tf.cast(num_timescales, tf.float32) - 1))\n",
    "        inv_timescales = min_timescale * tf.exp(\n",
    "                tf.cast(tf.range(num_timescales), tf.float32) *\n",
    "                -log_timescale_increment)\n",
    "        scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(\n",
    "                inv_timescales, 0)\n",
    "        position_embeddings = tf.concat(\n",
    "                [tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n",
    "        position_embeddings = tf.expand_dims(position_embeddings, axis=0)\n",
    "        return tf.tile(position_embeddings, (tf.shape(inputs)[0], 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c32bb5-4520-4802-a8f0-5f7849905a83",
   "metadata": {},
   "source": [
    "---\n",
    "# XL Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7245b7dd-a3b2-464a-9f5b-7efd4a9d0ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size: The size of the token vocabulary.\n",
    "# hidden_size: The size of the transformer hidden layers.\n",
    "# num_attention_heads: The number of attention heads.\n",
    "# head_size: The dimension size of each attention head.\n",
    "# inner_size: The inner size for the transformer layers.\n",
    "# dropout_rate: Dropout rate for the output of this layer.\n",
    "# attention_dropout_rate: Dropout rate on attention probabilities.\n",
    "# norm_epsilon: Epsilon value to initialize normalization layers.\n",
    "# inner_activation: The activation to use for the inner\n",
    "#     FFN layers.\n",
    "# kernel_initializer: Initializer for dense layer kernels.\n",
    "# inner_dropout: Dropout probability for the inner dropout\n",
    "#     layer.\n",
    "class TransformerXLBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 hidden_size,\n",
    "                 num_attention_heads,\n",
    "                 head_size,\n",
    "                 inner_size,\n",
    "                 dropout_rate,\n",
    "                 attention_dropout_rate,\n",
    "                 norm_epsilon=1e-12,\n",
    "                 inner_activation=\"relu\",\n",
    "                 kernel_initializer=\"variance_scaling\",\n",
    "                 inner_dropout=0.0,\n",
    "                 **kwargs):\n",
    "        \"\"\"Initializes TransformerXLBlock layer.\"\"\"\n",
    "\n",
    "        super(TransformerXLBlock, self).__init__(**kwargs)\n",
    "        self._vocab_size = vocab_size\n",
    "        self._num_heads = num_attention_heads\n",
    "        self._head_size = head_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._inner_size = inner_size\n",
    "        self._dropout_rate = dropout_rate\n",
    "        self._attention_dropout_rate = attention_dropout_rate\n",
    "        self._inner_activation = inner_activation\n",
    "        self._norm_epsilon = norm_epsilon\n",
    "        self._kernel_initializer = kernel_initializer\n",
    "        self._inner_dropout = inner_dropout\n",
    "        self._attention_layer_type = MultiHeadRelativeAttention\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_tensor = input_shape[0] if len(input_shape) == 2 else input_shape\n",
    "        input_tensor_shape = tf.TensorShape(input_tensor)\n",
    "        if len(input_tensor_shape.as_list()) != 3:\n",
    "            raise ValueError(\"TransformerLayer expects a three-dimensional input of \"\n",
    "                                             \"shape [batch, sequence, width].\")\n",
    "        batch_size, sequence_length, hidden_size = input_tensor_shape\n",
    "\n",
    "        if len(input_shape) == 2:\n",
    "            mask_tensor_shape = tf.TensorShape(input_shape[1])\n",
    "            expected_mask_tensor_shape = tf.TensorShape(\n",
    "                    [batch_size, sequence_length, sequence_length])\n",
    "            if not expected_mask_tensor_shape.is_compatible_with(mask_tensor_shape):\n",
    "                raise ValueError(\"When passing a mask tensor to TransformerXLBlock, \"\n",
    "                                                 \"the mask tensor must be of shape [batch, \"\n",
    "                                                 \"sequence_length, sequence_length] (here %s). Got a \"\n",
    "                                                 \"mask tensor of shape %s.\" %\n",
    "                                                 (expected_mask_tensor_shape, mask_tensor_shape))\n",
    "        if hidden_size % self._num_heads != 0:\n",
    "            raise ValueError(\n",
    "                    \"The input size (%d) is not a multiple of the number of attention \"\n",
    "                    \"heads (%d)\" % (hidden_size, self._num_heads))\n",
    "        self._attention_layer = self._attention_layer_type(\n",
    "                num_heads=self._num_heads,\n",
    "                key_dim=self._head_size,\n",
    "                value_dim=self._head_size,\n",
    "                dropout=self._attention_dropout_rate,\n",
    "                use_bias=False,\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer),\n",
    "                name=\"rel_attn\")\n",
    "        self._attention_dropout = tf.keras.layers.Dropout(\n",
    "                rate=self._attention_dropout_rate)\n",
    "        self._attention_layer_norm = tf.keras.layers.LayerNormalization(\n",
    "                name=\"self_attention_layer_norm\",\n",
    "                axis=-1,\n",
    "                epsilon=self._norm_epsilon,\n",
    "                dtype=tf.float32)\n",
    "        self._inner_dense = tf.keras.layers.experimental.EinsumDense(\n",
    "                \"abc,cd->abd\",\n",
    "                output_shape=(None, self._inner_size),\n",
    "                bias_axes=\"d\",\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer),\n",
    "                name=\"inner\")\n",
    "\n",
    "        self._inner_activation_layer = tf.keras.layers.Activation(\n",
    "                self._inner_activation)\n",
    "        self._inner_dropout_layer = tf.keras.layers.Dropout(\n",
    "                rate=self._inner_dropout)\n",
    "        self._output_dense = tf.keras.layers.experimental.EinsumDense(\n",
    "                \"abc,cd->abd\",\n",
    "                output_shape=(None, hidden_size),\n",
    "                bias_axes=\"d\",\n",
    "                name=\"output\",\n",
    "                kernel_initializer=clone_initializer(self._kernel_initializer))\n",
    "        self._output_dropout = tf.keras.layers.Dropout(rate=self._dropout_rate)\n",
    "        self._output_layer_norm = tf.keras.layers.LayerNormalization(\n",
    "                name=\"output_layer_norm\",\n",
    "                axis=-1,\n",
    "                epsilon=self._norm_epsilon)\n",
    "\n",
    "        super(TransformerXLBlock, self).build(input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "                \"vocab_size\":\n",
    "                        self._vocab_size,\n",
    "                \"hidden_size\":\n",
    "                        self._hidden_size,\n",
    "                \"num_attention_heads\":\n",
    "                        self._num_heads,\n",
    "                \"head_size\":\n",
    "                        self._head_size,\n",
    "                \"inner_size\":\n",
    "                        self._inner_size,\n",
    "                \"dropout_rate\":\n",
    "                        self._dropout_rate,\n",
    "                \"attention_dropout_rate\":\n",
    "                        self._attention_dropout_rate,\n",
    "                \"norm_epsilon\":\n",
    "                        self._norm_epsilon,\n",
    "                \"inner_activation\":\n",
    "                        self._inner_activation,\n",
    "                \"kernel_initializer\":\n",
    "                        self._kernel_initializer,\n",
    "                \"inner_dropout\":\n",
    "                        self._inner_dropout,\n",
    "        }\n",
    "        base_config = super(TransformerXLBlock, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "# content_stream: `Tensor`, the input content stream. This is the standard\n",
    "# input to Transformer XL and is commonly referred to as `h` in XLNet.\n",
    "# content_attention_bias: Bias `Tensor` for content based attention of shape\n",
    "# `[num_heads, dim]`.\n",
    "# positional_attention_bias: Bias `Tensor` for position based attention of\n",
    "# shape `[num_heads, dim]`.\n",
    "# relative_position_encoding: Relative positional encoding `Tensor` of shape\n",
    "# `[B, L, dim]`.\n",
    "# state: Optional `Tensor` of shape `[B, M, E]`, where M is the length of\n",
    "# the state or memory. If passed, this is also attended over as in\n",
    "# Transformer XL.\n",
    "# content_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to content attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# query_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to query attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# target_mapping: Optional `Tensor` representing the target mapping when\n",
    "# calculating query attention.\n",
    "# Returns:\n",
    "# A `dict` object, containing the key value pairs for `content_attention`\n",
    "    def call(self,\n",
    "             content_stream,\n",
    "             content_attention_bias,\n",
    "             positional_attention_bias,\n",
    "             relative_position_encoding=None,\n",
    "             state=None,\n",
    "             content_attention_mask=None,\n",
    "             query_attention_mask=None,\n",
    "             target_mapping=None):\n",
    "        \n",
    "        attention_kwargs = dict(\n",
    "                query=content_stream,\n",
    "                value=content_stream,\n",
    "                key=content_stream,\n",
    "                attention_mask=content_attention_mask)\n",
    "\n",
    "        common_attention_kwargs = dict(\n",
    "                content_attention_bias=content_attention_bias,\n",
    "                relative_position_encoding=relative_position_encoding,\n",
    "                positional_attention_bias=positional_attention_bias,\n",
    "                state=state)\n",
    "\n",
    "        attention_kwargs.update(common_attention_kwargs)\n",
    "        attention_output = self._attention_layer(**attention_kwargs)\n",
    "\n",
    "        attention_stream = attention_output\n",
    "        input_stream = content_stream\n",
    "        attention_key = \"content_attention\"\n",
    "        attention_output = {}\n",
    "        \n",
    "        attention_stream = self._attention_dropout(attention_stream)\n",
    "        attention_stream = self._attention_layer_norm(attention_stream + input_stream)\n",
    "        inner_output = self._inner_dense(attention_stream)\n",
    "        inner_output = self._inner_activation_layer(\n",
    "                inner_output)\n",
    "        inner_output = self._inner_dropout_layer(\n",
    "                inner_output)\n",
    "        layer_output = self._output_dense(inner_output)\n",
    "        layer_output = self._output_dropout(layer_output)\n",
    "        layer_output = self._output_layer_norm(layer_output + attention_stream)\n",
    "        attention_output[attention_key] = layer_output\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8749ec30-1281-4f9d-8baa-54b890fb7949",
   "metadata": {},
   "source": [
    "---\n",
    "# Transformer XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "019ca25d-e899-4e17-8f41-dfdc3d9bdee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_layers: The number of layers.\n",
    "# hidden_size: The hidden size.\n",
    "# num_attention_heads: The number of attention heads.\n",
    "# head_size: The dimension size of each attention head.\n",
    "# inner_size: The hidden size in feed-forward layers.\n",
    "# dropout_rate: Dropout rate used in each Transformer XL block.\n",
    "# attention_dropout_rate: Dropout rate on attention probabilities.\n",
    "# initializer: The initializer to use for attention biases.\n",
    "# tie_attention_biases: Whether or not to tie biases together. If `True`, then\n",
    "# each Transformer XL block shares the same trainable attention bias. If\n",
    "# `False`, then each block has its own attention bias. This is usually set\n",
    "# to `True`.\n",
    "# memory_length: The number of tokens to cache.\n",
    "# reuse_length: The number of tokens in the current batch to be cached\n",
    "# and reused in the future.\n",
    "# inner_activation: The activation to use in the inner layers\n",
    "# for Transformer XL blocks. Typically \"relu\" or \"gelu\".\n",
    "class TransformerXL(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 num_layers,\n",
    "                 hidden_size,\n",
    "                 maxlen,\n",
    "                 embed_dim,\n",
    "                 num_attention_heads,\n",
    "                 head_size,\n",
    "                 inner_size,\n",
    "                 dropout_rate,\n",
    "                 attention_dropout_rate,\n",
    "                 initializer,\n",
    "                 tie_attention_biases=True,\n",
    "                 memory_length=None,\n",
    "                 reuse_length=None,\n",
    "                 inner_activation=\"relu\",\n",
    "                 **kwargs):\n",
    "        super(TransformerXL, self).__init__(**kwargs)\n",
    "\n",
    "        self._vocab_size = vocab_size\n",
    "        self._initializer = initializer\n",
    "        self._num_layers = num_layers\n",
    "        self._hidden_size = hidden_size\n",
    "        self._num_attention_heads = num_attention_heads\n",
    "        self._head_size = head_size\n",
    "        self._inner_size = inner_size\n",
    "        self._inner_activation = inner_activation\n",
    "        self._dropout_rate = dropout_rate\n",
    "        self._attention_dropout_rate = attention_dropout_rate\n",
    "        self._tie_attention_biases = tie_attention_biases\n",
    "\n",
    "        self._memory_length = memory_length\n",
    "        self._reuse_length = reuse_length\n",
    "\n",
    "        if self._tie_attention_biases:\n",
    "            attention_bias_shape = [self._num_attention_heads, self._head_size]\n",
    "        else:\n",
    "            attention_bias_shape = [self._num_layers, self._num_attention_heads, self._head_size]\n",
    "\n",
    "        self.content_attention_bias = self.add_weight(\n",
    "                \"content_attention_bias\",\n",
    "                shape=attention_bias_shape,\n",
    "                dtype=tf.float32,\n",
    "                initializer=clone_initializer(self._initializer))\n",
    "        self.positional_attention_bias = self.add_weight(\n",
    "                \"positional_attention_bias\",\n",
    "                shape=attention_bias_shape,\n",
    "                dtype=tf.float32,\n",
    "                initializer=clone_initializer(self._initializer))\n",
    "\n",
    "        self.transformer_xl_layers = []\n",
    "        for i in range(self._num_layers):\n",
    "            self.transformer_xl_layers.append(\n",
    "                    TransformerXLBlock(\n",
    "                            vocab_size=self._vocab_size,\n",
    "                            hidden_size=self._head_size * self._num_attention_heads,\n",
    "                            num_attention_heads=self._num_attention_heads,\n",
    "                            head_size=self._head_size,\n",
    "                            inner_size=self._inner_size,\n",
    "                            dropout_rate=self._dropout_rate,\n",
    "                            attention_dropout_rate=self._attention_dropout_rate,\n",
    "                            norm_epsilon=1e-12,\n",
    "                            inner_activation=self._inner_activation,\n",
    "                            kernel_initializer=\"variance_scaling\",\n",
    "                            name=\"layer_%d\" % i))\n",
    "\n",
    "        self.output_dropout = tf.keras.layers.Dropout(rate=self._dropout_rate)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "                \"vocab_size\":\n",
    "                        self._vocab_size,\n",
    "                \"num_layers\":\n",
    "                        self._num_layers,\n",
    "                \"hidden_size\":\n",
    "                        self._hidden_size,\n",
    "                \"num_attention_heads\":\n",
    "                        self._num_attention_heads,\n",
    "                \"head_size\":\n",
    "                        self._head_size,\n",
    "                \"inner_size\":\n",
    "                        self._inner_size,\n",
    "                \"dropout_rate\":\n",
    "                        self._dropout_rate,\n",
    "                \"attention_dropout_rate\":\n",
    "                        self._attention_dropout_rate,\n",
    "                \"initializer\":\n",
    "                        self._initializer,\n",
    "                \"tie_attention_biases\":\n",
    "                        self._tie_attention_biases,\n",
    "                \"memory_length\":\n",
    "                        self._memory_length,\n",
    "                \"reuse_length\":\n",
    "                        self._reuse_length,\n",
    "                \"inner_activation\":\n",
    "                        self._inner_activation,\n",
    "        }\n",
    "        base_config = super(TransformerXL, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "# content_stream: `Tensor`, the input content stream. This is the standard\n",
    "# input to Transformer XL and is commonly referred to as `h` in XLNet.\n",
    "# relative_position_encoding: Relative positional encoding `Tensor` of shape\n",
    "# `[B, L, dim]`.\n",
    "# state: Optional `Tensor` of shape `[B, M, E]`, where M is the length of\n",
    "# the state or memory. If passed, this is also attended over as in\n",
    "# Transformer XL.\n",
    "# content_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to content attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# query_attention_mask: Optional `Tensor` representing the mask that is\n",
    "# added to query attention logits. If state is not None, the mask source\n",
    "# sequence dimension should extend M.\n",
    "# target_mapping: Optional `Tensor` representing the target mapping when\n",
    "# calculating query attention.\n",
    "# Returns:\n",
    "# A tuple consisting of the attention output and the list of cached memory\n",
    "# states.\n",
    "# The attention output is `content_attention`\n",
    "    def call(self,\n",
    "             content_stream,\n",
    "             relative_position_encoding,\n",
    "             state=None,\n",
    "             content_attention_mask=None,\n",
    "             query_attention_mask=None,\n",
    "             target_mapping=None):\n",
    "        \n",
    "        new_mems = []\n",
    "\n",
    "        if state is None:\n",
    "            state = [None] * self._num_layers\n",
    "        for i in range(self._num_layers):\n",
    "            # cache new mems\n",
    "            new_mems.append(\n",
    "                    _cache_memory(content_stream, state[i],\n",
    "                                                self._memory_length, self._reuse_length))\n",
    "\n",
    "            if self._tie_attention_biases:\n",
    "                content_attention_bias = self.content_attention_bias\n",
    "            else:\n",
    "                content_attention_bias = self.content_attention_bias[i]\n",
    "                \n",
    "            if self._tie_attention_biases:\n",
    "                positional_attention_bias = self.positional_attention_bias\n",
    "            else:\n",
    "                positional_attention_bias = self.positional_attention_bias[i]\n",
    "\n",
    "            transformer_xl_layer = self.transformer_xl_layers[i]\n",
    "            \n",
    "            transformer_xl_output = transformer_xl_layer(\n",
    "                    content_stream=content_stream,\n",
    "                    content_attention_bias=content_attention_bias,\n",
    "                    positional_attention_bias=positional_attention_bias,\n",
    "                    relative_position_encoding=relative_position_encoding,\n",
    "                    state=state[i],\n",
    "                    content_attention_mask=content_attention_mask,\n",
    "                    query_attention_mask=query_attention_mask,\n",
    "                    target_mapping=target_mapping)\n",
    "            content_stream = transformer_xl_output[\"content_attention\"]\n",
    "\n",
    "        output_stream = content_stream\n",
    "        return output_stream, new_mems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a127a4b5-15e3-46e3-838b-a5e18e4d8dc8",
   "metadata": {},
   "source": [
    "---\n",
    "# Xl Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "855d43dd-e182-4c2a-961a-ff9781141f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XlModel(keras.Model):\n",
    "    def __init__(self, block_size, seq_len_padded, embed_dim, vocab_size, num_layers, hidden_size, num_attention_heads, maxlen, memory_length, reuse_length, head_size, inner_size, dropout_rate, attention_dropout_rate, initializer):\n",
    "        super(XlModel, self).__init__()\n",
    "        \n",
    "        self.block_size = block_size\n",
    "        self.seq_len_padded = seq_len_padded\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_attention_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.maxlen = maxlen\n",
    "        self.memory_length = memory_length\n",
    "        \n",
    "        self.embedding_layer = MaskedTokenAndPositionEmbedding(maxlen=maxlen, vocab_size=vocab_size, embed_dim=embed_dim)\n",
    "        \n",
    "        self.rel_embeddings = RelativePositionEmbedding(embed_dim)\n",
    "\n",
    "        self.transformer_xl = TransformerXL(\n",
    "                vocab_size=vocab_size,\n",
    "                num_layers=num_layers,\n",
    "                hidden_size=hidden_size,\n",
    "                num_attention_heads=num_attention_heads,\n",
    "                maxlen=maxlen,\n",
    "                embed_dim=embed_dim,\n",
    "                memory_length=memory_length,\n",
    "                reuse_length=reuse_length,\n",
    "                head_size=head_size,\n",
    "                inner_size=inner_size,\n",
    "                dropout_rate=dropout_rate,\n",
    "                attention_dropout_rate=attention_dropout_rate,\n",
    "                initializer=initializer, \n",
    "            )\n",
    "        \n",
    "    \n",
    "    def call(self, x, training=None):        \n",
    "        \n",
    "        model_output = []\n",
    " \n",
    "        mems = tf.zeros((self.num_layers, tf.shape(x)[0], self.memory_length, self.embed_dim))\n",
    "        \n",
    "        embeddings = self.embedding_layer(x)\n",
    "        \n",
    "        if self.memory_length > 0:\n",
    "            rel_embeddings = self.rel_embeddings(tf.concat((mems[0], embeddings), axis=1))\n",
    "        else:\n",
    "            rel_embeddings = self.rel_embeddings(embeddings)\n",
    "            \n",
    "        for i in range(0, self.seq_len_padded, self.block_size):\n",
    "            block = embeddings[:,i:i+block_size]\n",
    "            rel_block = rel_embeddings[:,i:i+self.block_size+self.memory_length]\n",
    "            \n",
    "            output, mems = self.transformer_xl(content_stream=block, relative_position_encoding=rel_block, state=mems)\n",
    "        \n",
    "            if i == 0:\n",
    "                model_output = output\n",
    "            else:\n",
    "                model_output = tf.concat([model_output, output], axis=1)\n",
    "        \n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3469de1-250d-4c14-bb79-a4e74f0fe93e",
   "metadata": {},
   "source": [
    "---\n",
    "# Xl Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2bc3ea7d-d4a1-4a1e-bf2a-7cd1c91430cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xl Parameters \n",
    "embed_dim = 64\n",
    "num_layers = 8\n",
    "hidden_size = 64\n",
    "num_attention_heads = 8\n",
    "memory_length = 2\n",
    "reuse_length = 0\n",
    "head_size = 32\n",
    "inner_size = 32\n",
    "dropout_rate = 0.01\n",
    "attention_dropout_rate = 0.01\n",
    "initializer = keras.initializers.RandomNormal(stddev=0.1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89649663-7d18-48bc-8678-ce5f093908cc",
   "metadata": {},
   "source": [
    "---\n",
    "# Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "821b0c4a-6d05-4867-a713-474bfdbff881",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inp = keras.layers.Input((None, embed_dim))\n",
    "model = XlModel(block_size, seq_len_padded, embed_dim, vocab_size, num_layers, hidden_size, num_attention_heads, maxlen, memory_length, reuse_length, head_size, inner_size, dropout_rate, attention_dropout_rate, initializer)\n",
    "# model = keras.Model(inp, xl)\n",
    "model.compile(loss = MaskedSparseCategoricalCrossentropy, optimizer = keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c726b5aa-bc90-44a8-bc2e-5bf755a0dcdc",
   "metadata": {},
   "source": [
    "---\n",
    "# Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c9d81f1-57e9-416c-a7ea-bf8b8a3c2881",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(batches):\n",
    "    \n",
    "    batch_train, batch_test = batches\n",
    "    \n",
    "    x_train, pre_y_train, post_y_train = batch_train\n",
    "    x_test, pre_y_test, post_y_test = batch_test\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        output_train = model(x_train)\n",
    "        loss_train = MaskedSparseCategoricalCrossentropy(post_y_train, output_train)\n",
    "        \n",
    "    output_test = model(x_test)\n",
    "    loss_test = MaskedSparseCategoricalCrossentropy(post_y_test, output_test)\n",
    "        \n",
    "    accuracy_train = MaskedSparseCategoricalAccuracy(post_y_train, output_train)\n",
    "    accuracy_test = MaskedSparseCategoricalAccuracy(post_y_test, output_test)\n",
    "          \n",
    "    grads = tape.gradient(loss_train, model.trainable_weights)\n",
    "    \n",
    "    model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "    return loss_train, accuracy_train, loss_test, accuracy_test\n",
    "\n",
    "# @tf.function()\n",
    "# def dist_train_step(batches):\n",
    "#     loss_train, accuracy_train, loss_test, accuracy_test = strategy.run(train_step, args=(batches,))\n",
    "#     return strategy.reduce(tf.distribute.ReduceOp.SUM, loss_train, axis=None), accuracy_train, loss_test, accuracy_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63c3a2e-02b5-4dbe-9527-a33c23338ce5",
   "metadata": {},
   "source": [
    "---\n",
    "# Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5a6127f4-fb94-4bf2-b3cf-fae83ef0aa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_loss_train = []\n",
    "history_accuracy_train = []\n",
    "history_loss_test = []\n",
    "history_accuracy_test = []\n",
    "\n",
    "epochs = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a260118e-c39d-4232-8ee2-11ae86d8c8f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/400 Train Loss: 2.2290749549865723 Train Accuracy = 0.3636363744735718 Test Loss: 3.354614019393921 Test Accuracy = 0.2363636344671249424"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    batch_train = Full_Batch(batch_size, x_flattened_train, pre_y_flattened_train, post_y_flattened_train, num_chars_data_train, seq_len, rng)\n",
    "    batch_test = Full_Batch(batch_size, x_flattened_test, pre_y_flattened_test, post_y_flattened_test, num_chars_data_test, seq_len, rng)\n",
    "\n",
    "    batches = [batch_train, batch_test]\n",
    "\n",
    "    loss_train, accuracy_train, loss_test, accuracy_test = train_step(batches)\n",
    "\n",
    "    history_loss_train.append(loss_train)\n",
    "    history_accuracy_train.append(accuracy_train) \n",
    "    history_loss_test.append(loss_test)\n",
    "    history_accuracy_test.append(accuracy_test) \n",
    "\n",
    "    print(f\"\\r{epoch+1}/{epochs} Train Loss: {loss_train} Train Accuracy = {accuracy_train} Test Loss: {loss_test} Test Accuracy = {accuracy_test}\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5029869f-09b0-4ad7-8b69-06a155104667",
   "metadata": {},
   "source": [
    "---\n",
    "# Train vs Test Accuracy and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c3ecbaa-4117-47fa-9ad3-de755a5b98e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.plot(history_accuracy_train)\n",
    "plt.ylabel('Train Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.plot(history_accuracy_test)\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.legend(['Training','Testing'],loc='lower right') \n",
    "\n",
    "plt.figure(2)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(history_loss_train)\n",
    "plt.ylabel('Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(history_loss_test)\n",
    "plt.ylabel('Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.legend(['Training','Testing'],loc='lower right') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031720c2-16f7-486c-83e0-d52cd42661ac",
   "metadata": {},
   "source": [
    "---\n",
    "# Memory vs Accuracy Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "724ebe19-73b6-4aeb-b506-f24d6722ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_lens = []\n",
    "final_accuracies_train = []\n",
    "final_accuracies_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cc71d008-1f06-4d2b-a8b4-0ed7fb8603d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_lens = np.loadtxt(\"/home/jovyan/TransformerXL/XL/Mem_Lens.txt\", dtype=np.float64)\n",
    "final_accuracies_train =  np.loadtxt(\"/home/jovyan/TransformerXL/XL/Final_Accuracies_Train.txt\", dtype=np.float64)\n",
    "final_accuracies_test =  np.loadtxt(\"/home/jovyan/TransformerXL/XL/Final_Accuracies_Test.txt\", dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b5293b4-e6a7-4fbb-a40a-512e965573c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10.  40.  40. 100.  40.  40. 100. 100.   0. 100. 200. 200. 100. 100.\n",
      " 100.]\n",
      "[0.927778   1.         1.         1.         1.         0.948756\n",
      " 0.996197   0.996197   0.998736   0.171383   0.994533   0.993177\n",
      " 0.988822   0.871429   0.98927683]\n",
      "[0.188889   0.091156   0.091156   0.08429    0.108081   0.069364\n",
      " 0.076513   0.076513   0.072531   0.191552   0.067179   0.094536\n",
      " 0.065421   0.064865   0.07931249]\n"
     ]
    }
   ],
   "source": [
    "mem_lens = np.append(mem_lens, memory_length)\n",
    "final_accuracies_train = np.append(final_accuracies_train, accuracy_train)\n",
    "final_accuracies_test = np.append(final_accuracies_test, accuracy_test)\n",
    "print(mem_lens)\n",
    "print(final_accuracies_train)\n",
    "print(final_accuracies_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "779262e0-716e-431b-9e95-464bb76b8f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_accuracy_train = ({mem_len:accuracy_train for mem_len, accuracy_train in zip(mem_lens, final_accuracies_train)})\n",
    "mem_accuracy_test = ({mem_len:accuracy_test for mem_len, accuracy_test in zip(mem_lens, final_accuracies_test)})\n",
    "\n",
    "mem_lens_train_sorted = np.array([l for l in sorted(mem_accuracy_train)])\n",
    "mem_lens_test_sorted = np.array([l for l in sorted(mem_accuracy_test)])\n",
    "\n",
    "final_accuracies_train_sorted = np.array([mem_accuracy_train[l] for l in mem_lens_train_sorted])\n",
    "final_accuracies_test_sorted = np.array([mem_accuracy_test[l] for l in mem_lens_test_sorted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9358244f-1349-4be4-bc01-cf7298f784d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('/home/jovyan/TransformerXL/XL/Mem_Lens.txt', (mem_lens), fmt='%f', delimiter='\\n')\n",
    "np.savetxt('/home/jovyan/TransformerXL/XL//Final_Accuracies_Train.txt', (final_accuracies_train), fmt='%f', delimiter='\\n')\n",
    "np.savetxt('/home/jovyan/TransformerXL/XL//Final_Accuracies_Test.txt', (final_accuracies_test), fmt='%f', delimiter='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e931bbd-a1a9-4098-a4f2-0ef29f7fd89c",
   "metadata": {},
   "source": [
    "---\n",
    "# Memory vs Accuracy Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ba942651-37b0-465e-b3c0-5dd22621b61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(2)\n",
    "plt.subplot(222)\n",
    "plt.plot(mem_lens_train_sorted, final_accuracies_train_sorted)\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(mem_lens_test_sorted, final_accuracies_test_sorted)\n",
    "plt.xlabel('Memory Length')\n",
    "\n",
    "plt.legend(['Training','Testing'],loc='lower right') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91287ca0-3ff6-4d16-a8fe-837dcd5bf883",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e20aaf95-5aa2-4a44-8373-6eb89dc1fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, pre_y_train, post_y_train = batch_train = Full_Batch(batch_size, x_flattened_train, pre_y_flattened_train, post_y_flattened_train, num_chars_data_train, seq_len, rng)\n",
    "x_test, pre_y_test, post_y_test = Full_Batch(batch_size, x_flattened_test, pre_y_flattened_test, post_y_flattened_test, num_chars_data_test, seq_len, rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "611f5f79-1b82-404b-85f8-71a495e6915c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 450)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a0ea41ea-f3c5-46f1-b188-2e8708c6c4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy = x_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1bf27abd-667b-4e4a-9909-421e92b4e106",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy[:,50:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9cdb7e65-0255-4d6d-9784-1d70199b4c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ow so? How can it affect them?\"thousani a year coyyyybbbbbbbbbbbbbbbbbbbb'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = model.predict(copy)\n",
    "u = tf.argmax(u, -1)\n",
    "decode_seq(u[0], i_to_c_pandp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a8efcd49-a96f-46f5-b595-14d383c8ca64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ey have none of them much to recommend them,\" replied he; \"they aresuddenly addressed her with:hearty consent to his marrying whichever he chooses of the girls; thoughgo, merely on that account, for in general, you know, they visit no\"You are over-scrupulous, surely. I dare say Mr. Bingley will be veryMrs. Bennet deigned not to make any reply, but, unable to contain\"You mistake me, my dear. I have\n",
      "\n",
      "\n",
      " ow so? How can it affect them?\"thousani a year come into the neighbourhood.\"\"But you forget, mamma,\" said Elizabeth, \"that we shall meet him at the\"They have none of them much to recommend them,\" reelied he; \"they arereserve, and caprice, that the experience of three-and-twenty years hadlittle information, and uncertain temper. When she was discontented,of her own. She is a selfish, hypocritical \n",
      "\n",
      "\n",
      "How so? How can it affect them?\"thousand a year come into the neighbourhood.\"\"But you forget, mamma,\" said Elizabeth, \"that we shall meet him at the\"They have none of them much to recommend them,\" replied he; \"they arereserve, and caprice, that the experience of three-and-twenty years hadlittle information, and uncertain temper. When she was discontented,of her own. She is a selfish, hypocritical \n"
     ]
    }
   ],
   "source": [
    "train = model.predict(x_train)\n",
    "train = tf.argmax(train, -1)\n",
    "train = decode_seq(train[0], i_to_c_pandp)\n",
    "target = decode_seq(post_y_train[0], i_to_c_pandp)\n",
    "print(decode_seq(x_train[0], i_to_c_pandp))\n",
    "print('\\n')\n",
    "print(train)\n",
    "print('\\n')\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b1ea5909-94e5-4b2b-bb58-47cdef22ce7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on do bet mItoy,wo\"hao y. ny ttmlt lthin_ rmw my hoouro r,ilersrv  oraikie tMr onu oont  hsan s ws o dec haus,n  h forsh aautron vsunas rtouy s hie  hhhsh assunr suriara resheihhgls tiigr tt liadr noaae hllinfhe rolorsitiee miea n  oo ehaoryI glese l iterenrhrs tenuwuiw.t t r t ts a heeka vbhburohtt hf  rslnet k r unil re \"tterio nlo gryy bery r on doou etdot snu. n air  le l esow eempa\", ddh o so\n",
      "\n",
      "\n",
      "s immediately; that he\n"
     ]
    }
   ],
   "source": [
    "test = model.predict(x_test)\n",
    "test = tf.argmax(test, -1)\n",
    "test = decode_seq(test[0], i_to_c_pandp)\n",
    "target = decode_seq(post_y_test[0], i_to_c_pandp)\n",
    "print(test)\n",
    "print('\\n')\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbef70d4-a6d2-424e-ae07-c6d59e753b05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fcde74-9c5a-434b-ab22-1b6b9c0c5223",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
